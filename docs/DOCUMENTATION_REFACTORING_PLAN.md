# üìã Discernus Documentation Refactoring Plan
**Version**: 1.0  
**Date**: January 2025  
**Purpose**: Transform scattered, domain-specific documentation into coherent, domain-neutral platform guide

---

## **üéØ Executive Summary: Why This Matters**

This documentation refactoring is **foundational to project success**, not routine maintenance. Current documentation problems are causing:

### **Agent Drift Crisis**
- AI agents consistently "run off like puppies" and write THICK code that violates architecture
- They solve immediate problems but create 10 new problems
- User must constantly play shepherd role - it's tedious and unsustainable
- Agents deviate from vision, philosophy, and architecture despite briefings

### **Strategic Incoherence**
- Scattered PM documents create conflicting visions (SOAR vs. Discernus branding)
- Domain-specific language limits platform adoption beyond political science
- Key infrastructure (agent registry, model registry) is foundational but underdocumented
- THIN compliance violations found during comprehensive audit

### **Mission-Critical Requirements**
- **Mathematical Reliability**: LLMs cannot be trusted for calculations (inconsistent and opaque)
- **Human-Readable Provenance**: Current logs are machine-readable "wall of code"
- **Universal Applicability**: Platform must work for think tanks, journalists, corporations, religious organizations, etc.
- **Drupal-Style Ecosystem**: Tightly controlled core + open extension modules

**Strategic Context**: We audited THIN compliance and found critical violations. The solution requires systematic documentation refactoring that prevents agent drift and enables the platform vision.

---

## **üèóÔ∏è Architecture Requirements**

### **Mathematical Reliability (Critical)**
- **Problem**: LLMs are inconsistent and opaque for calculations - fatal flaws for statistical analysis
- **Solution**: Hybrid intelligence architecture
  - LLM designs analytical approach
  - Secure code executor performs calculations
  - LLM interprets results in natural language
- **Documentation Impact**: All analytical workflows must demonstrate this pattern

### **Human-Readable Provenance**
- **Problem**: Current artifacts are machine-readable but not human-accessible
- **Current State**: "Wall of code" that doesn't serve research archive needs
- **Solution**: Make reports look like "well-structured academic research archive"
- **Documentation Impact**: Chronolog and provenance systems need human-friendly presentation

### **Agent Drift Prevention**
- **Problem**: Agents consistently write THICK code despite briefings
- **Examples**: Extensive parsing, hardcoded intelligence, monolithic architecture
- **Solution**: Documentation that makes it "easier to do right thing, harder to do wrong thing"
- **Documentation Impact**: Every page must include THIN compliance guidance and examples

### **Universal Applicability**
- **Problem**: Current docs focus on "computational social science" and "political analysis"
- **Vision**: Platform for any rhetorical text analysis across domains
- **Use Cases**: Think tanks, journalists, media organizations, corporations, government agencies, NGOs, hobbyists, divinity departments, religious organizations
- **Documentation Impact**: Domain-neutral language and examples throughout

---

## **üèõÔ∏è Governance Model: Drupal-Style Ecosystem**

### **User's Strategic Experience**
- Former CMO at Acquia with "front row seat" to Dries's Drupal management
- Wants to take those architectural and governance principles forward
- Proven model for open source platform with controlled core

### **Core Principles**
- **Tightly controlled core** with predictable interfaces
- **Open extension ecosystem** with independent lifecycles  
- **Trademark protection** for fork deterrence and governance
- **Community governance** with quality standards
- **Clear boundaries** between core platform and extension modules

### **Documentation Impact**
- Every document must clearly define core vs. extension boundaries
- Extension development guides must follow Drupal patterns
- Governance documentation must establish trademark and quality standards
- Platform architecture must enable module ecosystem

---

## **üéØ Strategic Vision**

**Mission**: Create documentation that makes it "easier to do the right thing and harder to do the wrong thing" for both human developers and AI agents working on Discernus.

**Core Challenge**: Current documentation is scattered across PM folders, uses domain-specific language, and lacks operational detail for fresh agents.

**Solution**: Systematic 6-phase refactoring with detailed execution guides, content transformation rules, and quality gates.

## **üèóÔ∏è Three Foundational Platform Commitments**

**Implemented**: January 2025 during Phase 1 completion
**Rationale**: Source material analysis revealed these themes as equally critical to platform success

### **Mathematical Reliability**
**Critical Foundation**: LLMs are inconsistent and opaque for calculations‚Äîfatal flaws for statistical analysis. Every architectural component must implement the hybrid intelligence pattern: LLM designs analytical approach ‚Üí secure code executor performs calculations ‚Üí LLM interprets results in natural language.

### **Cost Transparency**
**Institutional Adoption Enabler**: Academic and organizational adoption requires cost predictability. Every architectural component must provide upfront cost estimation, budget controls, and intelligent model selection to ensure predictable pricing.

### **Complete Reproducibility**
**Zero Mystery Commitment**: Non-negotiable commitment that there should be zero mystery as to how analysis was done and how results were generated. Every architectural component must maintain complete audit trails, decision documentation, and provenance chains that enable independent researchers to achieve deterministically identical results and defend their work under academic scrutiny. This includes full calculation transparency with complete visibility into all mathematical operations, parameters, and computational procedures.

### **Implementation Record**
**Date**: January 2025, Phase 1 implementation
**Reason**: User feedback during Phase 1 completion identified the need to elevate cost transparency from feature to core principle and establish complete reproducibility as equally foundational to mathematical reliability. User specified that if complete reproducibility is not reflected in documentation at the end of refactor, "we've failed" - emphasizing this is about academic credibility and institutional trust, not just documentation quality.

**Changes Made**:
- Added Critical Themes Checklist to all phases and agent templates
- Elevated cost transparency in strategic positioning
- Emphasized complete reproducibility as architectural requirement
- Integrated calculation transparency into reproducibility commitment
- Softened activist language to avoid "activating antibodies"
- Updated all deliverables to reflect these three equally-weighted commitments

---

## **üìä Current State Analysis**

### **Key Insights from Strategic Review**
1. **SOAR terminology must be absorbed** into Discernus (not separate namespace)
2. **Domain-neutrality is critical** - political science is lead use case, not only use case  
3. **THIN compliance** means LLM intelligence + minimal software parsing
4. **Drupal-style ecosystem** - controlled core, open extensions
5. **Agent registry and model registry** are foundational but underdocumented

### **Content Transformation Rules**
**Domain-Neutral Language**:
- "computational social science" ‚Üí "computational text analysis"
- "political rhetoric analysis" ‚Üí "rhetorical communication analysis"  
- "academic researchers" ‚Üí "researchers, analysts, and organizations"
- "political speeches" ‚Üí "rhetorical texts"

**Audience Expansion**:
- Add examples: think tanks, journalists, corporations, religious studies
- Include use cases: brand analysis, policy research, content strategy
- Emphasize platform flexibility across domains

---

## **üöÄ Phase 1: Foundation Documents**

### **Execution Context**
**Duration**: 2-3 hours  
**Context Budget**: ~100KB  
**Dependencies**: Strategic PM documents  
**Risk**: Low

### **Source Material Analysis**

#### **Document 1: pm/strategy/RETHINKING_PROCESS_PLAN.md**
**Extract These Sections**:
- **Lines 20-50**: Mission statement and core purpose
- **Lines 51-80**: Strategic context and marketplace of ideas  
- **Lines 180-220**: Clear-eyed pragmatism principles
- **Lines 300-350**: Academic adoption strategy

**Content Transformations**:
- Extract "dramatically advance understanding of human rhetoric" 
- Transform to "dramatically advance understanding of rhetorical communication"
- Preserve "rigorous and scalable" emphasis
- Generalize academic adoption to "institutional adoption"

**Skip These Sections**:
- DCS framework specifics (lines 400-800)
- Attesor methodology details (lines 900-1200)
- Technical implementation details (lines 1300-1600)

#### **Document 2: pm/cara/Discernus Vision Thick LLM + Thin Software = Epistemic Trust.md**
**Extract These Sections**:
- **Lines 1-30**: Core philosophy and workflow overview
- **Lines 80-90**: Positioning and epistemic trust principles
- **Lines 95-140**: FAQ insights about LLM reliability

**Content Transformations**:
- Extract "Thick LLM + Thin Software = Epistemic Trust"
- Preserve adversarial review and transparency principles
- Generalize rhetorical analysis to "analytical frameworks"

### **Deliverable 1: docs/DISCERNUS_STRATEGIC_VISION.md**

**Document Template**:
```markdown
# Discernus Strategic Vision
*Domain-Neutral Computational Text Analysis Platform*

## Mission Statement
[2-3 sentences from RETHINKING_PROCESS_PLAN lines 20-50]

## Strategic Context
[3-4 paragraphs from RETHINKING_PROCESS_PLAN lines 51-80]

## Core Philosophy: "Thick LLM + Thin Software = Epistemic Trust"
[2-3 paragraphs from Discernus Vision lines 80-90]

## Platform Principles
[4-5 bullet points from RETHINKING_PROCESS_PLAN lines 180-220]

## Institutional Adoption Strategy
[2-3 paragraphs from RETHINKING_PROCESS_PLAN lines 300-350]

## Use Case Examples
- **Academic Research**: Political discourse, literary analysis, historical texts
- **Think Tanks**: Policy document analysis, strategic messaging
- **Journalism**: Source verification, narrative analysis
- **Corporations**: Brand sentiment, competitive intelligence
- **Religious Organizations**: Theological text analysis, pastoral guidance
- **Government Agencies**: Policy impact assessment, public communication analysis
- **NGOs**: Advocacy messaging, stakeholder communication
- **Media Organizations**: Content strategy, editorial analysis

## Architecture Requirements Integration
### **Mathematical Reliability**
- Document how analytical workflows use hybrid intelligence pattern
- Show LLM design + secure execution + LLM interpretation examples
- Emphasize that LLMs cannot be trusted for calculations

### **Human-Readable Provenance**
- Demonstrate how chronolog creates "academic research archive" feel
- Show human-friendly presentation of analytical processes
- Avoid "wall of code" documentation patterns

## Success Metrics
[Quantitative list from strategic documents]
```

**Quality Criteria**:
- ‚úÖ Domain-neutral language throughout
- ‚úÖ No references to specific frameworks (CFF, PDAF)
- ‚úÖ Clear mission in first paragraph
- ‚úÖ Use case examples span 8+ domains (expanded from 5)
- ‚úÖ Universal applicability emphasized
- ‚úÖ Architecture requirements integrated
- ‚úÖ THIN compliance guidance included
- ‚úÖ Length: 1000-1500 words

**Critical Themes Checklist**:
- ‚úÖ **Mathematical Reliability**: Hybrid intelligence pattern (LLM designs ‚Üí secure code executes ‚Üí LLM interprets) prominently featured
- ‚úÖ **Cost Transparency**: Upfront estimation, budget controls, and predictable pricing emphasized as institutional adoption enabler
- ‚úÖ **Complete Reproducibility**: Zero mystery commitment, deterministic replication, calculation transparency, and interrogation-proof outputs as core platform principle

### **Deliverable 2: docs/PLATFORM_ARCHITECTURE_OVERVIEW.md**

**Source Material**: pm/soar/soar_v2/Simple Atomic Orchestrated Research (SOAR) v2.0.md

**Content Extraction**:
- **Framework-agnostic orchestration** principles
- **Multi-model ensemble** architecture
- **Adversarial review** processes
- **Drupal-style module** ecosystem vision

**Document Template**:
```markdown
# Platform Architecture Overview
*Discernus Core System Design*

## Architecture Philosophy
[Extract from SOAR v2.0 orchestration principles]

## Core Components
### Agent Registry
[Brief description of dynamic agent discovery]

### Model Registry  
[Brief description of intelligent model selection]

### Orchestration Engine
[Extract from SOAR v2.0 ensemble coordination]

### Extension Ecosystem
[Drupal-style module architecture]

## THIN Architecture Principles
[Extract from Discernus Vision THIN philosophy]

## Adversarial Review Process
[Extract from SOAR v2.0 and Discernus Vision]

## Platform Boundaries
### Core Platform (Controlled)
- Agent orchestration
- Model management
- Security and provenance
- Basic analytical workflows

### Extension Modules (Open)
- Domain-specific frameworks
- Custom analysis agents
- Visualization tools
- Integration adapters
```

**Quality Criteria**:
- ‚úÖ Clear core vs. extension boundaries
- ‚úÖ No "SOAR" terminology used
- ‚úÖ Domain-neutral examples
- ‚úÖ Drupal-style ecosystem emphasized
- ‚úÖ Length: 800-1200 words

**Critical Themes Checklist**:
- ‚úÖ **Mathematical Reliability**: Hybrid intelligence pattern implemented in every architectural component
- ‚úÖ **Cost Transparency**: Upfront cost estimation and budget controls integrated throughout all components
- ‚úÖ **Complete Reproducibility**: Full audit trails, decision documentation, calculation transparency, and provenance chains in every component

### **Execution Steps**
1. **Read source documents** (30 minutes)
2. **Extract key sections** per guidelines above (45 minutes)
3. **Transform language** using domain-neutral rules (30 minutes)
4. **Create deliverables** using templates (60 minutes)
5. **Quality review** against criteria (15 minutes)

### **Handoff Validation**
**Success Criteria**:
- [ ] Both documents created and saved
- [ ] Domain-neutral language verified (no "computational social science" or "political analysis")
- [ ] No SOAR terminology present (absorbed into Discernus platform)
- [ ] Use case examples span 8+ domains (think tanks, journalists, corporations, religious organizations, etc.)
- [ ] Architecture requirements integrated (mathematical reliability, human-readable provenance)
- [ ] Agent drift prevention guidance included
- [ ] Quality criteria met for both documents

**Critical Validation Questions**:
- Would this prevent an agent from writing THICK code?
- Does this work for non-academic use cases?
- Are mathematical reliability requirements clear?
- Does this support the Drupal-style ecosystem vision?

---

## **üöÄ Phase 2: Core Infrastructure Documentation**

### **Execution Context**
**Duration**: 2-3 hours  
**Context Budget**: ~80KB  
**Dependencies**: Agent registry, model registry, gateway code  
**Risk**: Medium

### **Source Material Analysis**

#### **Document 1: discernus/core/agent_registry.yaml**
**Extract These Elements**:
- Agent archetypes and classification
- Execution method patterns
- Input/output contracts
- Dynamic discovery principles

#### **Document 2: discernus/gateway/model_registry.py + models.yaml**
**Extract These Elements**:
- Model selection algorithms
- Failover and routing logic
- Cost optimization strategies
- Rate limiting approaches

#### **Document 3: discernus/core/agent_roles.py**
**Extract These Elements**:
- Centralized prompt management
- Role-based agent patterns
- THIN compliance examples

### **Deliverable 1: docs/CORE_INFRASTRUCTURE_GUIDE.md**

**Document Template**:
```markdown
# Core Infrastructure Guide
*Agent Registry, Model Registry, and LLM Gateway*

## Agent Registry Architecture
[Extract from agent_registry.yaml structure]

## Model Registry and Intelligent Selection
[Extract from model_registry.py logic]

## LLM Gateway Routing
[Extract from llm_gateway.py patterns]

## Centralized Prompt Management
[Extract from agent_roles.py system]

## Chronolog and Provenance
[Extract from chronolog system]

## Security and Audit Trail
[Extract from security patterns]
```

### **Deliverable 2: docs/THIN_ARCHITECTURE_REFERENCE.md**

**Document Template**:
```markdown
# THIN Architecture Reference
*LLM Intelligence + Minimal Software*

## Core Philosophy
[Extract from Discernus Vision]

## THIN Patterns (Examples)
### Natural Language Flow
### Secure Code Execution
### Centralized Prompts
### Dynamic Agent Discovery

## THICK Anti-Patterns (Avoid)
### Extensive Parsing
### Hardcoded Intelligence
### Complex State Management
### Monolithic Architecture

## Mathematical Reliability
[Extract from secure executor patterns]

## Implementation Examples
[Code examples from existing agents]
```

**Quality Criteria**:
- ‚úÖ Infrastructure components clearly documented
- ‚úÖ THIN patterns with concrete examples
- ‚úÖ Anti-patterns clearly identified (extensive parsing, hardcoded intelligence, monolithic architecture)
- ‚úÖ Code examples from actual system
- ‚úÖ Mathematical reliability patterns demonstrated
- ‚úÖ Agent drift prevention examples included
- ‚úÖ Universal applicability emphasized in all examples
- ‚úÖ Length: 1200-1800 words combined

**Critical Themes Checklist**:
- ‚úÖ **Mathematical Reliability**: Secure code execution patterns and hybrid intelligence examples throughout
- ‚úÖ **Cost Transparency**: Model selection algorithms and budget control mechanisms documented
- ‚úÖ **Complete Reproducibility**: Chronolog system, calculation transparency, and provenance tracking fully explained with examples

---

## **üöÄ Phase 3: Developer Experience**

### **Execution Context**
**Duration**: 1-2 hours  
**Context Budget**: ~60KB  
**Dependencies**: Existing QUICK_START.md, validation rubrics  
**Risk**: Low

### **Deliverable 1: docs/QUICK_START_GUIDE.md**

**Replace**: Current QUICK_START.md  
**Transform**: Domain-specific examples to generic patterns  
**Add**: Multiple use case walkthroughs

### **Deliverable 2: docs/FRAMEWORK_INTEGRATION_GUIDE.md**

**Source**: discernus/core/framework_specification_validation_rubric.md  
**Transform**: Academic language to developer-friendly  
**Add**: Generic framework interface examples

---

## **üöÄ Phase 4: Extension Ecosystem**

### **Execution Context**
**Duration**: 1-2 hours  
**Context Budget**: ~40KB  
**Dependencies**: EXTENSION_GUIDE.md  
**Risk**: Medium

### **Deliverable 1: docs/EXTENSION_DEVELOPMENT_GUIDE.md**

**Source**: docs/EXTENSION_GUIDE.md  
**Enhance**: Drupal-style module patterns  
**Add**: API contracts and lifecycle management

### **Deliverable 2: docs/GOVERNANCE_PRINCIPLES.md**

**New Document**: Trademark and fork deterrence strategy  
**Add**: Community contribution guidelines  
**Include**: Quality standards and review processes

---

## **üöÄ Phase 5: User Experience** ‚úÖ

### **Execution Context**
**Duration**: 2-3 hours  
**Context Budget**: ~80KB  
**Dependencies**: Workflow patterns and interface design  
**Risk**: Medium

### **Deliverable 1: docs/USER_WORKFLOW_GUIDE.md** ‚úÖ

**Source**: User experience patterns and workflow optimization  
**Transform**: Technical capabilities into practical workflows  
**Add**: Three user types with optimized patterns

### **Deliverable 2: docs/INTERFACE_DESIGN_PRINCIPLES.md** ‚úÖ

**Source**: Interface design patterns and user experience principles  
**Add**: Human-centered design philosophy  
**Include**: Multi-modal interface standards and accessibility

---

## **üöÄ Phase 6: Quality Assurance**

### **Execution Context**
**Duration**: 1 hour  
**Context Budget**: ~30KB  
**Dependencies**: Testing patterns  
**Risk**: Low

### **Deliverable 1: docs/TESTING_STRATEGY.md**

**Source**: Existing test patterns  
**Generalize**: Framework-agnostic testing approaches  
**Add**: Quality gates and validation workflows

### **Deliverable 2: docs/TROUBLESHOOTING_GUIDE.md**

**Source**: Common issues from development  
**Add**: Performance optimization guides  
**Include**: Debug workflows and support resources

---

## **üöÄ Phase 6: Consolidation & Cleanup**

### **Execution Context**
**Duration**: 1-2 hours  
**Context Budget**: ~50KB  
**Dependencies**: All created documentation  
**Risk**: Low

### **Archive Strategy**
- Move `pm/soar/` to `deprecated/by-date/2025-01-XX/`
- Move domain-specific examples to `examples/academic/`
- Update all cross-references to new documentation

### **Deliverable 1: docs/README.md**

**Master Index**: Navigation for all documentation  
**Quality Check**: Link validation and consistency  
**User Journey**: Clear paths for different user types

---

## **üõ°Ô∏è Agent Protection Strategy**

### **Per-Phase Handoff Template**
```markdown
## Context for Agent [Phase X]

### Mission
[Specific phase mission from plan above]

### Key Principles
- THIN Architecture: LLM intelligence + minimal software
- Domain Neutral: Works for poli-sci, corporate, religious, etc.
- Drupal-Style: Core platform + extension ecosystem

### Your Focus
[Specific deliverables and constraints from plan]

### Source Material
[Exact files and sections to read]

### Content Transformations
[Domain-neutral language rules]

### Document Templates
[Specific section structure and requirements]

### Quality Criteria
[Measurable success criteria]

### Critical Themes Checklist
- [ ] **Mathematical Reliability**: Hybrid intelligence pattern (LLM designs ‚Üí secure code executes ‚Üí LLM interprets) prominently featured
- [ ] **Cost Transparency**: Upfront estimation, budget controls, and predictable pricing emphasized as institutional adoption enabler
- [ ] **Complete Reproducibility**: Zero mystery commitment, deterministic replication, calculation transparency, and interrogation-proof outputs as core platform principle

### Execution Steps
[Step-by-step process with time estimates]
```

### **Context Budget Management**
- **Strategic documents**: 40% of context
- **Technical examples**: 30% of context
- **Templates and patterns**: 20% of context
- **Working memory**: 10% of context

---

## **üìä Success Metrics**

### **Quantitative**
- [ ] Documentation coverage: 100% of core components
- [ ] Agent drift incidents: <10% of new contributors
- [ ] Onboarding time: <15 minutes to productivity
- [ ] Cross-references: 100% validated links

### **Qualitative**
- [ ] New contributors understand platform vision
- [ ] AI agents stay on THIN architecture
- [ ] Domain-neutral examples work across use cases
- [ ] Extension developers can build independently

---

## **üéØ Implementation Timeline**

**Total Duration**: 8-12 hours over 6 agent sessions  
**Risk Level**: Low-Medium (well-defined deliverables)  
**Dependencies**: None (phases are independent)

### **Recommended Schedule**
- **Week 1**: Phases 1-2 (Foundation + Infrastructure)
- **Week 2**: Phases 3-4 (Developer Experience + Extensions)
- **Week 3**: Phases 5-6 (Quality + Cleanup)

This plan provides the detailed execution guidance needed for fresh agents to successfully implement the documentation refactoring without additional context or clarification.

---

## **üî• Critical Success Factors**

### **For Fresh Agents: Remember These Key Points**

1. **This is foundational work** - not routine documentation cleanup
2. **Agent drift is a real problem** - your documentation must prevent THICK code
3. **Mathematical reliability is non-negotiable** - LLMs cannot be trusted for calculations
4. **Universal applicability is the goal** - not just academic research
5. **Drupal-style ecosystem is the vision** - core platform + extension modules

### **Agent Drift Prevention Checklist**
Before completing any phase, ask yourself:
- [ ] Would this prevent an agent from writing extensive parsing logic?
- [ ] Does this make THIN patterns easier than THICK patterns?
- [ ] Are the architecture requirements crystal clear?
- [ ] Would a corporate user understand this is for them too?
- [ ] Does this support the module ecosystem vision?

### **Context Handoff Template (Enhanced)**
```markdown
## Context for Agent [Phase X]

### **üéØ Mission Context**
You are implementing foundational documentation refactoring that prevents agent drift and enables the Drupal-style ecosystem vision. This work is critical to project success.

### **üö® Problems You're Solving**
- Agent drift crisis: Agents write THICK code despite briefings
- Strategic incoherence: Scattered PM documents create conflicting visions
- Mathematical reliability: LLMs cannot be trusted for calculations
- Universal applicability: Platform must work beyond academic use cases

### **üèóÔ∏è Architecture Requirements**
- Hybrid intelligence: LLM designs + secure code executes + LLM interprets
- Human-readable provenance: "Academic research archive" not "wall of code"
- THIN compliance: Natural language flow, centralized prompts, minimal parsing
- Domain-neutral: Think tanks, journalists, corporations, religious organizations

### **üéØ Your Focus**
[Specific deliverables and constraints from plan]

### **üìö Source Material**
[Exact files and sections to read]

### **üîÑ Content Transformations**
[Domain-neutral language rules]

### **üìã Quality Criteria**
[Measurable success criteria including architecture requirements]

### **üîç Critical Themes Checklist**
- [ ] **Mathematical Reliability**: Hybrid intelligence pattern (LLM designs ‚Üí secure code executes ‚Üí LLM interprets) prominently featured
- [ ] **Cost Transparency**: Upfront estimation, budget controls, and predictable pricing emphasized as institutional adoption enabler
- [ ] **Complete Reproducibility**: Zero mystery commitment, deterministic replication, calculation transparency, and interrogation-proof outputs as core platform principle

### **‚úÖ Success Validation**
[Critical validation questions for your specific phase]
```

This comprehensive plan addresses all critical gaps identified in the strategic review and provides fresh agents with the context needed to prevent drift and maintain architecture compliance. 