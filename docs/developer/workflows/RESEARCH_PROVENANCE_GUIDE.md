# Research Provenance Guide

**For Researchers Using Discernus**

## Quick Start: Automatic Research Preservation

**Every Discernus research run is automatically preserved with complete provenance.** You don't need to do anything special - just run your experiments and everything is saved for academic integrity.

### Default Behavior (Recommended)

```bash
# Run your experiment - everything preserved automatically
discernus run projects/my_study --analysis-only

# Output shows automatic preservation:
ğŸ“ Auto-committed to Git: Complete run 20250804T175152Z: my_study
âœ… Experiment completed successfully!
   ğŸ“‹ Run ID: 20250804T175152Z
   ğŸ“ Results: projects/my_study/runs/20250804T175152Z
```

**What happens automatically:**
- âœ… Complete run directory created with all results
- âœ… Comprehensive README generated for auditors
- âœ… All artifacts preserved with cryptographic integrity
- âœ… Run committed to Git with timestamp
- âœ… Validation script available for integrity checking

### Manual Control (If Preferred)

```bash
# Disable auto-commit for manual Git control
discernus run projects/my_study --no-auto-commit

# Manually commit when ready
git add projects/my_study/runs/20250804T175152Z
git commit -m "Analysis for paper submission"
```

## Understanding Your Research Provenance

### What Gets Preserved

**Every research run creates a complete academic package:**

```
projects/my_study/runs/20250804T175152Z/
â”œâ”€â”€ README.md                    # â† Start here for audit trail
â”œâ”€â”€ results/                     # â† Your research outputs
â”‚   â”œâ”€â”€ final_report.md         # Main findings
â”‚   â”œâ”€â”€ scores.csv              # Quantitative data
â”‚   â”œâ”€â”€ evidence.csv            # Supporting quotes
â”‚   â””â”€â”€ metadata.csv            # Complete provenance
â”œâ”€â”€ artifacts/                   # â† Complete transparency
â”‚   â”œâ”€â”€ analysis_results/       # What the AI actually said
â”‚   â”œâ”€â”€ statistical_results/    # Mathematical computations
â”‚   â”œâ”€â”€ evidence/               # How evidence was curated
â”‚   â””â”€â”€ inputs/                 # Framework and data used
â””â”€â”€ logs/                        # â† System execution record
    â”œâ”€â”€ llm_interactions.jsonl  # Complete AI conversations
    â”œâ”€â”€ system.jsonl            # System events
    â””â”€â”€ costs.jsonl             # API usage tracking
```

### Academic Value

**Citation-Ready Research:**
- **Permanent Identifier**: Git commit hash (e.g., `efba4a7b`)
- **Complete Methodology**: Auto-generated documentation
- **Reproducibility Package**: All inputs and parameters preserved
- **Audit Trail**: Every decision documented and traceable

**Example Citation:**
```
Analysis conducted using Civic Analysis Framework v7.1 applied to 2 political 
speeches. Computational analysis employed Gemini 2.5 Flash with validation 
correlation. Reproducibility materials: Git commit efba4a7b.
```

## Working with Provenance

### Validating Your Research

**Quick Integrity Check:**
```bash
# Verify everything is intact
python3 scripts/validate_run_integrity.py projects/my_study/runs/20250804T175152Z

# Output:
ğŸ‰ INTEGRITY VERIFICATION: PASSED
   This research run has consistent content-addressed integrity.
   All artifacts match expected hashes and are traceable.
```

**Detailed Validation:**
```bash
# See all validation steps
python3 scripts/validate_run_integrity.py projects/my_study/runs/20250804T175152Z --verbose

# Include Git history check
python3 scripts/validate_run_integrity.py projects/my_study/runs/20250804T175152Z --check-git
```

### Preparing for Peer Review

**Your research run includes everything reviewers need:**

1. **Start Point**: `README.md` - Complete audit guide
2. **Main Results**: `results/final_report.md` - Your findings
3. **Raw Data**: `results/*.csv` - All quantitative outputs
4. **Transparency**: `artifacts/` - What AI systems actually output
5. **Methodology**: `logs/llm_interactions.jsonl` - Complete AI conversations

**For External Reviewers:**
```bash
# Give reviewers this command for quick validation
python3 scripts/validate_run_integrity.py [your_run_path]

# They can also manually inspect
cat README.md                    # Audit guide
ls -la artifacts/               # Artifact structure
grep "cost_usd" logs/costs.jsonl # Resource usage
```

### Sharing Research

**Complete Reproducibility Package:**
```bash
# Your entire run directory is self-contained
tar -czf my_study_20250804T175152Z.tar.gz projects/my_study/runs/20250804T175152Z/

# Or share the Git repository with commit hash
git log --oneline | grep "20250804T175152Z"
efba4a7b Complete run 20250804T175152Z: my_study
```

**What collaborators get:**
- âœ… Exact framework and corpus used
- âœ… Complete AI interaction logs
- âœ… All mathematical computations
- âœ… Statistical validation data
- âœ… Comprehensive audit trail
- âœ… Validation tools for verification

## Academic Workflow Integration

### For Paper Writing

**Methodology Section Generation:**
```
Analysis conducted using [Framework Name] v[Version] applied to [N] documents. 
Computational analysis employed [AI Model] with cross-validation. 
Statistical reliability: [Metrics from statistical_results.csv].
Reproducibility materials: Git commit [hash] in [repository].
```

**Data Availability Statement:**
```
All analysis data, computational logs, and reproducibility materials are 
available in the research repository at [Git URL], commit [hash]. 
Validation tools are provided for independent verification.
```

### For Conference Presentations

**Transparency Slide:**
```
Research Transparency
â€¢ Complete computational audit trail preserved
â€¢ All AI interactions logged and available
â€¢ Statistical computations independently verifiable  
â€¢ Reproducibility package: [Git commit hash]
â€¢ Validation: python3 scripts/validate_run_integrity.py [path]
```

### For Thesis/Dissertation

**Appendix: Computational Methods**
- Include complete `final_report.md` 
- Reference specific Git commits for each analysis
- Provide validation commands for committee members
- Include methodology sections from auto-generated documentation

## Best Practices

### Organizing Multiple Studies

**Project Structure:**
```
projects/
â”œâ”€â”€ dissertation_study_1/
â”‚   â””â”€â”€ runs/
â”‚       â”œâ”€â”€ 20250801T143022Z/  # Pilot analysis
â”‚       â”œâ”€â”€ 20250804T175152Z/  # Main analysis  
â”‚       â””â”€â”€ 20250807T091234Z/  # Robustness check
â”œâ”€â”€ dissertation_study_2/
â”‚   â””â”€â”€ runs/
â”‚       â””â”€â”€ 20250810T164455Z/  # Cross-validation
â””â”€â”€ conference_paper/
    â””â”€â”€ runs/
        â””â”€â”€ 20250815T102233Z/  # Final analysis
```

**Git History Navigation:**
```bash
# Find all runs for a project
git log --oneline --grep="dissertation_study_1"

# See what changed between runs
git diff efba4a7b..a1b2c3d4 projects/dissertation_study_1/

# Tag important analyses
git tag -a "thesis-chapter-3" efba4a7b -m "Main analysis for Chapter 3"
```

### Quality Assurance

**Before Submission:**
1. **Validate All Runs**: `python3 scripts/validate_run_integrity.py [path] --check-git`
2. **Review README Files**: Ensure audit trails are complete
3. **Check Git History**: Verify all important runs are committed
4. **Test Reproducibility**: Try following your own methodology documentation

**Red Flags to Check:**
- âŒ Empty CSV files in results/
- âŒ Missing artifacts/ directories
- âŒ Broken symlinks in artifacts/
- âŒ Git commits missing for important runs
- âŒ Validation script failures

### Collaboration

**Sharing with Collaborators:**
```bash
# Share specific run
git add projects/shared_study/runs/20250804T175152Z
git commit -m "Add main analysis for review"
git push origin main

# Collaborators can validate
git pull
python3 scripts/validate_run_integrity.py projects/shared_study/runs/20250804T175152Z
```

**Working with Advisors:**
- Share Git repository with complete history
- Use commit hashes to reference specific analyses
- Provide validation commands for independent checking
- Include README files for non-technical reviewers

## Troubleshooting

### Common Issues

**"Auto-commit failed" Warning:**
```bash
# Usually harmless - check if run was actually saved
ls projects/my_study/runs/
python3 scripts/validate_run_integrity.py [run_path]

# If needed, commit manually
git add projects/my_study/runs/[run_id]
git commit -m "Manual commit for run [run_id]"
```

**Missing Results Files:**
```bash
# Check if analysis completed successfully
cat projects/my_study/runs/[run_id]/logs/system.jsonl | grep "error"

# Validate run integrity
python3 scripts/validate_run_integrity.py [run_path] --verbose
```

**Broken Symlinks:**
```bash
# Check shared cache
ls projects/my_study/shared_cache/artifacts/

# Validate and get details
python3 scripts/validate_run_integrity.py [run_path] --verbose
```

### Getting Help

**Self-Diagnosis:**
1. Read the run's `README.md` file
2. Run validation script with `--verbose` flag
3. Check `logs/system.jsonl` for errors
4. Verify Git status: `git status`

**Reporting Issues:**
- Include validation script output
- Provide Git commit hash if available
- Share relevant log excerpts from `logs/` directory
- Specify what you were trying to accomplish

## Advanced Usage

### Custom Validation

**Create Your Own Checks:**
```python
# Custom validation script
import json
from pathlib import Path

def validate_my_requirements(run_path):
    manifest = json.loads((run_path / "manifest.json").read_text())
    
    # Check custom requirements
    assert manifest["costs"]["total_cost_usd"] < 5.00
    assert len(manifest["corpus_files"]) >= 10
    # ... your validation logic
```

### Integration with Other Tools

**R/Python Analysis:**
```python
# Load Discernus results into your analysis
import pandas as pd

scores = pd.read_csv("projects/my_study/runs/20250804T175152Z/results/scores.csv")
evidence = pd.read_csv("projects/my_study/runs/20250804T175152Z/results/evidence.csv")

# Your statistical analysis here
# Results are automatically linked to Discernus provenance
```

**LaTeX Integration:**
```latex
\section{Computational Methods}
Analysis conducted using Discernus v2.0 with Civic Analysis Framework v7.1 
applied to \texttt{corpus\_files} political speeches. 
Complete reproducibility package available at Git commit 
\texttt{efba4a7b} with validation via 
\texttt{python3 scripts/validate\_run\_integrity.py}.
```

## Conclusion

The Discernus provenance system **automatically handles research transparency** so you can focus on your research questions. Every run creates a complete academic package that meets the highest standards for:

- âœ… **Peer review** - Complete audit trails
- âœ… **Replication** - All inputs and parameters preserved  
- âœ… **Academic integrity** - Tamper-evident provenance
- âœ… **Collaboration** - Self-contained reproducibility packages

**Just run your experiments** - everything else is handled automatically. When you're ready for peer review, submission, or collaboration, your complete provenance package is ready to go.