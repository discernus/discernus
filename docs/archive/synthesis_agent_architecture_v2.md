# Sequential Synthesis Agent Architecture v2.0
## A THIN Approach to Evidence-Based Computational Discourse Synthesis

### Executive Summary

The current `InvestigativeSynthesisAgent` violates THIN principles through experiment-specific hardcoding, brittle parsing logic, and improper RAG usage that conflates contextual information with lookup data. This document outlines a complete architectural redesign based on proven patterns: sequential prompting, clean data separation, and intelligent RAG utilization. This architecture is designed to be framework-agnostic, scalable from small-N to large-N studies, and maintain rigorous, verifiable provenance for all claims.

### 1. Core Principles & Architectural Constraints

This architecture is governed by three non-negotiable principles:

#### 1.1. Evidence-First Synthesis
Every interpretive claim or conclusion generated by the synthesis agent must be explicitly linked to at least one statistical result from the verified MathToolkit and at least one corresponding textual quote from the corpus. This principle must be enforced at both the prompt level and in the final validation of the report.

#### 1.2. Token-Budget Discipline
Every sequential step of the synthesis process must operate under a strict, measurable token budget. Ceilings will be defined and enforced for: a) the direct context provided to the LLM, b) the evidence retrieved from the RAG index, and c) the generated output for each step. This prevents context window overruns and ensures predictable, scalable performance.

#### 1.3. No-LM-Math Constraint
The Large Language Model (LLM) is prohibited from performing any mathematical computations, transformations, or aggregations. Its role is to reason qualitatively about the quantitative findings presented in the pre-computed, verified statistical tables from MathToolkit. The LLM can restate, cite, and interpret these findings, but never modify or derive them. This is critical for maintaining the integrity of the provenance chain.

### 2. The What: Clean Architecture for Scalable Synthesis

#### 2.1. Core Design Principles

1.  **Sequential Focus**: The LLM concentrates on one synthesis aspect at a time, following the proven pattern from CFF analysis to ensure high-quality, focused reasoning.
2.  **Context vs. Lookup Separation**: Essential, pan-synthesis context flows directly to the LLM. Situational, high-volume data is retrieved on demand via a specialized RAG index.
3.  **Framework & Experiment Agnostic**: The agent has zero hardcoded assumptions about the experiment's hypotheses, corpus content, speakers, or topics. It adapts its analysis based on the provided context.
4.  **Provenance Integrity**: Every quantitative claim in the final report is traceable to a specific, verifiable result from the MathToolkit.
5.  **THIN Coordination**: The software provides a minimal coordination layer, delegating all complex reasoning, query generation, and synthesis to the LLM.

#### 2.2. Data Flow Architecture

```
┌───────────────────────────┐      ┌───────────────────────────┐      ┌───────────────────────────┐
│      DIRECT CONTEXT       │      │      RAG LOOKUP INDEX     │      │      SYNTHESIS LLM        │
│ (Always available to LLM) │      │ (On-demand retrieval)     │      │                           │
│                           │      │                           │      │ • Sequential Step Logic   │
│ • Experiment Configuration│      │ • Evidence Quotes         │      │ • RAG Query Generation    │
│ • Framework Specification │─────▶│ • Full Corpus Text Chunks │◀─────│ • Pattern Analysis        │
│ • Corpus Manifest         │      │ • Verified Stat Tables    │      │ • Statistical Reasoning   │
│                           │      │ • Raw Scores Table        │      │ • Report Composition      │
│                           │      │ • Calculated Metrics      │      │                           │
└───────────────────────────┘      └───────────────────────────┘      └───────────────────────────┘
```

### 3. The Why: Problems with Current Implementation

The existing agent is unsuitable for production due to four critical anti-patterns:
1.  **Experiment Hardcoding**: Queries like `"Quote from McCain or Sanders speech"` make the agent brittle and single-use.
2.  **RAG Architecture Misuse**: The index is polluted with framework definitions, which drown out actual corpus evidence in search results.
3.  **Brittle Parsing Logic**: Using `content.split()` and other deterministic string manipulation to parse LLM output is unreliable and violates THIN principles.
4.  **Single-Call Optimization Fallacy**: A single, massive prompt creates cognitive overload and prevents the focused, step-by-step reasoning required for high-quality analysis.

### 4. The How: Implementation Blueprint

#### 4.1. Clean Data Separation: Context vs. Lookup

-   **Direct Context (To LLM):** A compact (<100KB) JSON object containing the essential information required for reasoning across all synthesis steps.
    -   `experiment`: Hypotheses, research questions.
    -   `framework`: Dimensions, methodology overview.
    -   `corpus`: Document list, speakers, high-level metadata.
    -   `statistics`: The complete, final set of verified statistical tables from MathToolkit (e.g., ANOVA, correlation matrices, reliability scores).

-   **RAG Index (For Lookup):** A specialized index containing high-volume lookup data. As of v2.0, the index excludes raw corpus text; the corpus manifest is provided via direct context and textual evidence is indexed instead.
    -   `evidence_quotes`: Textual evidence snippets with provenance.
    -   `raw_scores`: The full table of dimension scores.
    -   `calculated_metrics`: The full table of derived metrics (e.g., cohesion indices).

#### 4.2. RAG Index & Curation Strategy (txtai Integration)

To be effective, the RAG index must be built with a clear strategy.

-   **Chunking Strategy:**
    -   `evidence_quotes`: Stored as exact character spans with a surrounding 1-2 sentence window for context.

-   **Index Schema:** Each item in the `txtai` index must include this metadata for filtering:
    -   `id`: Unique identifier for the chunk.
    -   `text`: The content.
    -   `content_type`: One of `['evidence_quotes', 'raw_scores', 'calculated_metrics']`.
    -   `source_artifact`: The hash of the artifact it originated from.
    -   `document_id`: The name or ID of the source document (if applicable).
    -   `speaker`: The speaker associated with the document (if applicable).

-   **Filtering & Hybrid Search:**
    -   All RAG queries *must* use `txtai`'s `where` clause to filter by `content_type` (e.g., `where="content_type = 'evidence_quotes'"`). This is the primary mechanism for solving the "framework pollution" problem.
    -   Employ a hybrid search approach (e.g., BM25 for keyword relevance + embeddings for semantic similarity) to improve retrieval precision.

#### 4.3. Statistical Table Contract (MathToolkit Integration)

MathToolkit must produce a standardized, LLM-friendly JSON output. This eliminates the need to pass raw `pandas` or `numpy` objects and provides data in a format optimized for direct inclusion in prompts.

```json
{
  "anova_summary": {
    "table_format": "ANOVA Results Table",
    "headers": ["Dimension", "F-Statistic", "P-Value", "Significant"],
    "rows": [
      ["tribal_dominance", 29.4, "<0.001", true],
      ["individual_dignity", 192.6, "<0.001", true]
    ],
    "interpretation": "Strong statistical differences found across all dimensions."
  },
  "reliability_summary": {
    "table_format": "Measurement Reliability Table",
    "headers": ["Dimension", "Cronbach's Alpha", "95% CI", "Interpretation"],
    "rows": [
      ["Tribal Dominance", 0.82, "[0.71, 0.90]", "Excellent"],
      ["Envy", 0.64, "[0.45, 0.78]", "Questionable"]
    ]
  },
  "descriptive_summary": {
      "table_format": "Descriptive Statistics",
      "headers": ["Dimension", "Mean", "Std Dev", "Min", "Max"],
      "rows": [
          ["tribal_dominance", 0.43, 0.21, 0.1, 0.9]
      ]
  }
}
```

#### 4.4. Externalized Prompting Framework (YAML)

All prompts will be defined in a `synthesis_prompts.yaml` file, allowing for easy iteration.

```yaml
# synthesis_prompts.yaml
sequential_synthesis_template: |
  You are a computational social scientist conducting a discourse analysis synthesis. Adhere to the following constraints:
  1.  **Evidence-First**: Every claim must be supported by citing both statistical results and textual evidence.
  2.  **No-LM-Math**: You must not perform calculations. Reason about the provided statistical tables as definitive facts.

  Work through the following step, and only this step.

  CONTEXT:
  {direct_context}

  EVIDENCE (if applicable for this step):
  {retrieved_evidence}

  TASK: {task_description}

step_definitions:
  hypothesis_testing:
    task_description: "First, generate 3-5 specific evidence queries to validate the statistical patterns in the ANOVA table. Then, based on the retrieved evidence, for each hypothesis, state whether it is SUPPORTED, NOT SUPPORTED, or INCONCLUSIVE, citing the specific statistical findings and textual quotes that justify your conclusion."
  # ... other steps defined here
```

#### 4.5. The Sequential Synthesis Pipeline

The agent will execute a multi-step process, with each step building on the last.

-   **Step 1: Hypothesis Testing**: The LLM generates targeted queries to find evidence for the key findings in the statistical tables. It then uses this evidence to confirm or reject the experiment's hypotheses.
-   **Step 2: Statistical Anomaly Investigation**: The LLM examines the statistical tables for surprising or anomalous results (e.g., a dimension with an unexpectedly high F-statistic or low reliability) and generates queries to investigate them.
-   **Step 3: Cross-Dimensional Pattern Discovery**: The LLM looks at the correlation matrix and other results to find relationships *between* dimensions, generating queries for evidence of those patterns.
-   **Step 4: Statistical Framework Fit Assessment (Tiered Approach)**: The LLM performs the most sophisticated fit assessment possible given the available data:
    -   **Gold Standard (ANOVA + Reliability available):** Assesses fit using both significance and consistency.
    -   **Silver Standard (Reliability only):** Assesses fit based on the consistency of measurement.
    -   **Bronze Standard (Descriptive Stats only):** Assesses fit using score variance and LLM confidence as proxies.
-   **Step 5: Final Integration**: The LLM synthesizes the findings from all previous steps into a single, coherent academic report, including all required sections (Executive Summary, Methodology, Results, Framework Fit, Limitations, etc.).

#### 4.6. Evidence Budgeting & Selection Policy

To avoid context window overload and ensure evidence quality, the agent will use a smart selection policy instead of arbitrary limits (e.g., "top 3 results").
-   **Token Budget:** Admit evidence into the context based on a pre-defined token budget (e.g., 4000 tokens), not a fixed number of items.
-   **Relevance First:** Prioritize evidence with the highest retrieval scores.
-   **Diversity via MMR:** Use a Maximal Marginal Relevance (MMR) algorithm to re-rank the top results, ensuring a diversity of topics and speakers to avoid redundant evidence.
-   **De-duplication:** Hash evidence quotes to prevent the same passage from being included multiple times.

### Curation Agent Review Requirements

The `RAGIndexManager` (replacing `ComprehensiveKnowledgeCurator`) needs architectural alignment:

#### Current Issues
1. **Equal Content Weighting**: Framework sections get same weight as speech quotes
2. **No Content-Type Filtering**: Can't specify "corpus only" or "evidence only" searches  
3. **Over-Granular Framework Indexing**: 20+ framework sections vs 2 corpus documents

#### Required Enhancements
```python
class OptimizedKnowledgeCurator:
    """Enhanced curator with proper content separation and weighting."""
    
    def build_evidence_index(self, request: EvidenceIndexRequest):
        """Build RAG index for evidence lookup only."""
        
        evidence_documents = []
        
        # Index evidence quotes with high weight
        for quote in request.evidence_quotes:
            evidence_documents.append({
                "id": f"evidence_{quote.id}",
                "text": f"Evidence quote from {quote.speaker}: {quote.text}",
                "content_type": "evidence_quotes",
                "weight": 2.0  # Higher weight for evidence
            })
        
        # Index corpus text with medium weight  
        for document in request.corpus_documents:
            evidence_documents.append({
                "id": f"corpus_{document.id}",
                "text": document.full_text,
                "content_type": "corpus_text", 
                "weight": 1.5  # Medium weight for corpus
            })
        
        # Index scores/metrics with lower weight for query access
        for metric in request.calculated_metrics:
            evidence_documents.append({
                "id": f"metric_{metric.id}",
                "text": f"{metric.name}: {metric.value} for {metric.document}",
                "content_type": "calculated_metrics",
                "weight": 1.0  # Lower weight, queryable for specific lookups
            })
    
    def query_evidence(self, query: EvidenceQuery) -> List[EvidenceResult]:
        """Query evidence with content-type filtering."""
        
        # Apply content-type filtering
        search_filter = {"content_type": query.content_types} if query.content_types else {}
        
        results = self.embeddings.search(
            query.semantic_query,
            limit=query.limit,
            where=search_filter  # txtai supports metadata filtering
        )
        
        return self._convert_to_evidence_results(results)
```

### Math and Stats Package Requirements

The statistical output format needs optimization for LLM consumption:

#### Current MathToolkit Output Format
```python
# Current: Raw pandas/numpy objects
{
    'anova_results': DataFrame(...),
    'correlation_matrix': numpy.array(...),
    'statistical_tests': {...}
}
```

#### Required: LLM-Optimized Statistical Tables
```python
class StatisticalResultsFormatter:
    """Format MathToolkit results for direct LLM consumption."""
    
    def format_for_synthesis(self, raw_results: Dict) -> Dict:
        """Convert raw statistical results to LLM-friendly tables."""
        
        return {
            "anova_summary": {
                "table_format": "ANOVA Results Table",
                "headers": ["Dimension", "F-Statistic", "P-Value", "Significant", "Effect Size"],
                "rows": [
                    ["tribal_dominance", "29.4", "<0.001", "Yes", "Large"],
                    ["individual_dignity", "192.6", "<0.001", "Yes", "Large"],
                    # ... more rows
                ],
                "interpretation": "Strong statistical differences found across all dimensions"
            },
            
            "correlation_summary": {
                "table_format": "Correlation Matrix", 
                "significant_correlations": [
                    {"dimensions": "envy-enmity", "correlation": 0.73, "p_value": "<0.001"},
                    {"dimensions": "fear-enmity", "correlation": 0.62, "p_value": "<0.01"}
                ],
                "interpretation": "Strong positive correlations between fragmentative dimensions"
            },
            
            "key_findings": [
                "All dimensions show statistical significance (p<0.001)",
                "Individual dignity shows strongest effect (F=192.6)", 
                "Fragmentative dimensions cluster together (r>0.6)"
            ]
        }
```

### 5. Provenance & Audit Logging

To ensure full reproducibility, the system will maintain a rigorous, append-only audit log for each run.
-   **Log Everything:** The log will capture the full context for each step: the exact prompt sent to the LLM, the model ID and temperature settings, every RAG query generated, and the full list of retrieved evidence with their IDs and relevance scores.
-   **Chain of Custody:** The final report will include a "References" appendix that explicitly links every claim to the specific statistical table and evidence quote IDs that support it, allowing for a complete, verifiable chain of custody from data to conclusion.

### Implementation Roadmap

#### Phase 1: Clean RAG Architecture (Week 1)
1. Modify `RAGIndexManager` for content-type filtering
2. Separate framework definitions from evidence index
3. Implement evidence-focused indexing with proper weighting
4. Test query effectiveness with new architecture

#### Phase 2: Sequential Synthesis Agent (Week 2)
1. Create new `SequentialSynthesisAgent` class
2. Implement externalized YAML prompt templates
3. Build query generation based on statistical patterns
4. Test with existing simple_test experiment

#### Phase 3: MathToolkit Integration (Week 3)
1. Add LLM-optimized formatting to statistical outputs
2. Create provenance-linked statistical tables
3. Test end-to-end pipeline with clean data flow
4. Validate academic report quality

#### Phase 4: Production Integration (Week 4)
1. Integrate with existing `UnifiedSynthesisAgent`
2. Add comprehensive error handling and audit logging
3. Performance testing with larger corpora
4. Documentation and training materials

### Success Metrics

#### Technical Metrics
- **RAG Precision**: >80% of queries return relevant evidence (not framework definitions)
- **Context Efficiency**: <100KB total context per synthesis (vs current >500KB)
- **Provenance Coverage**: 100% of quantitative findings trace to MathToolkit
- **Query Effectiveness**: Evidence quotes appear in >90% of final reports

#### Research Quality Metrics  
- **Hypothesis Support**: Clear SUPPORTED/NOT SUPPORTED conclusions with statistical backing
- **Evidence Integration**: Every statistical finding backed by textual evidence
- **Academic Rigor**: Reports meet peer-review standards for discourse analysis
- **Framework Agnosticity**: Agent works with any CFF-compatible framework
- **Framework Fit Assessment**: The final report includes a quantitative assessment of the framework's explanatory power and limitations for the given corpus.

### Conclusion

This architecture resolves the fundamental issues with the current synthesis approach:

1. **THIN Compliance**: LLM intelligence drives reasoning; minimal software coordination
2. **Scalability**: Clean separation allows scaling to thousands of documents  
3. **Provenance Integrity**: All numbers trace to verified calculations
4. **Framework Agnosticity**: Zero hardcoded assumptions about experiments

The sequential prompting approach follows proven patterns from CFF analysis, ensuring focused cognitive load and comprehensive coverage. The clean data separation eliminates RAG pollution while maintaining intelligent evidence retrieval.

This represents a mature, production-ready architecture for computational discourse synthesis that maintains academic rigor while scaling to enterprise research requirements.
