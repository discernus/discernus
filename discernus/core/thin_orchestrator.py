#!/usr/bin/env python3
"""
THIN Orchestrator v2.0 for Discernus
====================================

THIN v2.0 orchestrator implementing direct function calls instead of Redis coordination.
Coordinates the simplified 2-agent pipeline: Enhanced Analysis ‚Üí Enhanced Synthesis

Key THIN v2.0 principles:
- Direct Python function calls (no Redis coordination)
- LLM intelligence for complex reasoning
- Minimal software coordination
- Perfect caching through content-addressable storage
- Complete audit trails for academic integrity
"""

import json
import time
import yaml
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Any, List, Optional
import hashlib # Added for framework hash calculation

from .security_boundary import ExperimentSecurityBoundary, SecurityError
from .audit_logger import AuditLogger
from .local_artifact_storage import LocalArtifactStorage
from .enhanced_manifest import EnhancedManifest
from ..agents.EnhancedAnalysisAgent.main import EnhancedAnalysisAgent

# Import THIN Synthesis Pipeline for enhanced synthesis
from ..agents.thin_synthesis.orchestration.pipeline import (
    ProductionThinSynthesisPipeline, 
    ProductionPipelineRequest,
    ProductionPipelineResponse
)


class ThinOrchestratorError(Exception):
    """THIN orchestrator specific exceptions"""
    pass


class ThinOrchestrator:
    """
    THIN v2.0 orchestrator implementing direct function call coordination.
    
    Simplified 2-agent pipeline:
    1. Enhanced Analysis Agent (with mathematical validation)
    2. Enhanced Synthesis Agent (with mathematical spot-checking)
    
    Key features:
    - Direct function calls (no Redis)
    - Security boundary enforcement
    - Complete audit trails
    - Perfect caching for restart=resume
    - Enhanced mathematical validation
    """
    
    def __init__(self, experiment_path: Path):
        """
        Initialize THIN orchestrator for an experiment.
        
        Args:
            experiment_path: Path to experiment directory (containing experiment.md)
        """
        self.experiment_path = Path(experiment_path).resolve()
        
        # Initialize security boundary
        self.security = ExperimentSecurityBoundary(self.experiment_path)
        
        print(f"üéØ THIN Orchestrator v2.0 initialized for: {self.security.experiment_name}")
        
    def _create_thin_synthesis_pipeline(self, audit_logger: AuditLogger, storage: LocalArtifactStorage, model: str, debug_agent: Optional[str] = None, debug_level: str = "info") -> ProductionThinSynthesisPipeline:
        """Create a ProductionThinSynthesisPipeline with proper infrastructure."""
        
        # Create MinIO-compatible wrapper for LocalArtifactStorage
        class MinIOCompatibleStorage:
            def __init__(self, local_storage):
                self.local_storage = local_storage
                
            def put_artifact(self, content: bytes):
                return self.local_storage.put_artifact(content, {})
                
            def get_artifact(self, hash_id: str):
                return self.local_storage.get_artifact(hash_id)
        
        compatible_storage = MinIOCompatibleStorage(storage)
        
        return ProductionThinSynthesisPipeline(
            artifact_client=compatible_storage,
            audit_logger=audit_logger,
            model=model
        )

    def _run_thin_synthesis(self,
                           scores_hash: str,
                           evidence_hash: str,
                           framework_content: str,
                           experiment_config: Dict[str, Any],
                           model: str,
                           audit_logger: AuditLogger,
                           storage: LocalArtifactStorage,
                           framework_hash: str,
                           corpus_hash: str,
                           corpus_manifest: Optional[Dict[str, Any]] = None,
                           debug_agent: Optional[str] = None,
                           debug_level: str = "info") -> Dict[str, Any]:
        """
        Run synthesis using the new THIN Code-Generated Synthesis Architecture.
        
        Returns results in the same format as EnhancedSynthesisAgent for compatibility.
        """
        
        # Create THIN synthesis pipeline
        pipeline = self._create_thin_synthesis_pipeline(audit_logger, storage, model, debug_agent, debug_level)
        
        # Build comprehensive experiment context for ResultsInterpreter (THIN approach)
        experiment_context = self._build_comprehensive_experiment_context(experiment_config, framework_content, corpus_manifest)
        
        # Create pipeline request
        request = ProductionPipelineRequest(
            framework_spec=framework_content,
            scores_artifact_hash=scores_hash,
            evidence_artifact_hash=evidence_hash,
            experiment_context=experiment_context,
            max_evidence_per_finding=3,
            min_confidence_threshold=0.7,
            interpretation_focus="comprehensive",
            # Add provenance context (Issue #208 fix)
            framework_hash=framework_hash,
            corpus_hash=corpus_hash,
            framework_name=experiment_config.get('framework', 'Unknown framework'),
            corpus_manifest=corpus_manifest
        )
        
        # Execute pipeline
        start_time = time.time()
        response = pipeline.run(request)
        duration_seconds = time.time() - start_time
        
        if response.success:
            # Store the narrative report as an artifact for compatibility
            report_hash = storage.put_artifact(
                response.narrative_report.encode('utf-8'),
                {"artifact_type": "synthesis_report", "pipeline": "thin_architecture"}
            )
            
            # Return in EnhancedSynthesisAgent format for compatibility
            return {
                "result_hash": report_hash,
                "duration_seconds": duration_seconds,
                "synthesis_confidence": 0.95,  # THIN architecture generally high confidence
                "synthesis_report_markdown": response.narrative_report,
                
                # Additional THIN-specific metadata
                "thin_metadata": {
                    "pipeline_version": "production_v1.0",
                    "stage_timings": response.stage_timings,
                    "stage_success": response.stage_success,
                    "word_count": response.word_count,
                    "generated_artifacts": {
                        "analysis_plan": response.analysis_plan_hash,
                        "statistical_results": response.statistical_results_hash,
                        "curated_evidence": response.curated_evidence_hash
                    }
                }
            }
        else:
            # DEVELOPMENT: Fail fast instead of expensive fallbacks during debugging
            error_msg = f"THIN synthesis failed: {response.error_message}"
            print(f"‚ùå {error_msg}")
            print("üõë Stopping execution to allow debugging (no expensive fallback)")
            raise ThinOrchestratorError(error_msg)

    def run_experiment(self, 
                      analysis_model: str = "vertex_ai/gemini-2.5-flash-lite",
                      synthesis_model: str = "vertex_ai/gemini-2.5-pro",
                      synthesis_only: bool = False,
                      analysis_only: bool = False,
                      resume_stage: Optional[str] = None,
                      debug_agent: Optional[str] = None,
                      debug_level: str = "info") -> Dict[str, Any]:
        """
        Run experiment with enhanced agents and stage control for targeted debugging.
        
        Args:
            analysis_model: LLM model to use for analysis
            synthesis_model: LLM model to use for synthesis
            synthesis_only: If True, skip analysis and run synthesis on existing CSVs
            analysis_only: If True, run only analysis phase and save artifacts
            resume_stage: Resume at specific THIN synthesis sub-stage ('thin-gen', 'thin-exec', 'thin-cure', 'thin-interp')
            
        Returns:
            Experiment results with mathematical validation
        """
        start_time = datetime.now(timezone.utc).isoformat()
        
        # Create timestamped run folder
        run_timestamp = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")
        run_folder = self.experiment_path / "runs" / run_timestamp
        
        try:
            # Initialize run infrastructure
            self.security.secure_mkdir(run_folder)
            
            # Initialize audit logging
            audit = AuditLogger(self.security, run_folder)
            
            # Initialize experiment-level shared cache for perfect THIN caching
            shared_cache_dir = self.experiment_path / "shared_cache"
            self.security.secure_mkdir(shared_cache_dir)
            storage = LocalArtifactStorage(self.security, shared_cache_dir)
            
            # Initialize enhanced manifest
            manifest = EnhancedManifest(self.security, run_folder, audit, storage)
            
            audit.log_orchestrator_event("experiment_start", {
                "experiment_path": str(self.experiment_path),
                "run_folder": str(run_folder),
                "model": analysis_model,
                "synthesis_only": synthesis_only,
                "architecture": "thin_v2.0_direct_calls"
            })
            
            print(f"üöÄ Starting THIN v2.0 experiment: {run_timestamp}")
            
            # Load and validate experiment configuration
            experiment_config = self._load_experiment_config()
            manifest.set_run_metadata(
                experiment_config["name"], 
                str(self.experiment_path),
                "thin_v2.0_alpha"
            )
            manifest.set_experiment_config(experiment_config)
            
            # Load framework
            framework_content = self._load_framework(experiment_config["framework"])
            framework_hash = storage.put_artifact(
                framework_content.encode('utf-8'),
                {"artifact_type": "framework", "original_filename": experiment_config["framework"]}
            )
            manifest.add_input_artifact("framework", framework_hash, {
                "filename": experiment_config["framework"],
                "size_bytes": len(framework_content)
            })
            
            # Load corpus documents and manifest
            corpus_documents, corpus_manifest = self._load_corpus(experiment_config["corpus_path"])
            corpus_hashes = []
            corpus_metadata = []
            
            for doc in corpus_documents:
                doc_hash = storage.put_artifact(
                    doc["content"].encode('utf-8') if isinstance(doc["content"], str) else doc["content"],
                    {"artifact_type": "corpus_document", "original_filename": doc["filename"]}
                )
                corpus_hashes.append(doc_hash)
                corpus_metadata.append({
                    "filename": doc["filename"],
                    "size_bytes": len(doc["content"]) if isinstance(doc["content"], str) else len(doc["content"])
                })
            
            manifest.add_corpus_artifacts(corpus_hashes, corpus_metadata)
            
            audit.log_orchestrator_event("inputs_loaded", {
                "framework_hash": framework_hash,
                "corpus_documents": len(corpus_documents),
                "total_input_artifacts": len(corpus_hashes) + 1
            })

            # Handle analysis-only mode: run analysis and exit early
            if analysis_only:
                print("üîç Analysis-only mode: Running analysis and saving artifacts for later synthesis...")
                
                # Execute analysis phase
                analysis_agent = EnhancedAnalysisAgent(self.security, audit, storage)
                all_analysis_results, scores_hash, evidence_hash = self._execute_analysis_sequentially(
                    analysis_agent,
                    corpus_documents,
                    framework_content,
                    experiment_config,
                    analysis_model
                )
                
                # Check if analysis succeeded
                successful_analyses = [res for res in all_analysis_results if res.get('analysis_result', {}).get('result_hash')]
                if not successful_analyses:
                    raise ThinOrchestratorError("Analysis failed. No artifacts saved.")
                
                # Create results directory and save artifact references
                results_dir = run_folder / "results"
                self.security.secure_mkdir(results_dir)
                
                analysis_summary = {
                    "mode": "analysis_only",
                    "scores_hash": scores_hash,
                    "evidence_hash": evidence_hash,
                    "analysis_results_count": len(successful_analyses),
                    "available_for_synthesis": True
                }
                
                with open(results_dir / "analysis_summary.json", "w") as f:
                    json.dump(analysis_summary, f, indent=2)
                
                print(f"‚úÖ Analysis completed - artifacts saved for synthesis:")
                print(f"   - Scores: {scores_hash[:12]}...")
                print(f"   - Evidence: {evidence_hash[:12]}...")
                print(f"   - Ready for: discernus run --synthesis-only or --stage commands")
                
                # Update manifest and return
                end_time = datetime.now(timezone.utc).isoformat()
                manifest.add_execution_stage(
                    stage_name="analysis",
                    agent_name="EnhancedAnalysisAgent", 
                    start_time=start_time,
                    end_time=end_time,
                    status="completed",
                    metadata={"model": analysis_model, "mode": "analysis_only"}
                )
                manifest.finalize_manifest()
                
                return {
                    "run_id": run_timestamp,
                    "status": "analysis_completed",
                    "scores_hash": scores_hash,
                    "evidence_hash": evidence_hash,
                    "duration": self._calculate_duration(start_time, end_time)
                }
            
            # Handle resume from specific THIN synthesis stage
            if resume_stage:
                print(f"‚è© Resume mode: Starting from THIN synthesis stage '{resume_stage}'...")
                
                # Validate that we can resume (artifacts exist)
                shared_cache_dir = self.experiment_path / "shared_cache" / "artifacts" 
                if not shared_cache_dir.exists():
                    raise ThinOrchestratorError(f"No shared cache found for resume mode. Run analysis first.")
                
                # TODO: Add stage-specific artifact validation and resumption logic
                # For now, fall back to synthesis_only behavior for stages
                print(f"‚ö†Ô∏è  Stage-specific resumption not yet implemented. Using full synthesis...")
                synthesis_only = True  # Temporary fallback

            if synthesis_only:
                # Find latest run with complete analysis
                shared_cache_dir = self.experiment_path / "shared_cache" / "artifacts"
                if not shared_cache_dir.exists():
                    raise ThinOrchestratorError("No shared cache found for synthesis-only mode")
                
                # Create results directory for new run
                results_dir = run_folder / "results"
                self.security.secure_mkdir(results_dir)
                
                # Load artifact registry to find CSVs
                registry_file = shared_cache_dir / "artifact_registry.json"
                if not registry_file.exists():
                    raise ThinOrchestratorError("Artifact registry not found")
                
                with open(registry_file) as f:
                    registry = json.load(f)
                
                # Calculate current framework hash for provenance validation (Issue #208)
                current_framework_content = self._load_framework(experiment_config["framework"])
                current_framework_hash = hashlib.sha256(current_framework_content.encode('utf-8')).hexdigest()
                
                # Find latest JSON analysis artifact that matches current framework
                json_artifact_hash = None
                latest_json_time = None
                
                for artifact_id, info in registry.items():
                    metadata = info.get("metadata", {})
                    artifact_type = metadata.get("artifact_type")
                    artifact_framework_hash = metadata.get("framework_hash")
                    
                    # CRITICAL: Only consider artifacts from the same framework (Issue #208 fix)
                    # For existing artifacts without framework_hash, allow them (backward compatibility)
                    if artifact_framework_hash and artifact_framework_hash != current_framework_hash:
                        continue
                        
                    if artifact_type == "analysis_json_v6":
                        # JSON artifact contains both scores and evidence
                        timestamp = info["created_at"]
                        if not latest_json_time or timestamp > latest_json_time:
                            latest_json_time = timestamp
                            json_artifact_hash = artifact_id
                
                if json_artifact_hash:
                    # Use JSON artifact for both scores and evidence
                    scores_hash = json_artifact_hash
                    evidence_hash = json_artifact_hash
                    
                    print(f"üìä Using existing JSON analysis from shared cache")
                    print(f"   - Combined JSON: {json_artifact_hash[:8]}... ({latest_json_time})")
                    print(f"   - Framework: {current_framework_hash[:12]}... (provenance validated ‚úÖ)")
                else:
                    # Log detailed information about framework matching for debugging
                    framework_artifacts_found = []
                    for artifact_id, info in registry.items():
                        metadata = info.get("metadata", {})
                        if metadata.get("artifact_type") == "analysis_json_v6":
                            framework_artifacts_found.append({
                                "artifact_id": artifact_id[:12],
                                "type": metadata.get("artifact_type"),
                                "framework_hash": metadata.get("framework_hash", "MISSING")[:12],
                                "timestamp": info.get("created_at")
                            })
                    
                    print(f"‚ùå No analysis artifacts found matching current framework")
                    print(f"   Current framework hash: {current_framework_hash[:12]}...")
                    print(f"   Available artifacts: {len(framework_artifacts_found)}")
                    for artifact in framework_artifacts_found:
                        print(f"     - {artifact['type']}: {artifact['artifact_id']}... (fw: {artifact['framework_hash']}...)")
                    
                    raise ThinOrchestratorError(
                        f"No analysis artifacts found for current framework (hash: {current_framework_hash[:12]}...). "
                        f"Found {len(framework_artifacts_found)} artifacts from other frameworks. "
                        "This indicates framework provenance is working correctly - "
                        "run full analysis to generate artifacts for this framework."
                    )
                
                # Copy JSON artifact to new run for reference
                import shutil
                shutil.copy2(shared_cache_dir / json_artifact_hash, results_dir / "analysis.json")
                
                print(f"üìä Using existing analysis from shared cache")
                print(f"   - Combined JSON: {json_artifact_hash[:8]}... ({latest_json_time})")
                print(f"   - Framework: {current_framework_hash[:12]}... (provenance validated ‚úÖ)")
                
                # Load framework and corpus for synthesis context (even in synthesis-only mode)
                framework_content = self._load_framework(experiment_config["framework"])
                _, corpus_manifest = self._load_corpus(experiment_config["corpus_path"])
                
                # Run synthesis only
                print("üè≠ Using THIN Code-Generated Synthesis Architecture...")
                
                # Calculate framework hash for provenance
                framework_hash = hashlib.sha256(framework_content.encode('utf-8')).hexdigest()
                
                # Calculate corpus hash for complete provenance context
                corpus_content = ''.join([doc.get('filename', '') + str(doc.get('content', '')) for doc in corpus_documents])
                corpus_hash = hashlib.sha256(corpus_content.encode('utf-8')).hexdigest()
                
                synthesis_result = self._run_thin_synthesis(
                    scores_hash=scores_hash,
                    evidence_hash=evidence_hash,
                    framework_content=framework_content,
                    experiment_config=experiment_config,
                    model=synthesis_model,
                    audit_logger=audit,
                    storage=storage,
                    # Add provenance context (Issue #208 fix)
                    framework_hash=framework_hash,
                    corpus_hash=corpus_hash,
                    corpus_manifest=corpus_manifest,  # THIN approach: pass raw corpus content
                    debug_agent=debug_agent,
                    debug_level=debug_level
                )
                
                if not synthesis_result or not isinstance(synthesis_result, dict):
                    raise ThinOrchestratorError(f"Invalid synthesis result format: {type(synthesis_result)}")
                
                if "synthesis_report_markdown" not in synthesis_result:
                    raise ThinOrchestratorError("Missing synthesis_report_markdown in result")
                
                # Generate final report
                final_report = synthesis_result["synthesis_report_markdown"]
                
                # Save final report
                with open(results_dir / "final_report.md", "w") as f:
                    f.write(final_report)
                
                # Update manifest
                end_time = datetime.now(timezone.utc).isoformat()
                manifest.add_execution_stage(
                    stage_name="synthesis",
                    agent_name="EnhancedSynthesisAgent",
                    start_time=start_time,
                    end_time=end_time,
                    status="completed",
                    metadata={"model": synthesis_model}
                )
                manifest.finalize_manifest()
                
                return {
                    "run_id": run_timestamp,
                    "status": "completed",
                    "duration": self._calculate_duration(start_time, end_time)
                }
            
            # Normal full run - continue with analysis phase
            # Phase 1: Batch Planning and Enhanced Analysis with Context Window Management
            # CONTEXT_WINDOW_MANAGEMENT: This entire section can be removed when LLM context windows become unlimited
            batch_planning_start_time = datetime.now(timezone.utc).isoformat()
            manifest.add_execution_stage("batch_planning", "BatchPlannerAgent", batch_planning_start_time)
            
            # Create intelligent batch plan with production cost transparency
            # batch_planner = BatchPlannerAgent(self.security, audit)
            # batch_plan = batch_planner.create_batches(
            #     framework_content=framework_content,
            #     corpus_documents=corpus_documents,
            #     model=model
            # )
            
            batch_planning_end_time = datetime.now(timezone.utc).isoformat()
            manifest.add_execution_stage("batch_planning", "BatchPlannerAgent",
                                       batch_planning_start_time, batch_planning_end_time, "completed", {
                "total_batches": 0, # No batches in this new flow
                "total_estimated_cost": 0.0,
                "total_estimated_tokens": 0,
                "context_window_limit": 0
            })
            
            print(f"üí∞ Total estimated cost: ${0:.4f}")
            print(f"üìä Batch plan: 0 batches, "
                  f"‚è±Ô∏è ~{0:.1f} minutes")
            
            # Initialize analysis and synthesis agents
            analysis_agent = EnhancedAnalysisAgent(self.security, audit, storage)
            
            # Execute analysis (in chunks)
            all_analysis_results, scores_hash, evidence_hash = self._execute_analysis_sequentially(
                analysis_agent,
                corpus_documents,
                framework_content,
                experiment_config,
                analysis_model
            )

            # Check if any analysis tasks succeeded
            successful_analyses = [res for res in all_analysis_results if res.get('analysis_result', {}).get('result_hash')]
            if not successful_analyses:
                raise ThinOrchestratorError("All analysis batches failed. Halting experiment.")

            # Execute synthesis
            print("\nüî¨ Synthesizing results...")
            synthesis_start_time = datetime.now(timezone.utc).isoformat()
            
            print("üè≠ Using THIN Code-Generated Synthesis Architecture...")
            print(f"DEBUG: Passing scores_hash={scores_hash}, evidence_hash={evidence_hash} to THIN pipeline.")
            
            # Calculate framework hash for provenance
            framework_hash = hashlib.sha256(framework_content.encode('utf-8')).hexdigest()
            
            # Calculate corpus hash for complete provenance context
            corpus_content = ''.join([doc.get('filename', '') + str(doc.get('content', '')) for doc in corpus_documents])
            corpus_hash = hashlib.sha256(corpus_content.encode('utf-8')).hexdigest()
            
            synthesis_result = self._run_thin_synthesis(
                scores_hash=scores_hash,
                evidence_hash=evidence_hash,
                framework_content=framework_content,
                experiment_config=experiment_config,
                model=synthesis_model,
                audit_logger=audit,
                storage=storage,
                # Add provenance context (Issue #208 fix)
                framework_hash=framework_hash,
                corpus_hash=corpus_hash,
                corpus_manifest=corpus_manifest,  # THIN approach: pass raw corpus content
                debug_agent=debug_agent,
                debug_level=debug_level
            )
            
            synthesis_end_time = datetime.now(timezone.utc).isoformat()
            
            # Record synthesis stage
            agent_name = "ProductionThinSynthesisPipeline"
            stage_metadata = {
                "result_hash": synthesis_result["result_hash"],
                "duration_seconds": synthesis_result["duration_seconds"],
                "synthesis_confidence": synthesis_result["synthesis_confidence"]
            }
            
            # Add THIN-specific metadata if available
            if "thin_metadata" in synthesis_result:
                stage_metadata["thin_pipeline_data"] = synthesis_result["thin_metadata"]
            
            manifest.add_execution_stage("synthesis", agent_name,
                                       synthesis_start_time, synthesis_end_time, "completed", stage_metadata)
            
            # Finalize manifest (synthesis results already captured in execution stages)
            
            # Combine batch results for final summary
            analysis_summary = self._combine_batch_results(all_analysis_results)

            # Generate final report (optional, can be done by ReportAgent)
            # For now, we'll just use the synthesis markdown as the final report
            final_report_content = synthesis_result.get("synthesis_report_markdown", "Synthesis failed.")
            report_hash = storage.put_artifact(
                final_report_content.encode('utf-8'), 
                {"artifact_type": "final_report"}
            )

            # Write final report to results folder
            results_dir = self.security.secure_mkdir(run_folder / "results")
            report_file = results_dir / "final_report.md"
            self.security.secure_write_text(report_file, final_report_content)
            
            # Finalize manifest and audit
            manifest_file = manifest.finalize_manifest()
            audit.finalize_session()
            
            # Calculate total execution time
            end_time = datetime.now(timezone.utc).isoformat()
            total_duration = self._calculate_duration(start_time, end_time)
            
            # Final orchestrator event
            audit.log_orchestrator_event("experiment_complete", {
                "total_duration_seconds": total_duration,
                "analysis_duration": analysis_summary["total_duration_seconds"],
                "synthesis_duration": synthesis_result.get("execution_metadata", {}).get("duration_seconds", 0),
                "final_report_hash": report_hash,
                "manifest_file": str(manifest_file),
                "mathematical_validation": "completed"
            })
            
            print(f"‚úÖ THIN v2.0 experiment complete: {run_timestamp} ({total_duration:.1f}s)")
            print(f"üìã Results: {results_dir}")
            print(f"üìä Report: {report_file}")
            
            return {
                "run_id": run_timestamp,
                "run_folder": str(run_folder),
                "results_directory": str(results_dir),
                "final_report_file": str(report_file),
                "manifest_file": str(manifest_file),
                "total_duration_seconds": total_duration,
                "analysis_result": analysis_summary,
                "synthesis_result": synthesis_result,
                "mathematical_validation": True,
                "architecture": "thin_v2.0_direct_calls"
            }
            
        except Exception as e:
            # Log error and cleanup
            try:
                audit.log_error("orchestrator_error", str(e), {
                    "experiment_path": str(self.experiment_path),
                    "run_folder": str(run_folder) if 'run_folder' in locals() else None
                })
                if 'audit' in locals():
                    audit.finalize_session()
            except:
                pass  # Don't fail on logging errors
            
            raise ThinOrchestratorError(f"Experiment execution failed: {e}")

    def _execute_analysis_sequentially(self,
                                       analysis_agent: EnhancedAnalysisAgent,
                                       corpus_documents: List[Dict[str, Any]],
                                       framework_content: str,
                                       experiment_config: Dict[str, Any],
                                       model: str) -> tuple[List[Dict[str, Any]], Optional[str], Optional[str]]:
        """
        Executes the analysis agent for each document, then combines all results into a single artifact.
        """
        all_analysis_results = []
        total_docs = len(corpus_documents)
        print(f"\nüöÄ Starting sequential analysis of {total_docs} documents...")

        for i, doc in enumerate(corpus_documents):
            print(f"\n--- Analyzing document {i+1}/{total_docs}: {doc.get('filename')} ---")
            try:
                # Analyze each document individually
                result = analysis_agent.analyze_batch(
                    framework_content=framework_content,
                    corpus_documents=[doc],  # Pass a list with a single document
                    experiment_config=experiment_config,
                    model=model,
                    current_scores_hash=None,  # Don't accumulate hashes
                    current_evidence_hash=None
                )
                
                # Append the analysis result to the list
                all_analysis_results.append(result)
            except Exception as e:
                print(f"‚ùå Analysis failed for document {doc.get('filename')}: {e}")
                all_analysis_results.append({"error": str(e), "document": doc.get('filename')})

        # Combine all analysis results into a single artifact
        combined_result = self._combine_analysis_results(all_analysis_results)
        
        # Calculate framework hash for provenance tracking
        import hashlib
        framework_hash = hashlib.sha256(framework_content.encode('utf-8')).hexdigest()
        
        # Store the combined result and return its hash
        # Note: We need to access storage through the analysis agent's storage
        combined_hash = analysis_agent.storage.put_artifact(
            json.dumps(combined_result).encode('utf-8'),
            {
                "artifact_type": "analysis_json_v6", 
                "framework_version": "v6.0", 
                "combined": True,
                "framework_hash": framework_hash
            }
        )
        
        return all_analysis_results, combined_hash, combined_hash

    def _combine_analysis_results(self, analysis_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Combines multiple individual analysis results into a single combined result.
        """
        combined_document_analyses = []
        
        for i, result in enumerate(analysis_results):
            if "error" in result:
                # Skip failed analyses
                continue
                
            # Extract the actual analysis data from the nested structure
            if "analysis_result" in result and "result_content" in result["analysis_result"]:
                # Get the cached result content
                cached_result = result["analysis_result"]["result_content"]
                
                # Extract the actual analysis data from the raw_analysis_response
                if "raw_analysis_response" in cached_result:
                    # Parse the raw analysis response to get the actual JSON data
                    raw_response = cached_result["raw_analysis_response"]
                    
                    # Extract JSON from the delimited response
                    import re
                    json_pattern = r"<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n(.*?)\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>"
                    json_match = re.search(json_pattern, raw_response, re.DOTALL)
                    
                    if json_match:
                        try:
                            analysis_data = json.loads(json_match.group(1))
                            if "document_analyses" in analysis_data:
                                combined_document_analyses.extend(analysis_data["document_analyses"])
                        except json.JSONDecodeError as e:
                            print(f"Warning: Failed to parse JSON from analysis result {i}: {e}")
                            continue
                    else:
                        print(f"Warning: No JSON found in raw_analysis_response for result {i}")
                        continue
                else:
                    print(f"Warning: No raw_analysis_response found in cached result {i}")
                    continue
            elif "raw_analysis_response" in result:
                # Direct raw_analysis_response (fallback)
                raw_response = result["raw_analysis_response"]
                
                # Extract JSON from the delimited response
                import re
                json_pattern = r"<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n(.*?)\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>"
                json_match = re.search(json_pattern, raw_response, re.DOTALL)
                
                if json_match:
                    try:
                        analysis_data = json.loads(json_match.group(1))
                        if "document_analyses" in analysis_data:
                            combined_document_analyses.extend(analysis_data["document_analyses"])
                    except json.JSONDecodeError as e:
                        print(f"Warning: Failed to parse JSON from analysis result {i}: {e}")
                        continue
                else:
                    print(f"Warning: No JSON found in raw_analysis_response for result {i}")
                    continue
            elif "document_analyses" in result:
                # Direct document_analyses (shouldn't happen with current structure)
                combined_document_analyses.extend(result["document_analyses"])
            elif "analysis_metadata" in result:
                # Single document result (legacy format)
                combined_document_analyses.append({
                    "document_id": f"doc_{i}",
                    "document_name": f"document_{i}.txt",
                    "dimensional_scores": result.get("dimensional_scores", {}),
                    "evidence": result.get("evidence", [])
                })
            else:
                print(f"Warning: Unknown analysis result format for result {i}")
                continue
        
        # Create combined result structure
        combined_result = {
            "analysis_metadata": {
                "framework_name": "combined_analysis",
                "framework_version": "v6.0",
                "analyst_confidence": 0.85,
                "analysis_notes": f"Combined analysis of {len(combined_document_analyses)} documents"
            },
            "document_analyses": combined_document_analyses
        }
        
        return combined_result




    def _combine_batch_results(self, batch_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Combines results from multiple analysis batches into a single summary.
        """
        if not batch_results:
            return {"total_duration_seconds": 0, "num_batches": 0, "successful_batches": 0}

        total_duration = sum(r.get('analysis_result', {}).get('duration_seconds', 0) for r in batch_results)
        num_batches = len(batch_results)
        successful_batches = sum(1 for r in batch_results if r.get('analysis_result', {}).get('result_hash'))

        return {
            "total_duration_seconds": total_duration,
            "num_batches": num_batches,
            "successful_batches": successful_batches,
            "all_batches_successful": successful_batches == num_batches,
            "individual_batch_results": [
                {
                    "batch_id": r.get("analysis_result", {}).get("batch_id"),
                    "result_hash": r.get("analysis_result", {}).get("result_hash"),
                    "duration": r.get("analysis_result", {}).get("duration_seconds")
                } for r in batch_results
            ]
        }

    def _load_experiment_config(self) -> Dict[str, Any]:
        """Load and validate the experiment.md file."""
        exp_file = self.experiment_path / "experiment.md"
        
        if not exp_file.exists():
            raise ThinOrchestratorError(f"experiment.md not found in {self.experiment_path}")
        
        content = self.security.secure_read_text(exp_file)
        
        # Extract YAML from markdown
        if '---' in content:
            parts = content.split('---')
            if len(parts) >= 2:
                yaml_content = parts[1].strip()
            else:
                yaml_content = parts[0].strip()
        else:
            yaml_content = content
        
        try:
            config = yaml.safe_load(yaml_content)
        except yaml.YAMLError as e:
            raise ThinOrchestratorError(f"Invalid YAML in experiment.md: {e}")
        
        # Validate required fields
        required_fields = ['name', 'framework', 'corpus_path']
        missing_fields = [field for field in required_fields if field not in config]
        if missing_fields:
            raise ThinOrchestratorError(f"Missing required fields in experiment.md: {', '.join(missing_fields)}")
        
        return config
    
    def _build_comprehensive_experiment_context(self, experiment_config: Dict[str, Any], framework_content: str, corpus_manifest: Optional[Dict[str, Any]] = None) -> str:
        """
        Build comprehensive experiment context for the ResultsInterpreter.
        
        This provides the ResultsInterpreter with full context about:
        - Experiment design and hypotheses
        - Framework specifications and theoretical background
        - Corpus composition and speaker information
        - Research methodology and statistical approach
        """
        context_parts = []
        
        # 1. Experiment Overview
        context_parts.append("## EXPERIMENT CONTEXT")
        context_parts.append(f"**Experiment Name**: {experiment_config.get('name', 'Unknown')}")
        context_parts.append(f"**Framework**: {experiment_config.get('framework', 'Unknown')}")
        context_parts.append(f"**Corpus Path**: {experiment_config.get('corpus_path', 'Unknown')}")
        
        # 2. Experiment Description and Hypotheses
        if 'description' in experiment_config:
            context_parts.append(f"\n**Description**: {experiment_config['description']}")
        
        if 'hypothesis' in experiment_config:
            context_parts.append(f"\n**Primary Hypothesis**: {experiment_config['hypothesis']}")
        
        # 3. Multi-Hypothesis Framework
        if 'hypotheses' in experiment_config:
            context_parts.append("\n**Research Hypotheses**:")
            for key, hypothesis in experiment_config['hypotheses'].items():
                context_parts.append(f"- **{key}**: {hypothesis}")
        
        # 4. Experimental Design
        if 'validation' in experiment_config and 'required_tests' in experiment_config['validation']:
            context_parts.append(f"\n**Required Statistical Tests**: {', '.join(experiment_config['validation']['required_tests'])}")
        
        # 5. Reporting Structure
        if 'reporting' in experiment_config and 'structure' in experiment_config['reporting']:
            context_parts.append(f"\n**Required Report Sections**: {', '.join(experiment_config['reporting']['structure'])}")
        
        # 6. Framework Context (extract key information)
        context_parts.append("\n## FRAMEWORK CONTEXT")
        
        # Extract framework version and key concepts
        if "Version" in framework_content:
            version_line = [line for line in framework_content.split('\n') if "Version" in line and ":" in line][0]
            context_parts.append(f"**Framework Version**: {version_line.split(':')[1].strip()}")
        
        # Extract character dimensions
        if "Character Dimensions" in framework_content:
            context_parts.append("\n**Character Dimensions**:")
            lines = framework_content.split('\n')
            in_dimensions = False
            for line in lines:
                if "Character Dimensions" in line:
                    in_dimensions = True
                    continue
                elif in_dimensions and line.strip().startswith('#'):
                    break
                elif in_dimensions and line.strip() and not line.startswith('#'):
                    context_parts.append(f"- {line.strip()}")
        
        # Extract key formulas
        if "Character Tension Mathematics" in framework_content:
            context_parts.append("\n**Key Formulas**:")
            lines = framework_content.split('\n')
            in_math = False
            for line in lines:
                if "Character Tension Mathematics" in line:
                    in_math = True
                    continue
                elif in_math and line.strip().startswith('#'):
                    break
                elif in_math and line.strip() and not line.startswith('#'):
                    context_parts.append(f"- {line.strip()}")
        
        # 7. Corpus Context (load corpus manifest if available)
        try:
            corpus_dir = self.experiment_path / experiment_config.get('corpus_path', 'corpus')
            corpus_md_file = corpus_dir / "corpus.md"
            if corpus_md_file.exists():
                corpus_md_content = self.security.secure_read_text(corpus_md_file)
                
                # Extract file manifest from corpus.md
                if '```json' in corpus_md_content:
                    json_start = corpus_md_content.find('```json') + 7
                    json_end = corpus_md_content.find('```', json_start)
                    if json_end > json_start:
                        json_str = corpus_md_content[json_start:json_end].strip()
                        corpus_manifest = json.loads(json_str)
                        
                        context_parts.append("\n## CORPUS CONTEXT")
                        context_parts.append(f"**Total Documents**: {len(corpus_manifest.get('file_manifest', []))}")
                        
                        # Group by era and ideology
                        era_groups = {}
                        ideology_groups = {}
                        
                        for doc in corpus_manifest.get('file_manifest', []):
                            era = doc.get('era', 'Unknown')
                            ideology = doc.get('ideology', 'Unknown')
                            speaker = doc.get('speaker', 'Unknown')
                            
                            if era not in era_groups:
                                era_groups[era] = []
                            era_groups[era].append(speaker)
                            
                            if ideology not in ideology_groups:
                                ideology_groups[ideology] = []
                            ideology_groups[ideology].append(speaker)
                        
                        context_parts.append("\n**Era Distribution**:")
                        for era, speakers in era_groups.items():
                            context_parts.append(f"- **{era}**: {', '.join(speakers)}")
                        
                        context_parts.append("\n**Ideology Distribution**:")
                        for ideology, speakers in ideology_groups.items():
                            context_parts.append(f"- **{ideology}**: {', '.join(speakers)}")
                        
                        # Add experimental design context
                        if len(era_groups) > 1 and len(ideology_groups) > 1:
                            context_parts.append(f"\n**Experimental Design**: {len(ideology_groups)}√ó{len(era_groups)} factorial design")
                            context_parts.append("This enables analysis of main effects and interaction effects between ideology and era.")
        except Exception as e:
            context_parts.append(f"\n**Corpus Context**: Unable to load corpus manifest: {str(e)}")
        
        # 7. Raw Corpus Content (THIN approach)
        context_parts.append("\n## CORPUS CONTENT")
        if corpus_manifest:
            context_parts.append("**Corpus Manifest**:")
            context_parts.append("```json")
            context_parts.append(json.dumps(corpus_manifest, indent=2))
            context_parts.append("```")
        else:
            context_parts.append("**Corpus Manifest**: Not available")
        
        return "\n".join(context_parts)
    
    def _load_framework(self, framework_filename: str) -> str:
        """
        Load framework content with trusted canonical path resolution.
        
        The orchestrator is trusted infrastructure that can safely resolve canonical
        framework references (../../frameworks/...) while maintaining security 
        boundaries for agents.
        """
        
        # Check if this is a relative path to canonical frameworks
        if framework_filename.startswith("../../frameworks/"):
            # TRUSTED OPERATION: Orchestrator resolves canonical frameworks
            project_root = Path(__file__).parent.parent.parent  # Get to discernus/
            canonical_path = framework_filename.lstrip("../../")
            framework_file = project_root / canonical_path
            
            if not framework_file.exists():
                raise ThinOrchestratorError(f"Canonical framework not found: {framework_filename}")
            
            print(f"üõ°Ô∏è Security: Loading canonical framework: {framework_file.name}")
            # Direct read (orchestrator is trusted infrastructure)
            return framework_file.read_text(encoding='utf-8')
        
        else:
            # EXISTING LOGIC: Local framework in experiment directory
            framework_file = self.experiment_path / framework_filename
            
            if not framework_file.exists():
                raise ThinOrchestratorError(f"Framework file not found: {framework_filename}")
            
            print(f"üõ°Ô∏è Security: Loading local framework: {framework_file.name}")
            # Use security boundary for local files (agents must stay within boundary)
            return self.security.secure_read_text(framework_file)
    
    def _load_corpus(self, corpus_path: str) -> tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """Load corpus documents and corpus manifest."""
        corpus_dir = self.experiment_path / corpus_path
        
        if not corpus_dir.exists():
            raise ThinOrchestratorError(f"Corpus directory not found: {corpus_path}")
        
        # Find all text files in corpus directory
        corpus_files = [f for f in corpus_dir.iterdir() if f.is_file() and f.suffix == '.txt']
        
        if not corpus_files:
            raise ThinOrchestratorError(f"No .txt files found in corpus directory: {corpus_path}")
        
        # Load corpus documents
        documents = []
        for txt_file in sorted(corpus_files):
            content = self.security.secure_read_text(txt_file)
            documents.append({
                "filename": txt_file.name,
                "content": content,
                "filepath": str(txt_file.relative_to(self.experiment_path))
            })
        
        # Load corpus manifest from corpus.md
        corpus_manifest = {}
        corpus_md_file = corpus_dir / "corpus.md"
        if corpus_md_file.exists():
            try:
                corpus_md_content = self.security.secure_read_text(corpus_md_file)
                
                # Extract JSON from corpus.md (similar to framework parsing)
                if '```json' in corpus_md_content:
                    json_start = corpus_md_content.find('```json') + 7
                    json_end = corpus_md_content.find('```', json_start)
                    if json_end > json_start:
                        json_str = corpus_md_content[json_start:json_end].strip()
                        corpus_manifest = json.loads(json_str)
                        print(f"üìÑ Loaded corpus manifest with {len(corpus_manifest.get('file_manifest', []))} document metadata entries")
                    else:
                        print("‚ö†Ô∏è corpus.md found but no valid JSON block detected")
                else:
                    print("‚ö†Ô∏è corpus.md found but no JSON block detected")
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to parse corpus.md: {e}")
                corpus_manifest = {}
        else:
            print("‚ö†Ô∏è No corpus.md found - synthesis will have limited metadata awareness")
        
        return documents, corpus_manifest
    
    def _generate_final_report(self, 
                             analysis_result: Dict[str, Any],
                             synthesis_result: Dict[str, Any], 
                             experiment_config: Dict[str, Any],
                             manifest: EnhancedManifest) -> str:
        """Generate beautiful final markdown report."""
        
        timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")
        
        report = f"""# {experiment_config['name']} - Analysis Report

**Generated**: {timestamp}  
**Architecture**: THIN v2.0 Direct Function Calls  
**Mathematical Validation**: Enabled  

---

## Executive Summary

This report presents the results of computational research analysis using the Discernus THIN v2.0 architecture with enhanced mathematical validation capabilities.

**Key Features of This Analysis**:
- ‚úÖ Mathematical "show your work" requirements for all calculations
- ‚úÖ Dual-LLM validation with spot-checking of numerical results  
- ‚úÖ Complete audit trails for academic reproducibility
- ‚úÖ Content-addressable storage for perfect caching
- ‚úÖ Security boundary enforcement

---

## Analysis Results

### Enhanced Analysis Agent Results
**Agent**: {analysis_result['result_content']['agent_name']}  
**Version**: {analysis_result['result_content']['agent_version']}  
**Duration**: {analysis_result['duration_seconds']:.1f} seconds  
**Mathematical Validation**: {analysis_result['mathematical_validation']}  

{analysis_result['result_content']['analysis_results']}

---

## Synthesis Results

### Enhanced Synthesis Agent Results  
**Agent**: {synthesis_result['result_content']['agent_name']}  
**Version**: {synthesis_result['result_content']['agent_version']}  
**Duration**: {synthesis_result['duration_seconds']:.1f} seconds  
**Mathematical Confidence**: {synthesis_result['synthesis_confidence']:.2f}  

{synthesis_result['result_content']['synthesis_results']}

---

## Mathematical Validation Report

### Validation Summary
- **Dual-LLM Validation**: {synthesis_result['result_content'].get('mathematical_validation', {}).get('validation_enabled', False)}
- **Mathematical Confidence**: {synthesis_result['synthesis_confidence']:.2f}
- **Errors Detected**: {len(synthesis_result['mathematical_validation'].get('mathematical_errors_found', []))}

### Validation Details
{synthesis_result['result_content'].get('mathematical_validation', {}).get('validation_content', 'No validation details available')}

---

## Provenance Information

### Experiment Configuration
- **Experiment**: {experiment_config['name']}
- **Framework**: {experiment_config['framework']}
- **Corpus Path**: {experiment_config['corpus_path']}

### Execution Metadata
- **Run ID**: {analysis_result['result_content']['execution_metadata']['start_time']}
- **Security Boundary**: {analysis_result['result_content']['provenance']['security_boundary']['experiment_name']}
- **Audit Session**: {analysis_result['result_content']['provenance']['audit_session_id']}

### Artifact Hashes
- **Framework**: {analysis_result['result_content']['input_artifacts']['framework_hash'][:16]}...
- **Documents**: {len(analysis_result['result_content']['input_artifacts']['document_hashes'])} corpus documents
- **Analysis Result**: {analysis_result['result_hash'][:16]}...
- **Synthesis Result**: {synthesis_result['result_hash'][:16]}...

---

## Quality Assurance

### THIN v2.0 Architecture Validation
- ‚úÖ Direct function calls (no Redis coordination)
- ‚úÖ LLM intelligence for complex reasoning
- ‚úÖ Minimal software coordination  
- ‚úÖ Perfect caching through content-addressable storage
- ‚úÖ Complete audit trails for academic integrity

### Mathematical Validation
- ‚úÖ "Show your work" requirements implemented
- ‚úÖ Dual-LLM validation with spot-checking
- ‚úÖ Confidence estimates for all numerical results
- ‚úÖ Independent recalculation of key metrics

---

*This report was generated by the Discernus THIN v2.0 architecture with enhanced mathematical validation capabilities.*
"""
        
        return report
    
    def _calculate_duration(self, start: str, end: str) -> float:
        """Calculate duration between timestamps in seconds."""
        try:
            start_dt = datetime.fromisoformat(start.replace('Z', '+00:00'))
            end_dt = datetime.fromisoformat(end.replace('Z', '+00:00'))
            return (end_dt - start_dt).total_seconds()
        except Exception:
            return 0.0 