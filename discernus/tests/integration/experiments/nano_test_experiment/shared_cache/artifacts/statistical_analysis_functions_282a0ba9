{
  "status": "success",
  "functions_generated": 5,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 21572,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: nano_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-09-10T19:33:05.054622+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef get_sentiment_data(data):\n    \"\"\"\n    Extracts and restructures sentiment analysis data from the raw input.\n\n    This function processes the nested JSON structure within the 'raw_analysis_response'\n    field to extract document-level sentiment scores. It assumes the input data\n    is a list of dictionaries, where each dictionary represents an analysis result\n    for a single document.\n\n    The function aims to create a flat DataFrame with document names and their\n    corresponding sentiment scores (raw, salience, confidence) for both positive\n    and negative sentiment dimensions.\n\n    Args:\n        data (list): A list of dictionaries, where each dictionary contains\n                     analysis results, including 'filename' and\n                     'raw_analysis_response'.\n\n    Returns:\n        pandas.DataFrame: A DataFrame with columns:\n                          'document_name', 'positive_sentiment_raw',\n                          'positive_sentiment_salience', 'positive_sentiment_confidence',\n                          'negative_sentiment_raw', 'negative_sentiment_salience',\n                          'negative_sentiment_confidence'.\n                          Returns an empty DataFrame if input is invalid or no data is found.\n    \"\"\"\n    import pandas as pd\n    import json\n\n    all_results = []\n    for entry in data:\n        try:\n            filename = entry.get('filename')\n            raw_response_str = entry.get('raw_analysis_response')\n\n            if not filename or not raw_response_str:\n                continue\n\n            # Safely parse the JSON string\n            try:\n                analysis_result = json.loads(raw_response_str)\n            except json.JSONDecodeError:\n                # Handle cases where the string might not be valid JSON\n                # For this specific framework, it seems to be embedded within <<<DISCERNUS_ANALYSIS_JSON_v6>>>\n                # We need to extract the actual JSON part.\n                json_start_marker = \"<<<DISCERNUS_ANALYSIS_JSON_v6>>>\\n\"\n                json_end_marker = \"\\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>\"\n                \n                if json_start_marker in raw_response_str and json_end_marker in raw_response_str:\n                    json_content = raw_response_str.split(json_start_marker, 1)[1].rsplit(json_end_marker, 1)[0]\n                    try:\n                        analysis_result = json.loads(json_content)\n                    except json.JSONDecodeError:\n                        print(f\"Warning: Could not parse JSON content for {filename}\")\n                        continue\n                else:\n                    print(f\"Warning: Unexpected format for raw_analysis_response in {filename}\")\n                    continue\n\n            document_analyses = analysis_result.get('document_analyses', [])\n            if not document_analyses:\n                continue\n\n            # Assuming only one document analysis per entry for this experiment\n            doc_analysis = document_analyses[0]\n            dimensional_scores = doc_analysis.get('dimensional_scores', {})\n\n            positive_sentiment = dimensional_scores.get('positive_sentiment', {})\n            negative_sentiment = dimensional_scores.get('negative_sentiment', {})\n\n            result_row = {\n                'document_name': filename,\n                'positive_sentiment_raw': positive_sentiment.get('raw_score'),\n                'positive_sentiment_salience': positive_sentiment.get('salience'),\n                'positive_sentiment_confidence': positive_sentiment.get('confidence'),\n                'negative_sentiment_raw': negative_sentiment.get('raw_score'),\n                'negative_sentiment_salience': negative_sentiment.get('salience'),\n                'negative_sentiment_confidence': negative_sentiment.get('confidence'),\n            }\n            all_results.append(result_row)\n\n        except Exception as e:\n            print(f\"Error processing entry for {entry.get('filename', 'unknown')}: {e}\")\n            continue\n\n    if not all_results:\n        return pd.DataFrame()\n\n    df = pd.DataFrame(all_results)\n    return df\n\ndef describe_sentiment_scores(data):\n    \"\"\"\n    Calculates descriptive statistics for sentiment dimensions.\n\n    This function takes the processed sentiment data (output from get_sentiment_data)\n    and computes descriptive statistics (count, mean, std, min, 25%, 50%, 75%, max)\n    for the raw sentiment scores.\n\n    The analysis is classified as TIER 3 (Exploratory Analysis) due to the very\n    small sample size (N=2) in the provided data. Results should be interpreted\n    with extreme caution.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing sentiment scores, expected\n                                 to have columns like 'positive_sentiment_raw'\n                                 and 'negative_sentiment_raw'.\n\n    Returns:\n        dict: A dictionary containing descriptive statistics for each sentiment\n              dimension, or None if the input data is insufficient or invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    if not isinstance(data, pd.DataFrame) or data.empty:\n        print(\"Input data is empty or not a DataFrame.\")\n        return None\n\n    # Ensure the required columns exist\n    required_cols = ['positive_sentiment_raw', 'negative_sentiment_raw']\n    if not all(col in data.columns for col in required_cols):\n        print(f\"Missing required columns. Expected: {required_cols}\")\n        return None\n\n    # TIER 3: Exploratory Analysis (N < 15)\n    sample_size = len(data)\n    if sample_size < 15:\n        print(f\"Performing TIER 3 (Exploratory) analysis. Sample size: {sample_size}. Results are suggestive.\")\n\n    # Calculate descriptive statistics for raw scores\n    descriptive_stats = {}\n    for dim in ['positive_sentiment', 'negative_sentiment']:\n        raw_col = f'{dim}_raw'\n        if raw_col in data.columns:\n            # Use .describe() which provides count, mean, std, min, 25%, 50%, 75%, max\n            stats = data[raw_col].describe().to_dict()\n            descriptive_stats[dim] = stats\n        else:\n            descriptive_stats[dim] = {} # Or handle as an error if preferred\n\n    # Add sample size and tier information for clarity\n    analysis_summary = {\n        \"tier\": 3,\n        \"sample_size\": sample_size,\n        \"interpretation_caveat\": \"Exploratory analysis - results are suggestive rather than conclusive.\",\n        \"descriptive_statistics\": descriptive_stats\n    }\n\n    return analysis_summary\n\ndef compare_sentiment_groups(data):\n    \"\"\"\n    Compares sentiment scores between groups using t-tests.\n\n    This function is designed to compare the 'positive_sentiment_raw' and\n    'negative_sentiment_raw' scores between different groups. However, given the\n    current experiment setup (nano_test_experiment) with only two documents\n    ('positive_test.txt', 'negative_test.txt'), there are no explicit grouping\n    variables defined in the corpus manifest that would allow for meaningful\n    group comparisons with standard statistical tests.\n\n    The provided data structure and experiment description do not define any\n    grouping variables (e.g., speaker party, administration, etc.) that would\n    enable a t-test or ANOVA.\n\n    Therefore, this function will return None, indicating that the necessary\n    grouping information is not available for this specific experiment. If a\n    corpus manifest with grouping variables were available, this function would\n    be implemented to perform t-tests (for two groups) or ANOVA (for more than\n    two groups) on the sentiment scores.\n\n    For the current \"nano_test_experiment\", the sample size is N=2, which falls\n    into TIER 3 (Exploratory Analysis). Even if groups were defined, a t-test\n    would not be appropriate due to the extremely small sample size per group.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing sentiment scores and\n                                 document names.\n\n    Returns:\n        None: As no grouping variables are available in the current experiment\n              context to perform group comparisons.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy import stats\n\n    # The experiment spec and data do not provide any explicit grouping variables\n    # (e.g., from a corpus manifest) that would allow for group comparisons\n    # like t-tests or ANOVA. The provided data consists of only two documents:\n    # 'positive_test.txt' and 'negative_test.txt'.\n\n    # If a corpus manifest were available and defined groups, the logic would be:\n    # 1. Create a mapping from document_name to group.\n    # 2. Filter data by group.\n    # 3. Perform statistical tests (e.g., t-test for two groups).\n    # 4. Document the tier and power limitations.\n\n    # Given the current constraints (N=2 total, no defined groups),\n    # performing group comparisons is not feasible or statistically sound.\n    print(\"Skipping group comparison: No grouping variables available in the current experiment context.\")\n    return None\n\ndef correlate_sentiment_scores(data):\n    \"\"\"\n    Calculates the Pearson correlation between positive and negative sentiment scores.\n\n    This function computes the Pearson correlation coefficient between the\n    'positive_sentiment_raw' and 'negative_sentiment_raw' scores.\n\n    The analysis is classified as TIER 3 (Exploratory Analysis) due to the very\n    small sample size (N=2) in the provided data. Results should be interpreted\n    with extreme caution.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing sentiment scores, expected\n                                 to have columns like 'positive_sentiment_raw'\n                                 and 'negative_sentiment_raw'.\n\n    Returns:\n        dict: A dictionary containing the correlation coefficient, p-value,\n              sample size, tier, and interpretation caveat, or None if\n              the input data is insufficient or invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import pearsonr\n\n    if not isinstance(data, pd.DataFrame) or data.empty:\n        print(\"Input data is empty or not a DataFrame.\")\n        return None\n\n    # Ensure the required columns exist\n    required_cols = ['positive_sentiment_raw', 'negative_sentiment_raw']\n    if not all(col in data.columns for col in required_cols):\n        print(f\"Missing required columns. Expected: {required_cols}\")\n        return None\n\n    # TIER 3: Exploratory Analysis (N < 15)\n    sample_size = len(data)\n    if sample_size < 15:\n        print(f\"Performing TIER 3 (Exploratory) analysis. Sample size: {sample_size}. Results are suggestive.\")\n\n    # Calculate Pearson correlation\n    try:\n        # Drop rows with NaN in either of the sentiment columns for correlation calculation\n        data_for_corr = data.dropna(subset=required_cols)\n        \n        if len(data_for_corr) < 2: # Need at least 2 data points for correlation\n            print(\"Insufficient data points (after dropping NaNs) to calculate correlation.\")\n            return None\n\n        correlation, p_value = pearsonr(data_for_corr['positive_sentiment_raw'], data_for_corr['negative_sentiment_raw'])\n\n        results = {\n            \"tier\": 3,\n            \"sample_size\": len(data_for_corr),\n            \"interpretation_caveat\": \"Exploratory analysis - results are suggestive rather than conclusive.\",\n            \"correlation_coefficient\": correlation,\n            \"p_value\": p_value\n        }\n        return results\n\n    except Exception as e:\n        print(f\"Error calculating correlation: {e}\")\n        return None\n\ndef analyze_sentiment_pipeline_validation(data):\n    \"\"\"\n    Performs a basic validation of the sentiment analysis pipeline.\n\n    This function checks if the pipeline correctly identifies positive vs. negative\n    sentiment and if the analysis agent can process simple dimensional scoring.\n    It also assesses the distinction between positive and negative sentiment scores.\n\n    Given the \"nano_test_experiment\" with two documents:\n    1. 'positive_test.txt': Expected to have high positive sentiment, low negative.\n    2. 'negative_test.txt': Expected to have low positive sentiment, high negative.\n\n    The analysis is classified as TIER 3 (Exploratory Analysis) due to the very\n    small sample size (N=2). Results are indicative and not statistically robust.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing sentiment scores, expected\n                                 to have columns like 'document_name',\n                                 'positive_sentiment_raw', and 'negative_sentiment_raw'.\n\n    Returns:\n        dict: A dictionary summarizing the validation findings, including expected\n              outcomes and actual results, or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    if not isinstance(data, pd.DataFrame) or data.empty:\n        print(\"Input data is empty or not a DataFrame.\")\n        return None\n\n    # Define expected outcomes based on document names\n    expected_outcomes = {\n        'positive_test.txt': {'positive_sentiment_raw': (0.7, 1.0), 'negative_sentiment_raw': (0.0, 0.3)},\n        'negative_test.txt': {'positive_sentiment_raw': (0.0, 0.3), 'negative_sentiment_raw': (0.7, 1.0)}\n    }\n\n    # TIER 3: Exploratory Analysis (N < 15)\n    sample_size = len(data)\n    if sample_size < 15:\n        print(f\"Performing TIER 3 (Exploratory) analysis for pipeline validation. Sample size: {sample_size}. Results are suggestive.\")\n\n    validation_results = {\n        \"tier\": 3,\n        \"sample_size\": sample_size,\n        \"interpretation_caveat\": \"Exploratory analysis - results are suggestive rather than conclusive.\",\n        \"research_questions_addressed\": [\n            \"Does the pipeline correctly identify positive vs negative sentiment?\",\n            \"Can the analysis agent process simple dimensional scoring?\",\n            \"Clear distinction between positive and negative sentiment scores across the two test documents.\"\n        ],\n        \"actual_vs_expected\": {}\n    }\n\n    for index, row in data.iterrows():\n        doc_name = row['document_name']\n        if doc_name in expected_outcomes:\n            expected = expected_outcomes[doc_name]\n            actual_vs_expected_doc = {\n                'document': doc_name,\n                'positive_sentiment_raw': {\n                    'expected': expected['positive_sentiment_raw'],\n                    'actual': row.get('positive_sentiment_raw')\n                },\n                'negative_sentiment_raw': {\n                    'expected': expected['negative_sentiment_raw'],\n                    'actual': row.get('negative_sentiment_raw')\n                }\n            }\n            validation_results[\"actual_vs_expected\"][doc_name] = actual_vs_expected_doc\n        else:\n            validation_results[\"actual_vs_expected\"][doc_name] = {\n                'document': doc_name,\n                'message': 'Document not found in expected outcomes for validation.'\n            }\n\n    # Basic check for distinction\n    pos_doc = data[data['document_name'] == 'positive_test.txt']\n    neg_doc = data[data['document_name'] == 'negative_test.txt']\n\n    if not pos_doc.empty and not neg_doc.empty:\n        pos_sentiment_pos = pos_doc['positive_sentiment_raw'].iloc[0]\n        neg_sentiment_pos = pos_doc['negative_sentiment_raw'].iloc[0]\n        pos_sentiment_neg = neg_doc['positive_sentiment_raw'].iloc[0]\n        neg_sentiment_neg = neg_doc['negative_sentiment_raw'].iloc[0]\n\n        # Check if positive doc is indeed more positive than negative\n        positive_distinction = pos_sentiment_pos > neg_sentiment_pos\n        # Check if negative doc is indeed more negative than positive\n        negative_distinction = neg_sentiment_neg > pos_sentiment_neg\n\n        validation_results[\"sentiment_distinction_check\"] = {\n            \"positive_doc_more_positive\": positive_distinction,\n            \"negative_doc_more_negative\": negative_distinction,\n            \"notes\": \"Checks if positive document has higher positive score and lower negative score than negative document.\"\n        }\n    else:\n        validation_results[\"sentiment_distinction_check\"] = {\n            \"message\": \"Could not perform distinction check due to missing document data.\"\n        }\n\n    return validation_results\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}