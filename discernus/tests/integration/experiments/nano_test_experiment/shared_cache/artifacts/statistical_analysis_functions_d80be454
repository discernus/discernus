{
  "status": "success",
  "functions_generated": 3,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 14511,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: nano_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-09-03T15:06:15.459267+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef summarize_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates and summarizes descriptive statistics for all key numerical dimensions.\n\n    Methodology:\n    This function computes standard descriptive statistics (count, mean, standard deviation,\n    min, quartiles, and max) for the raw score, salience, and confidence columns of each\n    sentiment dimension. Given the extremely small sample size, this analysis is\n    categorized as Tier 3 (Exploratory). The results serve as a basic data quality\n    and pipeline validation check, not as a basis for statistical inference.\n\n    Statistical Tier: Tier 3 (Exploratory Analysis)\n    Power Assessment: The total sample size is N=2. This is insufficient for any\n    inferential statistical claims. Results are purely descriptive and should be\n    interpreted as a system functionality check.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             for sentiment scores (e.g., 'positive_sentiment_raw').\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing a summary of descriptive statistics for the\n              sentiment dimensions. Returns None if the data is insufficient\n              (e.g., fewer than 2 rows or missing required columns).\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_cols = [\n            'positive_sentiment_raw', 'positive_sentiment_salience', 'positive_sentiment_confidence',\n            'negative_sentiment_raw', 'negative_sentiment_salience', 'negative_sentiment_confidence'\n        ]\n\n        if data is None or data.shape[0] < 1:\n            return None\n\n        # Ensure all required columns exist\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        # As N=2, std dev calculation requires ddof=1, which is the pandas default.\n        # With N=1, std dev would be NaN. With N=2, it is well-defined.\n        if data.shape[0] < 2:\n            return {\n                \"warning\": \"Sample size is 1. Standard deviation cannot be calculated.\",\n                \"statistics\": data[required_cols].describe().to_dict()\n            }\n\n        stats = data[required_cols].describe()\n        \n        results = {\n            \"sample_size\": int(data.shape[0]),\n            \"tier\": \"Tier 3: Exploratory Analysis\",\n            \"notes\": \"Exploratory analysis - results are suggestive rather than conclusive (N=2). These statistics validate data presence and range.\",\n            \"descriptive_statistics\": stats.to_dict()\n        }\n        return results\n\n    except Exception as e:\n        # Log the exception in a real application\n        # print(f\"Error in summarize_descriptive_statistics: {e}\")\n        return None\n\ndef analyze_sentiment_distinction(data, **kwargs):\n    \"\"\"\n    Performs a descriptive comparison of sentiment scores between document types.\n\n    Methodology:\n    This function addresses the research question about distinguishing between positive\n    and negative sentiment. It groups the data by 'document_name' and calculates the\n    mean scores for 'positive_sentiment_raw' and 'negative_sentiment_raw' for each\n    document. This directly tests the \"Expected Outcome\" of a \"clear distinction\".\n\n    No inferential tests (e.g., t-test) are performed, as this would be statistically\n    invalid and methodologically unsound with a sample size of N=1 per group. The\n    analysis is strictly descriptive.\n\n    Statistical Tier: Tier 3 (Exploratory Analysis)\n    Power Assessment: With a total sample size of N=2 (N=1 per group), this analysis\n    is purely for pipeline validation. It confirms that the positive test document\n    scores high on positive sentiment and low on negative, and vice-versa for the\n    negative test document.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             'document_name', 'positive_sentiment_raw', and\n                             'negative_sentiment_raw'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing the mean scores for each document group.\n              Returns None if data is insufficient (e.g., fewer than 2 rows,\n              missing required columns, or fewer than 2 unique documents).\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_cols = ['document_name', 'positive_sentiment_raw', 'negative_sentiment_raw']\n        if data is None or data.shape[0] < 2 or not all(col in data.columns for col in required_cols):\n            return None\n\n        if data['document_name'].nunique() < 2:\n            return None\n\n        # Group by document name and calculate the mean. With N=1 per group, mean is just the value.\n        grouped_stats = data.groupby('document_name')[['positive_sentiment_raw', 'negative_sentiment_raw']].mean()\n\n        # Create a summary dictionary for clear interpretation\n        summary = {\n            doc_name: {\n                \"positive_sentiment_mean\": row['positive_sentiment_raw'],\n                \"negative_sentiment_mean\": row['negative_sentiment_raw']\n            }\n            for doc_name, row in grouped_stats.iterrows()\n        }\n\n        # Check for expected outcome\n        pos_doc_name = 'positive_test.txt'\n        neg_doc_name = 'negative_test.txt'\n        \n        outcome_check = {}\n        if pos_doc_name in summary and neg_doc_name in summary:\n            pos_doc = summary[pos_doc_name]\n            neg_doc = summary[neg_doc_name]\n            \n            check1 = pos_doc['positive_sentiment_mean'] > neg_doc['positive_sentiment_mean']\n            check2 = pos_doc['negative_sentiment_mean'] < neg_doc['negative_sentiment_mean']\n            \n            outcome_check = {\n                \"expected_outcome_validation\": {\n                    \"description\": \"Checks if positive doc has higher positive score and negative doc has higher negative score.\",\n                    \"positive_doc_has_higher_positive_score\": bool(check1),\n                    \"negative_doc_has_higher_negative_score\": bool(check2),\n                    \"overall_distinction_achieved\": bool(check1 and check2)\n                }\n            }\n\n        results = {\n            \"sample_size\": int(data.shape[0]),\n            \"groups\": data['document_name'].nunique(),\n            \"tier\": \"Tier 3: Exploratory Analysis\",\n            \"notes\": \"Descriptive comparison for pipeline validation (N=2). Not an inferential test.\",\n            \"mean_scores_by_document\": summary,\n            **outcome_check\n        }\n\n        return results\n\n    except Exception as e:\n        # print(f\"Error in analyze_sentiment_distinction: {e}\")\n        return None\n\ndef analyze_dimension_correlations(data, **kwargs):\n    \"\"\"\n    Calculates the Pearson correlation matrix for the primary sentiment dimensions.\n\n    Methodology:\n    This function computes a Pearson correlation matrix for the 'positive_sentiment_raw'\n    and 'negative_sentiment_raw' scores. This is an exploratory step to understand the\n    relationship between the dimensions as measured by the analysis agent.\n\n    Statistical Tier: Tier 3 (Exploratory Analysis)\n    Power Assessment: The total sample size is N=2. A correlation coefficient calculated\n    from two data points will always be either +1.0, -1.0, or undefined. It holds no\n    inferential value and cannot be used to make claims about the true relationship\n    between the variables. This function is included for completeness as a standard\n    exploratory technique, but its output must be interpreted with extreme caution.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             'positive_sentiment_raw' and 'negative_sentiment_raw'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing the correlation matrix. Returns None if data\n              is insufficient (e.g., fewer than 2 rows or missing required columns).\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_cols = ['positive_sentiment_raw', 'negative_sentiment_raw']\n        if data is None or data.shape[0] < 2 or not all(col in data.columns for col in required_cols):\n            return None\n\n        # Select only the raw score columns for correlation\n        score_data = data[required_cols]\n\n        # Drop rows with NaN in these specific columns for correlation calculation\n        score_data = score_data.dropna()\n\n        if score_data.shape[0] < 2:\n            return None\n\n        correlation_matrix = score_data.corr(method='pearson')\n\n        results = {\n            \"sample_size_for_correlation\": int(score_data.shape[0]),\n            \"tier\": \"Tier 3: Exploratory Analysis\",\n            \"warning\": \"Exploratory analysis - results are suggestive rather than conclusive (N=2). With N=2, correlation will be either +1, -1, or undefined, and should not be interpreted for inferential purposes.\",\n            \"correlation_matrix\": correlation_matrix.to_dict()\n        }\n        return results\n\n    except Exception as e:\n        # print(f\"Error in analyze_dimension_correlations: {e}\")\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}