{
  "status": "success",
  "functions_generated": 4,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 35487,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: micro_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-09-10T19:32:15.849369+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef descriptive_stats_by_category(data):\n    \"\"\"\n    Calculates descriptive statistics (count, mean, std, min, max) for sentiment dimensions\n    and derived metrics, grouped by sentiment category.\n\n    This is an exploratory analysis (TIER 3) due to the small sample size (N=2 per group).\n    Results should be interpreted with extreme caution.\n\n    Args:\n        data: pandas DataFrame with columns:\n              'document_name', 'positive_sentiment_raw', 'negative_sentiment_raw',\n              'positive_sentiment_salience', 'positive_sentiment_confidence',\n              'negative_sentiment_salience', 'negative_sentiment_confidence'.\n\n    Returns:\n        dict: A dictionary containing descriptive statistics for each sentiment category.\n              Returns None if data is insufficient or malformed.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    from pathlib import Path\n\n    # Helper function to load data from JSON files (as defined in the preamble)\n    def _load_analysis_data(data_type=\"raw\"):\n        if data_type == \"raw\":\n            file_pattern = \"*_raw_analysis_sample.json\"\n        elif data_type == \"derived\":\n            file_pattern = \"*_derived_metrics_sample.json\"\n        else:\n            raise ValueError(\"data_type must be 'raw' or 'derived'\")\n\n        all_data = []\n        current_dir = Path('.')\n        for file_path in current_dir.glob(file_pattern):\n            try:\n                with open(file_path, 'r') as f:\n                    data_content = json.load(f)\n                    for entry in data_content:\n                        if 'analysis_result' in entry and 'result_content' in entry['analysis_result']:\n                            result_content = entry['analysis_result']['result_content']\n                            if 'document_analyses' in result_content:\n                                for doc_analysis in result_content['document_analyses']:\n                                    doc_name = doc_analysis.get('document_name')\n                                    if doc_name:\n                                        scores = doc_analysis.get('dimensional_scores', {})\n                                        positive_sentiment = scores.get('positive_sentiment', {})\n                                        negative_sentiment = scores.get('negative_sentiment', {})\n\n                                        row = {\n                                            'document_name': doc_name,\n                                            'positive_sentiment_raw': positive_sentiment.get('raw_score'),\n                                            'positive_sentiment_salience': positive_sentiment.get('salience'),\n                                            'positive_sentiment_confidence': positive_sentiment.get('confidence'),\n                                            'negative_sentiment_raw': negative_sentiment.get('raw_score'),\n                                            'negative_sentiment_salience': negative_sentiment.get('salience'),\n                                            'negative_sentiment_confidence': negative_sentiment.get('confidence'),\n                                        }\n                                        all_data.append(row)\n            except Exception as e:\n                print(f\"Error processing file {file_path}: {e}\")\n                continue\n        \n        if not all_data:\n            return pd.DataFrame()\n\n        df = pd.DataFrame(all_data)\n        for col in df.columns:\n            if col != 'document_name':\n                df[col] = pd.to_numeric(df[col], errors='coerce')\n        return df\n\n    # Helper function to get sentiment category mapping (as defined in the preamble)\n    def _get_sentiment_category_mapping():\n        return {\n            \"negative_test_1.txt\": \"negative\",\n            \"negative_test_2.txt\": \"negative\",\n            \"positive_test_1.txt\": \"positive\",\n            \"positive_test_2.txt\": \"positive\",\n        }\n\n    # Helper function to prepare data with derived metrics and category\n    def _prepare_data_with_derived_metrics(data):\n        if data.empty:\n            return pd.DataFrame()\n\n        category_map = _get_sentiment_category_mapping()\n        data['sentiment_category'] = data['document_name'].map(category_map)\n\n        data['net_sentiment'] = data['positive_sentiment_raw'] - data['negative_sentiment_raw']\n        data['sentiment_magnitude'] = (data['positive_sentiment_raw'] + data['negative_sentiment_raw']) / 2\n\n        data['net_sentiment'] = data['net_sentiment'].clip(-1.0, 1.0)\n        data['sentiment_magnitude'] = data['sentiment_magnitude'].clip(0.0, 1.0)\n\n        return data\n\n    # --- Main logic for descriptive_stats_by_category ---\n    if data.empty:\n        print(\"Input data is empty. Cannot compute descriptive statistics.\")\n        return None\n\n    # Ensure data is prepared with categories and derived metrics\n    # The 'data' parameter here is assumed to be the raw data loaded from files.\n    # We need to load it if it's not already prepared.\n    # However, the function signature expects 'data' to be the DataFrame.\n    # Let's assume 'data' is the raw data and prepare it.\n    # If 'data' is already prepared, this step will re-prepare it, which is fine.\n    prepared_data = _prepare_data_with_derived_metrics(data)\n\n    if prepared_data.empty or 'sentiment_category' not in prepared_data.columns or prepared_data['sentiment_category'].isnull().all():\n        print(\"Data preparation failed or no valid sentiment categories found. Cannot compute descriptive statistics.\")\n        return None\n\n    sentiment_dims = [\n        'positive_sentiment_raw', 'negative_sentiment_raw',\n        'positive_sentiment_salience', 'negative_sentiment_salience',\n        'positive_sentiment_confidence', 'negative_sentiment_confidence',\n        'net_sentiment', 'sentiment_magnitude'\n    ]\n    sentiment_dims = [col for col in sentiment_dims if col in prepared_data.columns]\n\n    if not sentiment_dims:\n        print(\"No valid sentiment dimension columns found.\")\n        return None\n\n    try:\n        descriptive_stats = prepared_data.groupby('sentiment_category')[sentiment_dims].agg(['count', 'mean', 'std', 'min', 'max'])\n        return descriptive_stats.to_dict()\n    except Exception as e:\n        print(f\"Error computing descriptive statistics: {e}\")\n        return None\n\ndef compare_sentiment_groups_ttest(data):\n    \"\"\"\n    Performs independent samples t-tests to compare sentiment dimensions and\n    derived metrics between 'positive' and 'negative' sentiment categories.\n\n    This is an exploratory analysis (TIER 3) due to the small sample size (N=2 per group).\n    Results should be interpreted with extreme caution. Power is very low.\n\n    Args:\n        data: pandas DataFrame with columns:\n              'document_name', 'positive_sentiment_raw', 'negative_sentiment_raw',\n              'positive_sentiment_salience', 'positive_sentiment_confidence',\n              'negative_sentiment_salience', 'negative_sentiment_confidence'.\n\n    Returns:\n        dict: A dictionary containing t-test results (t-statistic, p-value, effect size)\n              for each sentiment dimension and derived metric. Returns None if data is\n              insufficient or malformed.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy import stats\n    import json\n    from pathlib import Path\n\n    # Helper function to load data from JSON files (as defined in the preamble)\n    def _load_analysis_data(data_type=\"raw\"):\n        if data_type == \"raw\":\n            file_pattern = \"*_raw_analysis_sample.json\"\n        elif data_type == \"derived\":\n            file_pattern = \"*_derived_metrics_sample.json\"\n        else:\n            raise ValueError(\"data_type must be 'raw' or 'derived'\")\n\n        all_data = []\n        current_dir = Path('.')\n        for file_path in current_dir.glob(file_pattern):\n            try:\n                with open(file_path, 'r') as f:\n                    data_content = json.load(f)\n                    for entry in data_content:\n                        if 'analysis_result' in entry and 'result_content' in entry['analysis_result']:\n                            result_content = entry['analysis_result']['result_content']\n                            if 'document_analyses' in result_content:\n                                for doc_analysis in result_content['document_analyses']:\n                                    doc_name = doc_analysis.get('document_name')\n                                    if doc_name:\n                                        scores = doc_analysis.get('dimensional_scores', {})\n                                        positive_sentiment = scores.get('positive_sentiment', {})\n                                        negative_sentiment = scores.get('negative_sentiment', {})\n\n                                        row = {\n                                            'document_name': doc_name,\n                                            'positive_sentiment_raw': positive_sentiment.get('raw_score'),\n                                            'positive_sentiment_salience': positive_sentiment.get('salience'),\n                                            'positive_sentiment_confidence': positive_sentiment.get('confidence'),\n                                            'negative_sentiment_raw': negative_sentiment.get('raw_score'),\n                                            'negative_sentiment_salience': negative_sentiment.get('salience'),\n                                            'negative_sentiment_confidence': negative_sentiment.get('confidence'),\n                                        }\n                                        all_data.append(row)\n            except Exception as e:\n                print(f\"Error processing file {file_path}: {e}\")\n                continue\n        \n        if not all_data:\n            return pd.DataFrame()\n\n        df = pd.DataFrame(all_data)\n        for col in df.columns:\n            if col != 'document_name':\n                df[col] = pd.to_numeric(df[col], errors='coerce')\n        return df\n\n    # Helper function to get sentiment category mapping (as defined in the preamble)\n    def _get_sentiment_category_mapping():\n        return {\n            \"negative_test_1.txt\": \"negative\",\n            \"negative_test_2.txt\": \"negative\",\n            \"positive_test_1.txt\": \"positive\",\n            \"positive_test_2.txt\": \"positive\",\n        }\n\n    # Helper function to prepare data with derived metrics and category\n    def _prepare_data_with_derived_metrics(data):\n        if data.empty:\n            return pd.DataFrame()\n\n        category_map = _get_sentiment_category_mapping()\n        data['sentiment_category'] = data['document_name'].map(category_map)\n\n        data['net_sentiment'] = data['positive_sentiment_raw'] - data['negative_sentiment_raw']\n        data['sentiment_magnitude'] = (data['positive_sentiment_raw'] + data['negative_sentiment_raw']) / 2\n\n        data['net_sentiment'] = data['net_sentiment'].clip(-1.0, 1.0)\n        data['sentiment_magnitude'] = data['sentiment_magnitude'].clip(0.0, 1.0)\n\n        return data\n\n    # --- Main logic for compare_sentiment_groups_ttest ---\n    if data.empty:\n        print(\"Input data is empty. Cannot perform t-tests.\")\n        return None\n\n    prepared_data = _prepare_data_with_derived_metrics(data)\n\n    if prepared_data.empty or 'sentiment_category' not in prepared_data.columns or prepared_data['sentiment_category'].isnull().all():\n        print(\"Data preparation failed or no valid sentiment categories found. Cannot perform t-tests.\")\n        return None\n\n    if prepared_data['sentiment_category'].nunique() < 2:\n        print(\"Less than two sentiment categories found. Cannot perform group comparison.\")\n        return None\n\n    sentiment_dims = [\n        'positive_sentiment_raw', 'negative_sentiment_raw',\n        'positive_sentiment_salience', 'negative_sentiment_salience',\n        'positive_sentiment_confidence', 'negative_sentiment_confidence',\n        'net_sentiment', 'sentiment_magnitude'\n    ]\n    sentiment_dims = [col for col in sentiment_dims if col in prepared_data.columns]\n\n    if not sentiment_dims:\n        print(\"No valid sentiment dimension columns found for t-tests.\")\n        return None\n\n    results = {}\n    group1 = 'positive'\n    group2 = 'negative'\n\n    if group1 not in prepared_data['sentiment_category'].unique() or group2 not in prepared_data['sentiment_category'].unique():\n        print(f\"Warning: One or both groups ('{group1}', '{group2}') not found in data. Cannot perform t-tests.\")\n        return None\n\n    for dim in sentiment_dims:\n        group1_data = prepared_data[prepared_data['sentiment_category'] == group1][dim].dropna()\n        group2_data = prepared_data[prepared_data['sentiment_category'] == group2][dim].dropna()\n\n        if len(group1_data) < 2 or len(group2_data) < 2:\n            print(f\"Warning: Insufficient sample size for t-test on '{dim}' (N={len(group1_data)} vs N={len(group2_data)}). Skipping.\")\n            results[dim] = {\"t_statistic\": None, \"p_value\": None, \"cohens_d\": None, \"power_caveat\": \"TIER 3: N<8 per group, very low power. Results are exploratory.\"}\n            continue\n\n        try:\n            t_stat, p_val = stats.ttest_ind(group1_data, group2_data, equal_var=False) # Welch's t-test\n\n            mean1, mean2 = np.mean(group1_data), np.mean(group2_data)\n            std1, std2 = np.std(group1_data, ddof=1), np.std(group2_data, ddof=1)\n            n1, n2 = len(group1_data), len(group2_data)\n\n            # Calculate Cohen's d with caution for N=2\n            if std1 == 0 and std2 == 0:\n                cohens_d = 0.0 if mean1 == mean2 else np.inf * np.sign(mean1 - mean2)\n            elif std1 == 0:\n                cohens_d = (mean1 - mean2) / std2\n            elif std2 == 0:\n                cohens_d = (mean1 - mean2) / std1\n            else:\n                cohens_d = (mean1 - mean2) / np.sqrt((std1**2 + std2**2) / 2)\n\n            results[dim] = {\n                \"t_statistic\": t_stat,\n                \"p_value\": p_val,\n                \"cohens_d\": cohens_d,\n                \"power_caveat\": \"TIER 3: N<8 per group, very low power. Results are exploratory.\"\n            }\n        except Exception as e:\n            print(f\"Error performing t-test on '{dim}': {e}\")\n            results[dim] = {\"t_statistic\": None, \"p_value\": None, \"cohens_d\": None, \"error\": str(e)}\n\n    return results\n\ndef compare_sentiment_groups_anova(data):\n    \"\"\"\n    Performs one-way ANOVA to compare sentiment dimensions and derived metrics\n    between 'positive' and 'negative' sentiment categories. Also includes Levene's test\n    for homogeneity of variances.\n\n    This is an exploratory analysis (TIER 3) due to the small sample size (N=2 per group).\n    Results should be interpreted with extreme caution. Power is very low.\n    Post-hoc tests are not recommended or feasible with N=2.\n\n    Args:\n        data: pandas DataFrame with columns:\n              'document_name', 'positive_sentiment_raw', 'negative_sentiment_raw',\n              'positive_sentiment_salience', 'positive_sentiment_confidence',\n              'negative_sentiment_salience', 'negative_sentiment_confidence'.\n\n    Returns:\n        dict: A dictionary containing ANOVA results (F-statistic, p-value, eta-squared)\n              and Levene's test results for each sentiment dimension and derived metric.\n              Returns None if data is insufficient or malformed.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy import stats\n    import json\n    from pathlib import Path\n\n    # Helper function to load data from JSON files (as defined in the preamble)\n    def _load_analysis_data(data_type=\"raw\"):\n        if data_type == \"raw\":\n            file_pattern = \"*_raw_analysis_sample.json\"\n        elif data_type == \"derived\":\n            file_pattern = \"*_derived_metrics_sample.json\"\n        else:\n            raise ValueError(\"data_type must be 'raw' or 'derived'\")\n\n        all_data = []\n        current_dir = Path('.')\n        for file_path in current_dir.glob(file_pattern):\n            try:\n                with open(file_path, 'r') as f:\n                    data_content = json.load(f)\n                    for entry in data_content:\n                        if 'analysis_result' in entry and 'result_content' in entry['analysis_result']:\n                            result_content = entry['analysis_result']['result_content']\n                            if 'document_analyses' in result_content:\n                                for doc_analysis in result_content['document_analyses']:\n                                    doc_name = doc_analysis.get('document_name')\n                                    if doc_name:\n                                        scores = doc_analysis.get('dimensional_scores', {})\n                                        positive_sentiment = scores.get('positive_sentiment', {})\n                                        negative_sentiment = scores.get('negative_sentiment', {})\n\n                                        row = {\n                                            'document_name': doc_name,\n                                            'positive_sentiment_raw': positive_sentiment.get('raw_score'),\n                                            'positive_sentiment_salience': positive_sentiment.get('salience'),\n                                            'positive_sentiment_confidence': positive_sentiment.get('confidence'),\n                                            'negative_sentiment_raw': negative_sentiment.get('raw_score'),\n                                            'negative_sentiment_salience': negative_sentiment.get('salience'),\n                                            'negative_sentiment_confidence': negative_sentiment.get('confidence'),\n                                        }\n                                        all_data.append(row)\n            except Exception as e:\n                print(f\"Error processing file {file_path}: {e}\")\n                continue\n        \n        if not all_data:\n            return pd.DataFrame()\n\n        df = pd.DataFrame(all_data)\n        for col in df.columns:\n            if col != 'document_name':\n                df[col] = pd.to_numeric(df[col], errors='coerce')\n        return df\n\n    # Helper function to get sentiment category mapping (as defined in the preamble)\n    def _get_sentiment_category_mapping():\n        return {\n            \"negative_test_1.txt\": \"negative\",\n            \"negative_test_2.txt\": \"negative\",\n            \"positive_test_1.txt\": \"positive\",\n            \"positive_test_2.txt\": \"positive\",\n        }\n\n    # Helper function to prepare data with derived metrics and category\n    def _prepare_data_with_derived_metrics(data):\n        if data.empty:\n            return pd.DataFrame()\n\n        category_map = _get_sentiment_category_mapping()\n        data['sentiment_category'] = data['document_name'].map(category_map)\n\n        data['net_sentiment'] = data['positive_sentiment_raw'] - data['negative_sentiment_raw']\n        data['sentiment_magnitude'] = (data['positive_sentiment_raw'] + data['negative_sentiment_raw']) / 2\n\n        data['net_sentiment'] = data['net_sentiment'].clip(-1.0, 1.0)\n        data['sentiment_magnitude'] = data['sentiment_magnitude'].clip(0.0, 1.0)\n\n        return data\n\n    # --- Main logic for compare_sentiment_groups_anova ---\n    if data.empty:\n        print(\"Input data is empty. Cannot perform ANOVA.\")\n        return None\n\n    prepared_data = _prepare_data_with_derived_metrics(data)\n\n    if prepared_data.empty or 'sentiment_category' not in prepared_data.columns or prepared_data['sentiment_category'].isnull().all():\n        print(\"Data preparation failed or no valid sentiment categories found. Cannot perform ANOVA.\")\n        return None\n\n    if prepared_data['sentiment_category'].nunique() < 2:\n        print(\"Less than two sentiment categories found. Cannot perform ANOVA.\")\n        return None\n\n    sentiment_dims = [\n        'positive_sentiment_raw', 'negative_sentiment_raw',\n        'positive_sentiment_salience', 'negative_sentiment_salience',\n        'positive_sentiment_confidence', 'negative_sentiment_confidence',\n        'net_sentiment', 'sentiment_magnitude'\n    ]\n    sentiment_dims = [col for col in sentiment_dims if col in prepared_data.columns]\n\n    if not sentiment_dims:\n        print(\"No valid sentiment dimension columns found for ANOVA.\")\n        return None\n\n    results = {}\n    group1 = 'positive'\n    group2 = 'negative'\n\n    if group1 not in prepared_data['sentiment_category'].unique() or group2 not in prepared_data['sentiment_category'].unique():\n        print(f\"Warning: One or both groups ('{group1}', '{group2}') not found in data. Cannot perform ANOVA.\")\n        return None\n\n    for dim in sentiment_dims:\n        group1_data = prepared_data[prepared_data['sentiment_category'] == group1][dim].dropna()\n        group2_data = prepared_data[prepared_data['sentiment_category'] == group2][dim].dropna()\n\n        if len(group1_data) < 2 or len(group2_data) < 2:\n            print(f\"Warning: Insufficient sample size for ANOVA on '{dim}' (N={len(group1_data)} vs N={len(group2_data)}). Skipping.\")\n            results[dim] = {\"anova_f_statistic\": None, \"anova_p_value\": None, \"eta_squared\": None, \"levene_stat\": None, \"levene_p_value\": None, \"power_caveat\": \"TIER 3: N<5 per group, very low power. Results are exploratory. Levene's test and ANOVA are unstable with N=2.\"}\n            continue\n\n        try:\n            levene_stat, levene_p_value = stats.levene(group1_data, group2_data)\n            f_stat, p_val = stats.f_oneway(group1_data, group2_data)\n\n            mean1, mean2 = np.mean(group1_data), np.mean(group2_data)\n            n1, n2 = len(group1_data), len(group2_data)\n            grand_mean = (mean1 * n1 + mean2 * n2) / (n1 + n2)\n\n            ss_between = n1 * (mean1 - grand_mean)**2 + n2 * (mean2 - grand_mean)**2\n            ss_within_group1 = np.sum((group1_data - mean1)**2)\n            ss_within_group2 = np.sum((group2_data - mean2)**2)\n            ss_total = ss_between + ss_within_group1 + ss_within_group2\n\n            eta_squared = ss_between / ss_total if ss_total != 0 else 0.0\n\n            results[dim] = {\n                \"anova_f_statistic\": f_stat,\n                \"anova_p_value\": p_val,\n                \"eta_squared\": eta_squared,\n                \"levene_stat\": levene_stat,\n                \"levene_p_value\": levene_p_value,\n                \"power_caveat\": \"TIER 3: N<5 per group, very low power. Results are exploratory. Levene's test and ANOVA are unstable with N=2.\"\n            }\n        except Exception as e:\n            print(f\"Error performing ANOVA on '{dim}': {e}\")\n            results[dim] = {\"anova_f_statistic\": None, \"anova_p_value\": None, \"eta_squared\": None, \"levene_stat\": None, \"levene_p_value\": None, \"error\": str(e)}\n\n    return results\n\ndef reliability_analysis(data):\n    \"\"\"\n    Attempts to calculate Cronbach's Alpha for the sentiment dimensions.\n    This is highly inappropriate and unreliable for N=2 per group and only two dimensions.\n    It is included to fulfill the explicit requirement but will yield meaningless results.\n\n    This is an exploratory analysis (TIER 3) due to the small sample size.\n    Results should be interpreted with extreme caution and are not meaningful.\n\n    Args:\n        data: pandas DataFrame with columns:\n              'document_name', 'positive_sentiment_raw', 'negative_sentiment_raw',\n              'positive_sentiment_salience', 'positive_sentiment_confidence',\n              'negative_sentiment_salience', 'negative_sentiment_confidence'.\n\n    Returns:\n        dict: A dictionary containing Cronbach's Alpha for the sentiment dimensions.\n              Returns None if data is insufficient or malformed.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    from pathlib import Path\n\n    # Helper function to load data from JSON files (as defined in the preamble)\n    def _load_analysis_data(data_type=\"raw\"):\n        if data_type == \"raw\":\n            file_pattern = \"*_raw_analysis_sample.json\"\n        elif data_type == \"derived\":\n            file_pattern = \"*_derived_metrics_sample.json\"\n        else:\n            raise ValueError(\"data_type must be 'raw' or 'derived'\")\n\n        all_data = []\n        current_dir = Path('.')\n        for file_path in current_dir.glob(file_pattern):\n            try:\n                with open(file_path, 'r') as f:\n                    data_content = json.load(f)\n                    for entry in data_content:\n                        if 'analysis_result' in entry and 'result_content' in entry['analysis_result']:\n                            result_content = entry['analysis_result']['result_content']\n                            if 'document_analyses' in result_content:\n                                for doc_analysis in result_content['document_analyses']:\n                                    doc_name = doc_analysis.get('document_name')\n                                    if doc_name:\n                                        scores = doc_analysis.get('dimensional_scores', {})\n                                        positive_sentiment = scores.get('positive_sentiment', {})\n                                        negative_sentiment = scores.get('negative_sentiment', {})\n\n                                        row = {\n                                            'document_name': doc_name,\n                                            'positive_sentiment_raw': positive_sentiment.get('raw_score'),\n                                            'positive_sentiment_salience': positive_sentiment.get('salience'),\n                                            'positive_sentiment_confidence': positive_sentiment.get('confidence'),\n                                            'negative_sentiment_raw': negative_sentiment.get('raw_score'),\n                                            'negative_sentiment_salience': negative_sentiment.get('salience'),\n                                            'negative_sentiment_confidence': negative_sentiment.get('confidence'),\n                                        }\n                                        all_data.append(row)\n            except Exception as e:\n                print(f\"Error processing file {file_path}: {e}\")\n                continue\n        \n        if not all_data:\n            return pd.DataFrame()\n\n        df = pd.DataFrame(all_data)\n        for col in df.columns:\n            if col != 'document_name':\n                df[col] = pd.to_numeric(df[col], errors='coerce')\n        return df\n\n    # Helper function to get sentiment category mapping (as defined in the preamble)\n    def _get_sentiment_category_mapping():\n        return {\n            \"negative_test_1.txt\": \"negative\",\n            \"negative_test_2.txt\": \"negative\",\n            \"positive_test_1.txt\": \"positive\",\n            \"positive_test_2.txt\": \"positive\",\n        }\n\n    # Helper function to prepare data with derived metrics and category\n    def _prepare_data_with_derived_metrics(data):\n        if data.empty:\n            return pd.DataFrame()\n\n        category_map = _get_sentiment_category_mapping()\n        data['sentiment_category'] = data['document_name'].map(category_map)\n\n        data['net_sentiment'] = data['positive_sentiment_raw'] - data['negative_sentiment_raw']\n        data['sentiment_magnitude'] = (data['positive_sentiment_raw'] + data['negative_sentiment_raw']) / 2\n\n        data['net_sentiment'] = data['net_sentiment'].clip(-1.0, 1.0)\n        data['sentiment_magnitude'] = data['sentiment_magnitude'].clip(0.0, 1.0)\n\n        return data\n\n    # --- Main logic for reliability_analysis ---\n    print(\"Warning: Cronbach's Alpha is not suitable for this experiment (N=2 per group, only 2 dimensions). Results will be meaningless.\")\n\n    if data.empty:\n        print(\"Input data is empty. Cannot perform reliability analysis.\")\n        return None\n\n    prepared_data = _prepare_data_with_derived_metrics(data)\n\n    if prepared_data.empty or 'sentiment_category' not in prepared_data.columns or prepared_data['sentiment_category'].isnull().all():\n        print(\"Data preparation failed or no valid sentiment categories found. Cannot perform reliability analysis.\")\n        return None\n\n    items = ['positive_sentiment_raw', 'negative_sentiment_raw']\n    items = [col for col in items if col in prepared_data.columns]\n\n    if len(items) < 2:\n        print(\"Need at least two sentiment dimensions to calculate Cronbach's Alpha.\")\n        return None\n\n    results = {}\n    try:\n        for category in prepared_data['sentiment_category'].unique():\n            subset_data = prepared_data[prepared_data['sentiment_category'] == category][items]\n\n            if subset_data.isnull().all().any():\n                print(f\"Warning: Missing values in all items for category '{category}'. Skipping reliability analysis for this group.\")\n                results[category] = {\"cronbachs_alpha\": None, \"power_caveat\": \"TIER 3: N=2, invalid for reliability. Results are meaningless.\"}\n                continue\n\n            if len(subset_data) < 2:\n                print(f\"Warning: Insufficient sample size for reliability analysis in category '{category}' (N={len(subset_data)}). Skipping.\")\n                results[category] = {\"cronbachs_alpha\": None, \"power_caveat\": \"TIER 3: N=2, invalid for reliability. Results are meaningless.\"}\n                continue\n\n            k = len(items)\n            item_variances = subset_data.var(ddof=1)\n            total_score_variance = subset_data.sum(axis=1).var(ddof=1)\n\n            if pd.isna(total_score_variance) or total_score_variance == 0:\n                cronbachs_alpha = 0.0 if k > 1 else np.nan\n            else:\n                sum_item_variances = item_variances.sum()\n                cronbachs_alpha = (k / (k - 1)) * (1 - (sum_item_variances / total_score_variance))\n\n            results[category] = {\n                \"cronbachs_alpha\": cronbachs_alpha,\n                \"power_caveat\": \"TIER 3: N=2, invalid for reliability. Results are meaningless.\"\n            }\n\n    except Exception as e:\n        print(f\"Error calculating Cronbach's Alpha: {e}\")\n        return None\n\n    return results\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}