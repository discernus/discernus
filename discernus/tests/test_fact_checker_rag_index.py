#!/usr/bin/env python3
"""
Unit tests for fact-checker RAG index creation functionality.

Tests the _build_fact_checker_rag_index method to ensure it:
1. Creates a comprehensive RAG index with all experiment assets
2. Includes framework, experiment, corpus, and analysis data
3. Handles errors gracefully
4. Returns a properly structured RAG index

This test avoids LLM calls by mocking the TxtaiEvidenceCurator.
"""

import pytest
from unittest.mock import Mock, patch, MagicMock
from pathlib import Path
import json
from datetime import datetime, timezone

from discernus.core.clean_analysis_orchestrator import CleanAnalysisOrchestrator, CleanAnalysisError


class TestFactCheckerRagIndex:
    """Test fact-checker RAG index creation functionality."""
    
    @pytest.fixture
    def mock_experiment_path(self, tmp_path):
        """Create a mock experiment directory structure."""
        experiment_dir = tmp_path / "test_experiment"
        experiment_dir.mkdir()
        
        # Create experiment.md with proper v10.0 format
        experiment_file = experiment_dir / "experiment.md"
        experiment_file.write_text("""# Test Experiment

## Research Questions
- What is the main theme?

## Configuration Appendix
```yaml
metadata:
  experiment_name: test_experiment
  spec_version: "10.0"
  description: "Test experiment for fact-checker RAG index testing"

components:
  framework: test_framework_v10.md
  corpus: test_corpus.md
  analysis_model: vertex_ai/gemini-2.5-flash
  synthesis_model: vertex_ai/gemini-2.5-flash

parameters:
  max_tokens: 4000
  temperature: 0.1
```
""")
        
        # Create framework file with realistic naming pattern
        framework_file = experiment_dir / "test_framework_v10.md"
        framework_file.write_text("""# Test Framework v10

## Dimensions
- Cohesion
- Tension
- Flourishing

## Purpose
Analyze political discourse for cohesive flourishing patterns.
""")
        
        # Create corpus file
        corpus_file = experiment_dir / "corpus.md"
        corpus_file.write_text("""# Test Corpus

## Documents
- test_doc1.txt
- test_doc2.txt

## Metadata
- total_documents: 2
- date_range: 2024-01-01 to 2024-01-31
""")
        
        # Create corpus directory with test documents
        corpus_dir = experiment_dir / "corpus"
        corpus_dir.mkdir()
        
        (corpus_dir / "test_doc1.txt").write_text("Test document 1 content about political discourse")
        (corpus_dir / "test_doc2.txt").write_text("Test document 2 content about civic engagement")
        
        # Create shared_cache directory
        shared_cache = experiment_dir / "shared_cache"
        shared_cache.mkdir()
        
        return experiment_dir
    
    @pytest.fixture
    def orchestrator(self, mock_experiment_path):
        """Create a CleanAnalysisOrchestrator instance."""
        return CleanAnalysisOrchestrator(mock_experiment_path)
    
    @pytest.fixture
    def mock_synthesis_result(self):
        """Mock synthesis result for testing."""
        return {
            'report_hash': 'mock_report_hash_12345',
            'status': 'completed',
            'report_content': 'Mock synthesis report content'
        }
    
    @pytest.fixture
    def mock_statistical_results(self):
        """Mock statistical results for testing."""
        return {
            'status': 'completed',
            'stats_hash': 'mock_stats_hash_67890',  # Added stats_hash
            'statistical_data': {
                'analyze_baseline_cohesion_tension': {
                    'type': 'dataframe',
                    'data': [
                        {'document': 'doc1', 'cohesion_score': 0.8, 'tension_score': 0.3},
                        {'document': 'doc2', 'cohesion_score': 0.7, 'tension_score': 0.4}
                    ],
                    'columns': ['document', 'cohesion_score', 'tension_score'],
                    'shape': (2, 3)
                }
            }
        }
    
    def test_method_exists_and_callable(self, orchestrator):
        """Test that the method exists and can be called."""
        assert hasattr(orchestrator, '_build_fact_checker_rag_index')
        assert callable(getattr(orchestrator, '_build_fact_checker_rag_index'))
    
    def test_method_signature(self, orchestrator):
        """Test that the method has the expected signature."""
        import inspect
        sig = inspect.signature(orchestrator._build_fact_checker_rag_index)
        params = list(sig.parameters.keys())
        assert params == ['self', 'synthesis_result', 'statistical_results']
    
    @patch('discernus.core.clean_analysis_orchestrator.TxtaiEvidenceCurator')
    def test_build_fact_checker_rag_index_basic_call(self, mock_curator_class, orchestrator, mock_synthesis_result, mock_statistical_results):
        """Test basic call to the method to see what happens."""
        # Mock the curator
        mock_curator = Mock()
        mock_curator_class.return_value = mock_curator
        mock_rag_index = Mock()
        mock_curator.build_rag_index.return_value = mock_rag_index
        
        # Mock the artifact storage to return mock content
        mock_artifact_content = b"Mock artifact content"
        orchestrator.artifact_storage.get_artifact.return_value = mock_artifact_content
        
        # Mock _load_corpus_documents to return test documents
        with patch.object(orchestrator, '_load_corpus_documents') as mock_load_corpus:
            mock_load_corpus.return_value = [
                {'content': 'Test document 1 content about political discourse', 'metadata': {'filename': 'test_doc1.txt'}},
                {'content': 'Test document 2 content about civic engagement', 'metadata': {'filename': 'test_doc2.txt'}}
            ]
            
            # Call the method under test
            result = orchestrator._build_fact_checker_rag_index(mock_synthesis_result, mock_statistical_results)
        
        # The method should have been called
        mock_curator_class.assert_called_once()
        
        # Check if build_rag_index was called
        if mock_curator.build_rag_index.called:
            print("âœ… build_rag_index was called successfully")
            call_args = mock_curator.build_rag_index.call_args[0][0]
            print(f"ðŸ“‹ Number of source documents: {len(call_args)}")
            for i, doc in enumerate(call_args):
                print(f"  {i+1}. {doc['metadata']['source_type']}: {doc['metadata']['filename']}")
        else:
            print("âŒ build_rag_index was NOT called")
            print(f"ðŸ” Method returned: {result}")
            print(f"ðŸ” Method type: {type(result)}")
        
        # For now, just verify the method didn't crash
        assert result is not None
    
    def test_debug_why_method_fails(self, orchestrator, mock_synthesis_result, mock_statistical_results):
        """Debug why the method is not building the RAG index."""
        print(f"\nðŸ” Debugging _build_fact_checker_rag_index method")
        print(f"ðŸ” Experiment path: {orchestrator.experiment_path}")
        print(f"ðŸ” Config: {orchestrator.config}")
        
        # Manually load the specs to populate the config
        try:
            orchestrator.config = orchestrator._load_specs()
            print(f"ðŸ” Config loaded: {orchestrator.config}")
        except Exception as e:
            print(f"ðŸ” Failed to load specs: {e}")
            import traceback
            traceback.print_exc()
            return
        
        # Check if required files exist
        experiment_path = orchestrator.experiment_path / "experiment.md"
        framework_path = orchestrator.experiment_path / orchestrator.config.get('framework', 'framework.md')
        corpus_path = orchestrator.experiment_path / "corpus.md"
        
        print(f"ðŸ” experiment.md exists: {experiment_path.exists()}")
        print(f"ðŸ” framework exists: {framework_path.exists()}")
        print(f"ðŸ” corpus.md exists: {corpus_path.exists()}")
        
        # Check if _load_corpus_documents method exists
        print(f"ðŸ” _load_corpus_documents method exists: {hasattr(orchestrator, '_load_corpus_documents')}")
        
        # Check if artifact_storage exists
        print(f"ðŸ” artifact_storage exists: {hasattr(orchestrator, 'artifact_storage')}")
        
        # Try to call the method and see what happens
        try:
            # Add more debugging to see what's happening in the method
            print(f"ðŸ” About to call _build_fact_checker_rag_index...")
            
            # Initialize artifact storage manually since we're not running the full experiment
            from discernus.core.local_artifact_storage import LocalArtifactStorage
            orchestrator.artifact_storage = LocalArtifactStorage(
                security_boundary=orchestrator.security,
                run_folder=orchestrator.experiment_path / "shared_cache",
                run_name="test_run"
            )
            print(f"ðŸ” Artifact storage initialized: {orchestrator.artifact_storage}")
            
            # Mock _load_corpus_documents to return test documents
            with patch.object(orchestrator, '_load_corpus_documents') as mock_load_corpus:
                mock_load_corpus.return_value = [
                    {'content': 'Test document 1 content about political discourse', 'metadata': {'filename': 'test_doc1.txt'}},
                    {'content': 'Test document 2 content about civic engagement', 'metadata': {'filename': 'test_doc2.txt'}}
                ]
                
                result = orchestrator._build_fact_checker_rag_index(mock_synthesis_result, mock_statistical_results)
                print(f"ðŸ” Method returned: {result}")
                print(f"ðŸ” Return type: {type(result)}")
        except Exception as e:
            print(f"ðŸ” Method failed with error: {e}")
            import traceback
            traceback.print_exc()
        
        # This test is just for debugging, so always pass
        assert True
    
    def test_build_fact_checker_rag_index_success(self, orchestrator, mock_synthesis_result, mock_statistical_results):
        """Test successful creation of fact-checker RAG index."""
        # Initialize artifact storage manually since we're not running the full experiment
        from discernus.core.local_artifact_storage import LocalArtifactStorage
        orchestrator.artifact_storage = LocalArtifactStorage(
            security_boundary=orchestrator.security,
            run_folder=orchestrator.experiment_path / "shared_cache",
            run_name="test_run"
        )
        
        # Mock _load_corpus_documents to return test documents
        with patch.object(orchestrator, '_load_corpus_documents') as mock_load_corpus:
            mock_load_corpus.return_value = [
                {'content': 'Test document 1 content about political discourse', 'metadata': {'filename': 'test_doc1.txt'}},
                {'content': 'Test document 2 content about civic engagement', 'metadata': {'filename': 'test_doc2.txt'}}
            ]
            
            # Call the method under test
            result = orchestrator._build_fact_checker_rag_index(mock_synthesis_result, mock_statistical_results)
        
        # Verify the method returned a txtai embeddings object
        assert result is not None
        assert hasattr(result, 'index')  # Should have index method
        assert hasattr(result, 'search')  # Should have search method
        
        # Verify it's a txtai embeddings instance
        from txtai.embeddings import Embeddings
        assert isinstance(result, Embeddings)
        
        print(f"âœ… Fact-checker RAG index created successfully: {type(result)}")
        print(f"âœ… RAG index has index method: {hasattr(result, 'index')}")
        print(f"âœ… RAG index has search method: {hasattr(result, 'search')}")
    
    def test_fact_checker_agent_logging(self, orchestrator, mock_synthesis_result, mock_statistical_results):
        """Test that the fact-checker agent logging works correctly."""
        # Initialize artifact storage
        from discernus.core.local_artifact_storage import LocalArtifactStorage
        orchestrator.artifact_storage = LocalArtifactStorage(
            security_boundary=orchestrator.security,
            run_folder=orchestrator.experiment_path / "shared_cache",
            run_name="test_run"
        )
        
        # Build the RAG index
        with patch.object(orchestrator, '_load_corpus_documents') as mock_load_corpus:
            mock_load_corpus.return_value = [
                {'content': 'Test document 1 content about political discourse', 'metadata': {'filename': 'test_doc1.txt'}},
                {'content': 'Test document 2 content about civic engagement', 'metadata': {'filename': 'test_doc2.txt'}}
            ]
            
            rag_index = orchestrator._build_fact_checker_rag_index(mock_synthesis_result, mock_statistical_results)
        
        # Test the fact-checker agent logging
        from discernus.agents.fact_checker_agent.agent import FactCheckerAgent
        from discernus.gateway.llm_gateway import LLMGateway
        from discernus.gateway.model_registry import ModelRegistry
        
        model_registry = ModelRegistry()
        llm_gateway = LLMGateway(model_registry)
        fact_checker = FactCheckerAgent(llm_gateway)
        
        # Test the source context method (this will show the logging)
        test_report = "This report mentions 'political discourse' and 'civic engagement'."
        context = fact_checker._get_source_context_for_check("Dimension Hallucination", test_report, rag_index)
        
        print(f"ðŸ” Source context retrieved: {len(context)} chars")
        print(f"ðŸ” Context preview: {context[:200]}...")
        
        # Verify the method worked
        assert context is not None
        assert len(context) > 0
