# Discernus Model Registry
# ==========================
#
# This file is the central knowledge base for all LLMs available to the SOAR platform.
# It is the single source of truth for resource management, cost estimation, and execution planning.

# --- Provider Defaults: Single Source of Truth for API Parameters ---
provider_defaults:
  vertex_ai:
    forbidden_params: ['max_tokens']
    required_params:
      safety_settings:
        - {category: "HARM_CATEGORY_HARASSMENT", threshold: "BLOCK_NONE"}
        - {category: "HARM_CATEGORY_HATE_SPEECH", threshold: "BLOCK_NONE"}
        - {category: "HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold: "BLOCK_NONE"}
        - {category: "HARM_CATEGORY_DANGEROUS_CONTENT", threshold: "BLOCK_NONE"}
    timeout: 180
  
  openai:
    forbidden_params: ['max_tokens']
    timeout: 120

  anthropic:
    forbidden_params: ['max_tokens']
    timeout: 120

  mistral:
    forbidden_params: ['max_tokens']
    timeout: 120
    
  perplexity:
    forbidden_params: ['max_tokens']
    timeout: 120
    status: 'placeholder'
    
  ollama:
    forbidden_params: ['max_tokens']
    timeout: 600 # Default for local models, can be overridden per model

# --- Model Definitions ---
models:
  # --- PREFERRED PROVIDER: VERTEX AI (STABILITY & PERFORMANCE) ---
  
  # VERTEX AI CLAUDE MODELS - TEMPORARILY COMMENTED OUT
  # These models are returning 404 errors and causing system delays
  # Uncomment when GCP project access is confirmed working
  
  # # CLAUDE 3.5 SONNET - RESTORED: Available in multiple regions with excellent rate limits
  # vertex_ai/claude-3-5-sonnet@20240620:
  #   provider: "vertex_ai"
  #   performance_tier: "top-tier"
  #   context_window: 200000
  #   costs:
  #     input_per_million_tokens: 3.00
  #     output_per_million_tokens: 15.00
  #   utility_tier: 5 # DEMOTED: Not available for this project - use direct Anthropic API instead
  #   task_suitability: [synthesis, coordination, planning]
  #   # Regional availability with rate limits
  #   regions:
  #     us-east5:
  #       tpm: 350000
  #       rpm: 80
  #     europe-west1:
  #       tpm: 600000
  #       rpm: 130
  #     asia-southeast1:
  #       tpm: 150000
  #       rpm: 35
  #   optimal_batch_size: 8
  #   last_updated: "2025-01-15"
  #   review_by: "2025-07-15"
  #   notes: "PREFERRED path for Claude 3.5 Sonnet due to high, stable rate limits on Vertex AI. Available in us-east5, europe-west1, and asia-southeast1."

  # # CLAUDE 3.7 SONNET - NEWER MODEL: Available on Vertex AI (March 2025)
  # vertex_ai/claude-3-7-sonnet@20250219:
  #   provider: "vertex_ai"
  #   performance_tier: "top-tier"
  #   context_window: 200000
  #   costs:
  #     input_per_million_tokens: 3.00
  #     output_per_million_tokens: 15.00
  #   utility_tier: 5 # DEMOTED: Not available for this project - use direct Anthropic API instead
  #   task_suitability: [synthesis, coordination, planning, reasoning]
  #   regions:
  #     us-east5:
  #       tpm: 500000
  #       rpm: 55
  #     europe-west1:
  #       tpm: 300000
  #       rpm: 40
  #     global:
  #       tpm: 300000
  #       rpm: 35
  #   optimal_batch_size: 8
  #   last_updated: "2025-01-15"
  #   review_by: "2025-07-15"
  #   notes: "Latest Claude model with enhanced reasoning capabilities. Supports extended thinking/reasoning_content."

  # # CLAUDE SONNET 4 - NEWEST MODEL: Available on Vertex AI (May 2025)
  # vertex_ai/claude-sonnet-4@20250514:
  #   provider: "vertex_ai"
  #   performance_tier: "top-tier"
  #   context_window: 200000
  #   costs:
  #     input_per_million_tokens: 3.00
  #     output_per_million_tokens: 15.00
  #   utility_tier: 1 # RESTORED: Model is accessible, needs quota increase
  #   task_suitability: [synthesis, coordination, planning, reasoning, coding]
  #   regions:
  #     us-east5:
  #       tpm: 280000  # Input TPM
  #       rpm: 35
  #     europe-west1:
  #       tpm: 180000  # Input TPM
  #       rpm: 25
  #     asia-east1:
  #       tpm: 550000  # Input TPM
  #       rpm: 70
  #     global:
  #       tpm: 276000  # Input TPM
  #       rpm: 35
  #   optimal_batch_size: 8
  #   last_updated: "2025-01-15"
  #   review_by: "2025-07-15"
  #   notes: "Latest Claude 4 model with superior intelligence. Supports extended thinking and computer use."

  # --- SPECIALIZED MODEL PROVIDER: OPENROUTER (ECOSYSTEM ACCESS) ---
  openrouter/perplexity/r1-1776:
    provider: "openrouter"
    performance_tier: "specialized-reasoning"
    context_window: 128000
    costs:
      input_per_million_tokens: 2.00
      output_per_million_tokens: 8.00
    utility_tier: 10 # High utility for its specific purpose
    task_suitability: [synthesis, validation]
    # OpenRouter has global, not per-model, rate limits. These are placeholders.
    tpm: 100000 # Placeholder
    rpm: 100    # Placeholder
    optimal_batch_size: 6
    last_updated: "2024-07-17"
    review_by: "2025-01-17"
    notes: "HEALTH CHECK FAILED (Empty Response). Requires investigation. Uncensored DeepSeek R1 fine-tune. Requires an OpenRouter API key."

  # --- DIRECT API PROVIDERS (FLEXIBILITY) ---
  anthropic/claude-3-5-sonnet-20240620:
    provider: "anthropic"
    performance_tier: "top-tier"
    context_window: 200000
    costs:
      input_per_million_tokens: 3.00
      output_per_million_tokens: 15.00
    utility_tier: 1 # PROMOTED: Higher priority until Vertex AI access is fixed
    task_suitability: [synthesis, coordination, planning]
    # Rate limits are for a standard starting tier and may vary.
    tpm: 40000
    rpm: 50
    optimal_batch_size: 8
    last_updated: "2024-07-17"
    review_by: "2025-01-17"
    notes: "Direct API access. Good for flexibility or if outside the GCP ecosystem."

  openai/gpt-4o:
    provider: "openai"
    performance_tier: "top-tier"
    context_window: 128000
    costs:
      input_per_million_tokens: 5.00
      output_per_million_tokens: 15.00
    utility_tier: 4
    task_suitability: [synthesis]
    # Rate limits are highly tier-dependent and not publicly fixed. Verify against your account.
    tpm: 300000 # Placeholder
    rpm: 5000   # Placeholder
    optimal_batch_size: 6
    last_updated: "2024-07-17"
    review_by: "2025-01-17"

  # --- GOOGLE MODELS (via Vertex AI) - PREFERRED ---
  vertex_ai/gemini-2.5-pro:
    provider: "vertex_ai"
    performance_tier: "top-tier"
    context_window: 2000000 # 2M token context
    costs:
      input_per_million_tokens: 3.50
      output_per_million_tokens: 10.50
    utility_tier: 3
    task_suitability: [synthesis, planning]
    tpm: 81920 # Published by Google for us-central1
    rpm: 800
    optimal_batch_size: 12
    last_updated: "2024-07-17"
    review_by: "2025-01-17"

  # --- COST-EFFECTIVE & FALLBACKS ---
  vertex_ai/gemini-2.5-flash:
    provider: "vertex_ai"
    performance_tier: "cost-effective"
    context_window: 1000000
    costs:
      input_per_million_tokens: 0.35
      output_per_million_tokens: 1.05
    utility_tier: 5
    task_suitability: [code_generation, coordination]
    tpm: 163840
    rpm: 800
    optimal_batch_size: 15
    last_updated: "2024-07-17"
    review_by: "2025-01-17"

  anthropic/claude-3-haiku-20240307:
    provider: "anthropic"
    performance_tier: "cost-effective"
    context_window: 200000
    costs:
      input_per_million_tokens: 0.25
      output_per_million_tokens: 1.25
    utility_tier: 4 # High utility, but secondary to top-tier models
    task_suitability:
      - validation # Explicitly assign for validation tasks
    # Rate limits are for a standard starting tier and may vary.
    tpm: 25000
    rpm: 500
    optimal_batch_size: 10
    last_updated: "2024-07-17"
    review_by: "2025-01-17"

  mistral/mistral-large-latest:
    provider: "mistral"
    performance_tier: "local-fallback"
    context_window: 32768
    costs:
      input_per_million_tokens: 0.00
      output_per_million_tokens: 0.00
    utility_tier: 98
    task_suitability: [synthesis]
    tpm: 1000000
    rpm: 10000
    optimal_batch_size: 2 # Can handle the framework plus one file comfortably.
    last_updated: "2024-07-17"
    review_by: "2025-01-17"

  ollama/jobautomation/OpenEuroLLM-Portuguese:
    provider: "ollama"
    performance_tier: "local-specialized"
    context_window: 8192 # Assumed, based on Llama architecture.
    costs:
      input_per_million_tokens: 0.00
      output_per_million_tokens: 0.00
    utility_tier: 97
    task_suitability: [synthesis]
    tpm: 1000000
    rpm: 10000
    optimal_batch_size: 1
    last_updated: "2024-07-17"
    review_by: "2025-01-17"
    notes: "Portuguese language model. Context window is an assumption. The PDAF framework (~10k tokens) exceeds this model's context window. Use with extreme caution."

  ollama/mistral:
    provider: "ollama"
    performance_tier: "local-general"
    context_window: 32768 # Mistral 7B context window
    costs:
      input_per_million_tokens: 0.00
      output_per_million_tokens: 0.00
    utility_tier: 3
    task_suitability: [development only]
    tpm: 1000000
    rpm: 10000
    optimal_batch_size: 1 # Reduced to 1 due to slow local processing
    timeout: 120 # 2 minute timeout for local model
    last_updated: "2024-07-17"
    review_by: "2025-01-17"
    notes: "Local Mistral 7B model. WARNING: Can be very slow with complex prompts. Consider using cloud models for production."

  # --- Template for Proprietary In-House Models ---
  # proprietary/my-company-model-v1:
  #   provider: "proprietary" 