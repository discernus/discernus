#!/usr/bin/env python3
"""
Enhanced Analysis Agent - Multi-Tool Implementation
==================================================

Implements the simplified multi-tool approach for complex framework analysis.
Uses three focused tools for structured output while leveraging Gemini's full context window.
"""

import json
import hashlib
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple

from discernus.core.security_boundary import ExperimentSecurityBoundary
from discernus.core.audit_logger import AuditLogger
from discernus.core.local_artifact_storage import LocalArtifactStorage
from discernus.gateway.llm_gateway_enhanced import EnhancedLLMGateway
from discernus.gateway.model_registry import ModelRegistry


class EnhancedAnalysisAgentMultiTool:
    """
    Enhanced Analysis Agent using simplified multi-tool approach.
    
    Uses three focused tools:
    1. record_analysis_scores - Dimensional scores with confidence/salience
    2. record_evidence_quotes - Evidence quotes and reasoning
    3. record_computational_work - Derived metrics and code execution
    """
    
    def __init__(
        self,
        security_boundary: ExperimentSecurityBoundary,
        audit_logger: AuditLogger,
        storage: LocalArtifactStorage,
        llm_gateway: EnhancedLLMGateway,
        model: str = "vertex_ai/gemini-2.5-pro"
    ):
        self.security_boundary = security_boundary
        self.audit_logger = audit_logger
        self.storage = storage
        self.llm_gateway = llm_gateway
        self.model = model
        
        # Load prompt template
        self.prompt_template = self._load_prompt_template()
        
    def _load_prompt_template(self) -> str:
        """Load the multi-tool prompt template"""
        prompt_path = Path(__file__).parent / "prompt_multi_tool.txt"
        if prompt_path.exists():
            return prompt_path.read_text()
        else:
            # Fallback prompt
            return """
You are an expert discourse analyst. Analyze the document using the provided framework and return structured results using the three provided tools.

**ANALYSIS REQUIREMENTS:**
- Apply the framework's dimensional definitions precisely
- Score each dimension on a 0.0-1.0 scale for intensity, salience, and confidence
- Provide specific textual evidence for each scoring decision

**THREE-PHASE INTERNAL PROCESSING:**
Internally, generate three independent analytical approaches:
1. Evidence-First Analysis: Focus on direct textual evidence and explicit statements
2. Context-Weighted Analysis: Emphasize rhetorical context, structural positioning, and thematic centrality
3. Pattern-Based Analysis: Look for repetition patterns, rhetorical devices, and strategic emphasis

Calculate the MEDIAN score for each dimension across all three approaches.
Select the BEST evidence quote from the three approaches for each dimension.

**OUTPUT REQUIREMENTS:**
Call the three tools in sequence:
1. record_analysis_scores - with your final aggregated scores
2. record_evidence_quotes - with your best evidence selections  
3. record_computational_work - with your derived metrics calculations

**DOCUMENT:**
{document_content}

**FRAMEWORK:**
{framework_content}
"""
    
    def _get_tools_schema(self, framework_content: str = None) -> List[Dict[str, Any]]:
        """Get the three-tool schema for structured output, dynamically generated from framework"""
        
        # Extract dimensions from framework content if provided
        dimensions = self._extract_framework_dimensions(framework_content) if framework_content else self._get_default_dimensions()
        
        # Build dynamic scores properties
        scores_properties = {}
        for dimension in dimensions:
            scores_properties[dimension] = {
                "type": "object",
                "properties": {
                    "raw_score": {"type": "number"},
                    "salience": {"type": "number"},
                    "confidence": {"type": "number"}
                },
                "required": ["raw_score", "salience", "confidence"]
            }
        
        return [
            {
                "type": "function",
                "function": {
                    "name": "record_analysis_scores",
                    "description": "Record dimensional scores with confidence and salience",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "document_id": {"type": "string"},
                            "framework_name": {"type": "string"},
                            "framework_version": {"type": "string"},
                            "scores": {
                                "type": "object",
                                "description": "Dimensional scores as key-value pairs where keys are dimension names and values contain raw_score, salience, and confidence",
                                "properties": scores_properties,
                                "additionalProperties": {
                                    "type": "object",
                                    "properties": {
                                        "raw_score": {"type": "number"},
                                        "salience": {"type": "number"},
                                        "confidence": {"type": "number"}
                                    },
                                    "required": ["raw_score", "salience", "confidence"]
                                }
                            }
                        },
                        "required": ["document_id", "framework_name", "framework_version", "scores"]
                    }
                }
            },
            {
                "type": "function", 
                "function": {
                    "name": "record_evidence_quotes",
                    "description": "Record evidence quotes and reasoning for each dimension",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "document_id": {"type": "string"},
                            "evidence": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "dimension": {"type": "string"},
                                        "quote": {"type": "string"},
                                        "reasoning": {"type": "string"}
                                    },
                                    "required": ["dimension", "quote", "reasoning"]
                                }
                            }
                        },
                        "required": ["document_id", "evidence"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "record_computational_work", 
                    "description": "Record derived metrics calculations and code execution",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "document_id": {"type": "string"},
                            "executed_code": {"type": "string"},
                            "execution_output": {"type": "string"},
                            "derived_metrics": {
                                "type": "object",
                                "additionalProperties": {"type": "number"}
                            }
                        },
                        "required": ["document_id", "executed_code", "execution_output", "derived_metrics"]
                    }
                }
            }
        ]
    
    def _create_analysis_prompt(self, document_content: str, framework_content: str) -> str:
        """Create the analysis prompt with document and framework content"""
        return self.prompt_template.format(
            document_content=document_content,
            framework_content=framework_content
        )
    
    def _save_analysis_artifact(self, scores_data: Dict[str, Any]) -> str:
        """Save analysis scores artifact"""
        # Convert to JSON bytes
        json_data = json.dumps(scores_data, indent=2, sort_keys=True)
        artifact_hash = self.storage.put_artifact(
            content=json_data.encode('utf-8'),
            metadata={
                "artifact_type": "analysis_scores",
                "source_agent": "EnhancedAnalysisAgentMultiTool"
            }
        )
        return artifact_hash
    
    def _save_evidence_artifact(self, evidence_data: Dict[str, Any]) -> str:
        """Save evidence quotes artifact"""
        # Convert to JSON bytes
        json_data = json.dumps(evidence_data, indent=2, sort_keys=True)
        artifact_hash = self.storage.put_artifact(
            content=json_data.encode('utf-8'),
            metadata={
                "artifact_type": "evidence_quotes",
                "source_agent": "EnhancedAnalysisAgentMultiTool"
            }
        )
        return artifact_hash
    
    def _save_work_artifact(self, work_data: Dict[str, Any]) -> str:
        """Save computational work artifact"""
        # Convert to JSON bytes
        json_data = json.dumps(work_data, indent=2, sort_keys=True)
        artifact_hash = self.storage.put_artifact(
            content=json_data.encode('utf-8'),
            metadata={
                "artifact_type": "computational_work",
                "source_agent": "EnhancedAnalysisAgentMultiTool"
            }
        )
        return artifact_hash
    
    def _extract_framework_dimensions(self, framework_content: str) -> List[str]:
        """Extract dimension names from framework content"""
        dimensions = []
        
        # Look for dimension definitions in the framework text
        lines = framework_content.split('\n')
        in_dimensions_section = False
        
        for line in lines:
            line = line.strip()
            
            # Look for dimension section markers
            if any(marker in line.lower() for marker in ['dimensions:', 'dimensions', '**dimensions**']):
                in_dimensions_section = True
                continue
            
            # Look for dimension definitions (lines starting with - or *)
            if in_dimensions_section and (line.startswith('-') or line.startswith('*')):
                # Extract dimension name from line like "- Positive Sentiment (0.0-1.0): ..."
                dimension_text = line.lstrip('-*').strip()
                if '(' in dimension_text:
                    dimension_name = dimension_text.split('(')[0].strip()
                else:
                    dimension_name = dimension_text.split(':')[0].strip()
                
                # Convert to snake_case
                dimension_name = dimension_name.lower().replace(' ', '_').replace('-', '_')
                dimensions.append(dimension_name)
            
            # Stop at next major section
            elif in_dimensions_section and line.startswith('#'):
                break
        
        # If no dimensions found in text, try to extract from YAML
        if not dimensions:
            dimensions = self._extract_dimensions_from_yaml(framework_content)
        
        # Filter out derived metrics and other non-dimension items
        filtered_dimensions = []
        for dim in dimensions:
            # Skip items that are clearly not dimensions
            if any(skip_word in dim.lower() for skip_word in ['derived', 'metrics', 'calculation', 'balance', 'magnitude']):
                continue
            # Only include items that look like dimensions (short, descriptive)
            if len(dim) < 50 and not ':' in dim:
                filtered_dimensions.append(dim)
        
        return filtered_dimensions if filtered_dimensions else self._get_default_dimensions()
    
    def _extract_dimensions_from_yaml(self, framework_content: str) -> List[str]:
        """Extract dimensions from YAML metadata in framework"""
        dimensions = []
        
        # Look for YAML section
        yaml_start = framework_content.find("```yaml")
        yaml_end = framework_content.find("```", yaml_start + 7)
        
        if yaml_start != -1 and yaml_end != -1:
            yaml_content = framework_content[yaml_start+7:yaml_end]
            
            # Simple extraction of dimension names from YAML
            # This is a basic implementation - could be enhanced with proper YAML parsing
            for line in yaml_content.split('\n'):
                line = line.strip()
                if 'dimension' in line.lower() and ':' in line:
                    # Extract dimension name
                    dimension_name = line.split(':')[0].strip()
                    if dimension_name and not dimension_name.startswith('#'):
                        dimensions.append(dimension_name)
        
        return dimensions
    
    def _create_default_scores_data(self, document_id: str, framework_content: str) -> Dict[str, Any]:
        """Create default scores data when LLM doesn't provide it"""
        dimensions = self._extract_dimensions_from_yaml(framework_content)
        if not dimensions:
            dimensions = self._get_default_dimensions()
        
        scores = {}
        for dimension in dimensions:
            scores[dimension] = {
                "raw_score": 0.0,
                "salience": 0.0,
                "confidence": 0.0
            }
        
        return {
            "document_id": document_id,
            "framework_name": "Default Framework",
            "framework_version": "1.0",
            "scores": scores
        }
    
    def _create_default_evidence_data(self, document_id: str) -> Dict[str, Any]:
        """Create default evidence data when LLM doesn't provide it"""
        return {
            "document_id": document_id,
            "evidence": [
                {
                    "dimension": "default",
                    "quote": "No evidence provided by LLM",
                    "reasoning": "LLM did not call record_evidence_quotes tool",
                    "confidence": 0.0
                }
            ]
        }
    
    def _create_default_work_data(self, document_id: str) -> Dict[str, Any]:
        """Create default work data when LLM doesn't provide it"""
        return {
            "document_id": document_id,
            "executed_code": "# No code executed by LLM\nprint('No computational work provided')",
            "execution_output": "No computational work provided",
            "derived_metrics": {}
        }
    
    def _get_default_dimensions(self) -> List[str]:
        """Get default PDAF dimensions as fallback"""
        return [
            "manichaean_people_elite_framing",
            "crisis_restoration_temporal_narrative", 
            "popular_sovereignty_claims",
            "anti_pluralist_exclusion",
            "elite_conspiracy_systemic_corruption",
            "authenticity_vs_political_class",
            "homogeneous_people_construction",
            "nationalist_exclusion",
            "economic_populist_appeals"
        ]
    
    def analyze_document(
        self, 
        document_content: str, 
        framework_content: str,
        document_id: str = "test_document"
    ) -> Dict[str, Any]:
        """
        Analyze a document using the multi-tool approach.
        
        Args:
            document_content: The text content to analyze
            framework_content: The framework specification content
            document_id: Unique identifier for the document
            
        Returns:
            Dictionary containing analysis results and artifact IDs
        """
        try:
            # Create analysis prompt
            prompt = self._create_analysis_prompt(document_content, framework_content)
            print(f"   üìù Generated prompt length: {len(prompt)} characters")
            print(f"   üìù Framework content preview: {framework_content[:200]}...")
            
            # Get tools schema (dynamically generated from framework)
            tools = self._get_tools_schema(framework_content)
            
            # Log the analysis request
            self.audit_logger.log_agent_event(
                agent_name="EnhancedAnalysisAgentMultiTool",
                event_type="analysis_start",
                data={
                    "document_id": document_id,
                    "document_length": len(document_content),
                    "framework_length": len(framework_content),
                    "model": self.model
                }
            )
            
            # Execute LLM call with tools (force function calling to ensure all tools are used)
            print(f"   üîç Sending analysis prompt to {self.model}...")
            response, metadata = self.llm_gateway.execute_call_with_tools(
                model=self.model,
                prompt=prompt,
                system_prompt="You are a helpful assistant. You MUST call all three functions in your response. Complete STEP 1, STEP 2, and STEP 3 by calling record_analysis_scores, record_evidence_quotes, and record_computational_work respectively. Do not stop after the first function call.",
                tools=tools,
                force_function_calling=False  # Let model decide when to use tools (AUTO mode)
            )
            
            # Log the response
            self.audit_logger.log_agent_event(
                agent_name="EnhancedAnalysisAgentMultiTool", 
                event_type="analysis_complete",
                data={
                    "document_id": document_id,
                    "success": metadata.get('success', False),
                    "tool_calls": len(metadata.get('tool_calls', [])),
                    "usage": metadata.get('usage', {})
                }
            )
            
            if not metadata.get('success') or not metadata.get('tool_calls'):
                raise Exception(f"LLM call failed: {metadata.get('error', 'Unknown error')}")
            
            # Process tool calls
            tool_calls = metadata.get('tool_calls') or []
            print(f"   üîß Processing {len(tool_calls)} tool calls (expected 3)...")
            if len(tool_calls) < 1:
                print(f"   ‚ùå Expected at least 1 tool call, got {len(tool_calls)}")
                raise Exception(f"Expected at least 1 tool call, got {len(tool_calls)}")

            if len(tool_calls) < 3:
                print(f"   ‚ö†Ô∏è  Warning: Expected 3 tool calls, got {len(tool_calls)}")
                print(f"   Continuing with partial results...")
            
            # Extract data from each tool call
            scores_data = None
            evidence_data = None
            work_data = None
            
            for tool_call in tool_calls:
                if hasattr(tool_call, 'function'):
                    function_name = tool_call.function.name
                    print(f"   üîß Processing tool: {function_name}")
                    arguments = json.loads(tool_call.function.arguments)

                    if function_name == "record_analysis_scores":
                        scores_data = arguments
                        print(f"   ‚úÖ Found analysis scores data")
                        print(f"   üìä Scores object: {arguments.get('scores', {})}")
                    elif function_name == "record_evidence_quotes":
                        evidence_data = arguments
                        print(f"   ‚úÖ Found evidence quotes data")
                    elif function_name == "record_computational_work":
                        work_data = arguments
                        print(f"   ‚úÖ Found computational work data")

            # Check what we have
            missing_tools = []
            if not scores_data:
                missing_tools.append("record_analysis_scores")
            if not evidence_data:
                missing_tools.append("record_evidence_quotes")
            if not work_data:
                missing_tools.append("record_computational_work")

            if missing_tools:
                print(f"   ‚ö†Ô∏è  Missing tool calls: {missing_tools}")
                print(f"   Creating default artifacts for missing tools...")

            # Create default data for missing tools
            if not scores_data:
                scores_data = self._create_default_scores_data(document_id, framework_content)
            if not evidence_data:
                evidence_data = self._create_default_evidence_data(document_id)
            if not work_data:
                work_data = self._create_default_work_data(document_id)

            if not scores_data.get('scores'):
                print("   ‚ö†Ô∏è  Warning: Scores object is empty, but continuing for debugging")
                # For now, allow empty scores to see what the LLM is sending
            
            # Save artifacts
            scores_artifact_id = self._save_analysis_artifact(scores_data)
            evidence_artifact_id = self._save_evidence_artifact(evidence_data)
            work_artifact_id = self._save_work_artifact(work_data)
            
            # Return results
            return {
                "success": True,
                "document_id": document_id,
                "scores_artifact": scores_artifact_id,
                "evidence_artifact": evidence_artifact_id,
                "work_artifact": work_artifact_id,
                "tool_calls_count": len(tool_calls),
                "usage": metadata.get('usage', {})
            }
            
        except Exception as e:
            # Log error
            self.audit_logger.log_agent_event(
                agent_name="EnhancedAnalysisAgentMultiTool",
                event_type="analysis_error", 
                data={
                    "document_id": document_id,
                    "error": str(e)
                }
            )
            raise
    
    def analyze_documents(self, corpus_documents: List[Dict[str, Any]], framework_content: str, 
                         experiment_config: Dict[str, Any], model: str = None) -> Dict[str, Any]:
        """
        Compatibility wrapper for the old analyze_documents interface.
        
        Args:
            corpus_documents: List of document dictionaries
            framework_content: The framework specification content
            experiment_config: Experiment configuration (unused but kept for compatibility)
            model: Model to use (unused, uses instance model)
            
        Returns:
            Dictionary with analysis_result containing the old format
        """
        if not corpus_documents:
            return {"analysis_result": None}
        
        # Process the first document (orchestrator processes one at a time)
        document = corpus_documents[0]
        document_content = document.get('content', '')
        document_id = document.get('filename', 'unknown')
        
        # Use the new analyze_document method
        result = self.analyze_document(document_content, framework_content, document_id)
        
        # Convert to old format for compatibility
        # Load the actual artifact data
        scores_data = self._load_artifact_data(result.get("scores_artifact"))
        evidence_data = self._load_artifact_data(result.get("evidence_artifact"))
        work_data = self._load_artifact_data(result.get("work_artifact"))
        
        # Create the old format with raw_analysis_response
        document_analysis = {
            "document_id": document_id,
            "document_name": document_id,
            "dimensional_scores": scores_data.get("scores", {}),
            "evidence": evidence_data.get("evidence", []),
            "computational_work": work_data
        }
        
        analysis_json = {
            "document_analyses": [document_analysis]
        }
        
        raw_response = f"<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n{json.dumps(analysis_json, indent=2)}\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>"
        
        return {
            "analysis_result": {
                "result_content": {
                    "scores": scores_data.get("scores", {}),
                    "evidence": evidence_data.get("evidence", []),
                    "work": work_data
                },
                "raw_analysis_response": raw_response,
                "cached": False,  # New agent doesn't use caching yet
                "success": result.get("success", True)
            }
        }
    
    def _load_artifact_data(self, artifact_id: str) -> Dict[str, Any]:
        """Load artifact data from storage"""
        if not artifact_id:
            return {}
        
        try:
            artifact_bytes = self.storage.get_artifact(artifact_id)
            return json.loads(artifact_bytes.decode('utf-8'))
        except Exception as e:
            print(f"Warning: Could not load artifact {artifact_id}: {e}")
            return {}