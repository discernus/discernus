"""
Dual-Purpose Results Interpreter for Academic Collaboration

Generates reports that serve both primary researchers (scannable) and 
academic collaborators (credible) with integrated evidence and transparency.
"""

import yaml
import json
import os
from datetime import datetime
from typing import Dict, Any, List, Optional
from dataclasses import dataclass

from ....core.audit_logger import AuditLogger
from ....gateway.llm_gateway import LLMGateway
from ....gateway.model_registry import ModelRegistry


@dataclass
class DualPurposeReportRequest:
    """Request for dual-purpose report generation."""
    experiment_name: str
    experiment_subtitle: str
    run_id: str
    execution_time_utc: str
    execution_time_local: str
    analysis_model: str
    synthesis_model: str
    framework_name: str
    framework_version: str
    document_count: int
    corpus_type: str
    corpus_composition: str
    statistical_results: Dict[str, Any]
    evidence_data: Dict[str, Any]
    scores_data: Dict[str, Any]
    run_directory: str
    cost_data: Dict[str, Any]


@dataclass
class DualPurposeReportResponse:
    """Response containing the dual-purpose report."""
    report_content: str
    scanner_section: str
    collaborator_section: str
    transparency_section: str
    success: bool
    error_message: Optional[str] = None


class DualPurposeResultsInterpreter:
    """
    Enhanced results interpreter that generates dual-purpose reports.
    
    Creates reports suitable for both:
    - Primary researchers (scannable access to key findings)
    - Academic collaborators (credibility assessment with transparency)
    """
    
    def __init__(self, model: str, audit_logger: AuditLogger):
        self.model = model
        self.audit_logger = audit_logger
        self.model_registry = ModelRegistry()
        self.client = LLMGateway(self.model_registry)
        self.template_path = os.path.join(
            os.path.dirname(__file__), 
            "dual_purpose_report_template.yaml"
        )
    
    def _load_template(self) -> str:
        """Load the dual-purpose report template."""
        try:
            with open(self.template_path, 'r') as f:
                template_data = yaml.safe_load(f)
                return template_data.get('template', '')
        except Exception as e:
            self.audit_logger.warning(f"Failed to load template: {e}")
            return self._create_default_template()
    
    def _create_default_template(self) -> str:
        """Create a default template if loading fails."""
        return """
# {experiment_name}
## {experiment_subtitle}

*Generated by Discernus - Computational Political Discourse Analysis Platform*

---

## üìä PROVENANCE & STATUS

*   **Run ID**: {run_id}
*   **Execution Time (UTC)**: {execution_time_utc}
*   **Execution Time (Local)**: {execution_time_local}
*   **Models Used**: Analysis: `{analysis_model}`, Synthesis: `{synthesis_model}`
*   **Framework**: {framework_name} {framework_version}
*   **Corpus Info**: Document Count: {document_count}, Type: {corpus_type}, Composition: {corpus_composition}
*   **Quality Status**: ‚úÖ All tasks completed successfully.

---

## üéØ EXECUTIVE SUMMARY

{executive_summary}

---

## üìä KEY RESULTS AT A GLANCE

### Hypothesis Testing Results

{hypothesis_table}

### Core Statistical Findings

{core_statistical_findings}

### Key Visualizations

{key_visualizations}

---

## üî¨ CONCISE METHODOLOGY

{operational_methodology}

---

## üìä DETAILED FINDINGS WITH INTEGRATED EVIDENCE

{detailed_findings_with_evidence}

---

## üõ†Ô∏è TRANSPARENCY APPENDIX

### Complete Audit Trail

This analysis generated {evidence_count} evidence entries linking every score to source text. The complete evidence trail is available in the experiment run directory:

*   **Evidence CSV**: `{run_directory}/results/evidence.csv` ({evidence_entries} entries)
*   **Scores CSV**: `{run_directory}/results/scores.csv` ({score_count} scores)
*   **Statistical Results**: `{run_directory}/results/statistical_results.csv`
*   **Full Log**: `{run_directory}/logs/` (complete execution audit trail)

### Replication Information

**Framework**: {framework_name} {framework_version} - {framework_description}
**Corpus**: {corpus_description}
**Analysis Method**: {analysis_method_description}
**Statistical Validation**: {validation_summary}

### Statement of Transparency

Full log linking every score to source text available in run directory: `{run_directory}`

This report was generated using the Discernus computational research platform, ensuring complete transparency in computational costs, methodology, and evidence trails for academic validation and replication.

---

## üí∞ RESEARCH TRANSPARENCY: COMPUTATIONAL COST ANALYSIS

### Cost Summary
**Total Cost**: {total_cost} USD  
**Total Tokens**: {total_tokens}  
**Run Timestamp**: {run_timestamp}  

### Cost Breakdown by Operation
{cost_breakdown}

### Methodology Note
This research was conducted using the Discernus computational research platform, ensuring complete transparency in computational costs. All LLM interactions are logged with exact token counts and costs for reproducibility and academic integrity.

**Cost Calculation**: Based on provider pricing at time of execution  
**Token Counting**: Exact tokens reported by LLM providers  
**Audit Trail**: Complete logs available in experiment run directory
"""
    
    def _generate_executive_summary(self, statistical_results: Dict[str, Any]) -> str:
        """Generate a concise executive summary with key takeaways."""
        prompt = f"""
Generate a concise executive summary (1-2 paragraphs) that provides the 1-3 most critical findings from this analysis.

Statistical Results: {json.dumps(statistical_results, indent=2)}

Focus on:
1. The most significant statistical findings
2. Key hypothesis support/rejection
3. Practical implications or insights

Write in clear, academic language suitable for both researchers and collaborators.
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=500,
                temperature=0.3
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            self.audit_logger.log_agent_event(
                "DualPurposeResultsInterpreter",
                "warning",
                {"error": f"Failed to generate executive summary: {e}"}
            )
            return "Analysis completed successfully with comprehensive statistical results."
    
    def _generate_hypothesis_table(self, statistical_results: Dict[str, Any]) -> str:
        """Generate a hypothesis testing results table."""
        prompt = f"""
Create a hypothesis testing results table showing which hypotheses were supported, partially supported, or rejected.

Statistical Results: {json.dumps(statistical_results, indent=2)}

Format as a markdown table with columns:
- Hypothesis
- Status (SUPPORTED/PARTIALLY SUPPORTED/REJECTED)
- Key Evidence
- Statistical Significance

Focus on the most important hypotheses from the analysis.
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=800,
                temperature=0.3
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            self.audit_logger.log_agent_event(
                "DualPurposeResultsInterpreter",
                "warning",
                {"error": f"Failed to generate hypothesis table: {e}"}
            )
            return "Hypothesis testing results available in detailed findings section."
    
    def _generate_core_statistical_findings(self, statistical_results: Dict[str, Any]) -> str:
        """Generate core statistical findings for scanner's view."""
        prompt = f"""
Extract and present the 3-5 most important statistical findings from this analysis.

Statistical Results: {json.dumps(statistical_results, indent=2)}

Present as bullet points with:
- Clear metric names
- Numerical values
- Brief interpretation
- Significance indicators

Focus on findings that would be most important for a researcher scanning the report.
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=600,
                temperature=0.3
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            self.audit_logger.log_agent_event(
                "DualPurposeResultsInterpreter",
                "warning",
                {"error": f"Failed to generate core findings: {e}"}
            )
            return "Core statistical findings available in detailed analysis section."
    
    def _generate_operational_methodology(self, framework_name: str, framework_version: str) -> str:
        """Generate concise operational methodology (1-2 paragraphs)."""
        prompt = f"""
Explain how the {framework_name} {framework_version} framework was operationalized in this analysis.

Focus on:
1. How abstract concepts became measurable instructions
2. How the analysis agent identified and extracted specific patterns
3. The operational process of converting text to numerical scores

Write 1-2 paragraphs suitable for academic collaborators who need to understand the methodology but aren't framework experts.

Example format: "To measure [concept], the analysis agent identified [specific patterns] and [extraction method]..."
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=400,
                temperature=0.3
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            self.audit_logger.log_agent_event(
                "DualPurposeResultsInterpreter",
                "warning",
                {"error": f"Failed to generate methodology: {e}"}
            )
            return f"Analysis conducted using {framework_name} {framework_version} with systematic pattern extraction and scoring."
    
    def _generate_detailed_findings_with_evidence(self, statistical_results: Dict[str, Any], evidence_data: Dict[str, Any]) -> str:
        """Generate detailed findings with integrated evidence."""
        prompt = f"""
Generate detailed findings that integrate statistical results with supporting evidence quotes.

Statistical Results: {json.dumps(statistical_results, indent=2)}
Evidence Data: {json.dumps(evidence_data, indent=2)}

Structure as:
1. Statistical finding
2. Immediate supporting quote from evidence
3. Brief interpretation connecting the two

Weave evidence directly into the text, don't relegate to references. Show clear connection between quantitative findings and qualitative evidence.

Focus on the most significant findings with the strongest evidence support.
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1200,
                temperature=0.3
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            self.audit_logger.log_agent_event(
                "DualPurposeResultsInterpreter",
                "warning",
                {"error": f"Failed to generate detailed findings: {e}"}
            )
            return "Detailed findings with integrated evidence available in the full analysis."
    
    def _format_cost_breakdown(self, cost_data: Dict[str, Any]) -> str:
        """Format cost breakdown for transparency."""
        if not cost_data:
            return "Cost data not available"
        
        breakdown = []
        for operation, data in cost_data.items():
            if isinstance(data, dict):
                cost = data.get('cost', 0)
                tokens = data.get('tokens', 0)
                calls = data.get('calls', 0)
                breakdown.append(f"- **{operation}**: ${cost:.4f} USD ({tokens:,} tokens, {calls} calls)")
        
        return "\n".join(breakdown) if breakdown else "Cost breakdown not available"
    
    def generate_dual_purpose_report(self, request: DualPurposeReportRequest) -> DualPurposeReportResponse:
        """Generate a dual-purpose report for academic collaboration."""
        try:
            self.audit_logger.log_agent_event(
                "DualPurposeResultsInterpreter",
                "start",
                {"operation": "generate_dual_purpose_report"}
            )
            
            # Generate report sections
            executive_summary = self._generate_executive_summary(request.statistical_results)
            hypothesis_table = self._generate_hypothesis_table(request.statistical_results)
            core_findings = self._generate_core_statistical_findings(request.statistical_results)
            methodology = self._generate_operational_methodology(request.framework_name, request.framework_version)
            detailed_findings = self._generate_detailed_findings_with_evidence(request.statistical_results, request.evidence_data)
            
            # Calculate evidence and score counts
            evidence_count = len(request.evidence_data.get('evidence', []))
            score_count = len(request.scores_data.get('scores', []))
            
            # Format cost data
            cost_breakdown = self._format_cost_breakdown(request.cost_data)
            
            # Load and populate template
            template = self._load_template()
            report_content = template.format(
                experiment_name=request.experiment_name,
                experiment_subtitle=request.experiment_subtitle,
                run_id=request.run_id,
                execution_time_utc=request.execution_time_utc,
                execution_time_local=request.execution_time_local,
                analysis_model=request.analysis_model,
                synthesis_model=request.synthesis_model,
                framework_name=request.framework_name,
                framework_version=request.framework_version,
                document_count=request.document_count,
                corpus_type=request.corpus_type,
                corpus_composition=request.corpus_composition,
                executive_summary=executive_summary,
                hypothesis_table=hypothesis_table,
                core_statistical_findings=core_findings,
                key_visualizations="Statistical visualizations available in detailed results",
                operational_methodology=methodology,
                detailed_findings_with_evidence=detailed_findings,
                evidence_count=evidence_count,
                evidence_entries=evidence_count,
                score_count=score_count,
                run_directory=request.run_directory,
                framework_description=f"{request.framework_name} {request.framework_version} analysis",
                corpus_description=f"{request.document_count} documents of {request.corpus_type}",
                analysis_method_description="Systematic pattern extraction and scoring",
                validation_summary="Comprehensive statistical validation completed",
                total_cost=request.cost_data.get('total_cost', '0.0000'),
                total_tokens=request.cost_data.get('total_tokens', '0'),
                run_timestamp=request.run_id,
                cost_breakdown=cost_breakdown
            )
            
            # Extract sections for response
            lines = report_content.split('\n')
            scanner_end = 0
            collaborator_end = 0
            
            for i, line in enumerate(lines):
                if '## üî¨ CONCISE METHODOLOGY' in line:
                    scanner_end = i
                elif '## üõ†Ô∏è TRANSPARENCY APPENDIX' in line:
                    collaborator_end = i
            
            scanner_section = '\n'.join(lines[:scanner_end])
            collaborator_section = '\n'.join(lines[scanner_end:collaborator_end])
            transparency_section = '\n'.join(lines[collaborator_end:])
            
            self.audit_logger.log_agent_event(
                "DualPurposeResultsInterpreter",
                "complete",
                {"operation": "generate_dual_purpose_report", "success": True}
            )
            
            return DualPurposeReportResponse(
                report_content=report_content,
                scanner_section=scanner_section,
                collaborator_section=collaborator_section,
                transparency_section=transparency_section,
                success=True
            )
            
        except Exception as e:
            self.audit_logger.log_agent_event(
                "DualPurposeResultsInterpreter",
                "error",
                {"error": str(e)}
            )
            return DualPurposeReportResponse(
                report_content="",
                scanner_section="",
                collaborator_section="",
                transparency_section="",
                success=False,
                error_message=str(e)
            ) 