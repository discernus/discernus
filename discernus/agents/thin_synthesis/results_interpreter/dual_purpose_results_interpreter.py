"""
Dual-Purpose Results Interpreter for Academic Collaboration

Generates reports that serve both primary researchers (scannable) and 
academic collaborators (credible) with integrated evidence and transparency.
"""

import os
import yaml
import json
import re
from typing import Dict, Any, List, Optional
from dataclasses import dataclass

from ....core.audit_logger import AuditLogger
from ....gateway.llm_gateway import LLMGateway
from ....gateway.model_registry import ModelRegistry


@dataclass
class DualPurposeReportRequest:
    """Request for dual-purpose report generation."""
    experiment_name: str
    experiment_subtitle: str
    run_id: str
    execution_time_utc: str
    execution_time_local: str
    analysis_model: str
    synthesis_model: str
    framework_name: str
    framework_version: str
    document_count: int
    corpus_type: str
    corpus_composition: str
    statistical_results: Dict[str, Any]
    evidence_data: Dict[str, Any]  # Legacy field for backward compatibility
    scores_data: Dict[str, Any]
    run_directory: str
    cost_data: Dict[str, Any]
    # Configuration options for flexibility
    template_path: Optional[str] = None
    section_markers: Optional[Dict[str, str]] = None
    raw_llm_curation: Optional[str] = None  # THIN: Raw LLM output from evidence curator


@dataclass
class DualPurposeReportResponse:
    """Response containing the dual-purpose report."""
    report_content: str
    scanner_section: str
    collaborator_section: str
    transparency_section: str
    success: bool
    error_message: Optional[str] = None


class DualPurposeResultsInterpreter:
    """
    Generates dual-purpose reports for academic collaboration.
    
    Features:
    - Configurable template discovery
    - Flexible section parsing
    - LLM-powered content generation
    - Academic transparency focus
    """
    
    def __init__(self, model: str, audit_logger: AuditLogger):
        self.model = model
        self.audit_logger = audit_logger
        self.model_registry = ModelRegistry()
        self.client = LLMGateway(self.model_registry)
        
        # Default template discovery paths (configurable)
        self.default_template_paths = [
            os.path.join(os.path.dirname(__file__), "dual_purpose_report_template.yaml"),
            os.path.join(os.path.dirname(__file__), "templates", "dual_purpose_report_template.yaml"),
            os.path.join(os.path.dirname(__file__), "..", "..", "..", "templates", "dual_purpose_report_template.yaml")
        ]
        
        # Default section markers (configurable)
        self.default_section_markers = {
            'scanner_end': ['## ðŸ”¬ CONCISE METHODOLOGY', '## CONCISE METHODOLOGY', '## METHODOLOGY'],
            'collaborator_end': ['## ðŸ› ï¸ TRANSPARENCY APPENDIX', '## TRANSPARENCY APPENDIX', '## APPENDIX']
        }
    
    def _discover_template(self, custom_path: Optional[str] = None) -> str:
        """Discover template using configurable paths."""
        # Try custom path first
        if custom_path and os.path.exists(custom_path):
            return custom_path
        
        # Try default paths in order
        for path in self.default_template_paths:
            if os.path.exists(path):
                return path
        
        # Fallback to default template
        return None
    
    def _load_template(self, custom_path: Optional[str] = None) -> str:
        """Load the dual-purpose report template with configurable discovery."""
        template_path = self._discover_template(custom_path)
        
        if template_path:
            try:
                with open(template_path, 'r') as f:
                    template_data = yaml.safe_load(f)
                    return template_data.get('template', '')
            except Exception as e:
                self.audit_logger.log_agent_event(
                    "DualPurposeResultsInterpreter",
                    "warning",
                    {"error": f"Failed to load template from {template_path}: {e}"}
                )
        
        # Fallback to default template
        return self._create_default_template()
    
    def _create_default_template(self) -> str:
        """Create a default template if loading fails."""
        return """
# {experiment_name}
## {experiment_subtitle}

*Generated by Discernus - Computational Political Discourse Analysis Platform*

---

## ðŸ“Š PROVENANCE & STATUS

*   **Run ID**: {run_id}
*   **Execution Time (UTC)**: {execution_time_utc}
*   **Execution Time (Local)**: {execution_time_local}
*   **Models Used**: Analysis: `{analysis_model}`, Synthesis: `{synthesis_model}`
*   **Framework**: {framework_name} {framework_version}
*   **Corpus Info**: Document Count: {document_count}, Type: {corpus_type}, Composition: {corpus_composition}
*   **Quality Status**: âœ… All tasks completed successfully.

---

## ðŸŽ¯ EXECUTIVE SUMMARY

{executive_summary}

---

## ðŸ“Š KEY RESULTS AT A GLANCE

### Hypothesis Testing Results

{hypothesis_table}

### Core Statistical Findings

{core_statistical_findings}

### Key Visualizations

{key_visualizations}

---

## ðŸ”¬ CONCISE METHODOLOGY

{operational_methodology}

---

## ðŸ“Š DETAILED FINDINGS WITH INTEGRATED EVIDENCE

{detailed_findings_with_evidence}

---

## ðŸ› ï¸ TRANSPARENCY APPENDIX

### Complete Audit Trail

This analysis generated {evidence_count} evidence entries linking every score to source text. The complete evidence trail is available in the experiment run directory:

*   **Evidence CSV**: `{run_directory}/results/evidence.csv` ({evidence_entries} entries)
*   **Scores CSV**: `{run_directory}/results/scores.csv` ({score_count} scores)
*   **Statistical Results**: `{run_directory}/results/statistical_results.csv`
*   **Full Log**: `{run_directory}/logs/` (complete execution audit trail)

### Replication Information

**Framework**: {framework_name} {framework_version} - {framework_description}
**Corpus**: {corpus_description}
**Analysis Method**: {analysis_method_description}
**Statistical Validation**: {validation_summary}

### Statement of Transparency

Full log linking every score to source text available in run directory: `{run_directory}`

This report was generated using the Discernus computational research platform, ensuring complete transparency in computational costs, methodology, and evidence trails for academic validation and replication.

---

## ðŸ’° RESEARCH TRANSPARENCY: COMPUTATIONAL COST ANALYSIS

### Cost Summary
**Total Cost**: {total_cost} USD  
**Total Tokens**: {total_tokens}  
**Run Timestamp**: {run_timestamp}  

### Cost Breakdown by Operation
{cost_breakdown}

### Methodology Note
This research was conducted using the Discernus computational research platform, ensuring complete transparency in computational costs. All LLM interactions are logged with exact token counts and costs for reproducibility and academic integrity.

**Cost Calculation**: Based on provider pricing at time of execution  
**Token Counting**: Exact tokens reported by LLM providers  
**Audit Trail**: Complete logs available in experiment run directory
"""
    
    def _generate_executive_summary(self, statistical_results: Dict[str, Any]) -> str:
        """Generate a concise executive summary with key takeaways."""
        prompt = f"""
Generate a concise executive summary (1-2 paragraphs) that provides the 1-3 most critical findings from this analysis.

Statistical Results: {json.dumps(statistical_results, indent=2)}

Focus on:
1. The most significant statistical findings
2. Key hypothesis support/rejection
3. Practical implications or insights

Write in clear, academic language suitable for both researchers and collaborators.
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            self.audit_logger.log_agent_event(
                "DualPurposeResultsInterpreter",
                "warning",
                {"error": f"Failed to generate executive summary: {e}"}
            )
            return "Analysis completed successfully with comprehensive statistical results."
    
    def _generate_hypothesis_table(self, statistical_results: Dict[str, Any]) -> str:
        """Generate a hypothesis testing results table."""
        prompt = f"""
Create a hypothesis testing results table showing which hypotheses were supported, partially supported, or rejected.

Statistical Results: {json.dumps(statistical_results, indent=2)}

Format as a markdown table with columns:
- Hypothesis
- Status (SUPPORTED/PARTIALLY SUPPORTED/REJECTED)
- Key Evidence
- Statistical Significance

Focus on the most important hypotheses from the analysis.
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            self.audit_logger.log_agent_event(
                "DualPurposeResultsInterpreter",
                "warning",
                {"error": f"Failed to generate hypothesis table: {e}"}
            )
            return "Hypothesis testing results available in detailed findings section."
    
    def _generate_core_statistical_findings(self, statistical_results: Dict[str, Any]) -> str:
        """Generate core statistical findings for scanner's view."""
        prompt = f"""
Extract and present the 3-5 most important statistical findings from this analysis.

Statistical Results: {json.dumps(statistical_results, indent=2)}

Present as bullet points with:
- Clear metric names
- Numerical values
- Brief interpretation
- Significance indicators

Focus on findings that would be most important for a researcher scanning the report.
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            self.audit_logger.log_agent_event(
                "DualPurposeResultsInterpreter",
                "warning",
                {"error": f"Failed to generate core findings: {e}"}
            )
            return "Core statistical findings available in detailed analysis section."
    
    def _generate_operational_methodology(self, framework_name: str, framework_version: str) -> str:
        """Generate concise operational methodology (1-2 paragraphs)."""
        prompt = f"""
Explain how the {framework_name} {framework_version} framework was operationalized in this analysis.

Focus on:
1. How abstract concepts became measurable instructions
2. How the analysis agent identified and extracted specific patterns
3. The operational process of converting text to numerical scores

Write 1-2 paragraphs suitable for academic collaborators who need to understand the methodology but aren't framework experts.

Example format: "To measure [concept], the analysis agent identified [specific patterns] and [extraction method]..."
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            self.audit_logger.log_agent_event(
                "DualPurposeResultsInterpreter",
                "warning",
                {"error": f"Failed to generate methodology: {e}"}
            )
            return f"Analysis conducted using {framework_name} {framework_version} with systematic pattern extraction and scoring."
    
    def _generate_detailed_findings_with_evidence(self, statistical_results: Dict[str, Any], evidence_data: Dict[str, Any], raw_llm_curation: Optional[str] = None) -> str:
        """Generate detailed findings with integrated evidence (THIN approach)."""
        
        # THIN: Use raw_llm_curation if available, otherwise fall back to structured evidence_data
        evidence_source = raw_llm_curation if raw_llm_curation else json.dumps(evidence_data, indent=2)
        
        prompt = f"""
Generate detailed findings that integrate statistical results with supporting evidence quotes.

Statistical Results: {json.dumps(statistical_results, indent=2)}
Evidence Source: {evidence_source}

Structure as:
1. Statistical finding
2. Immediate supporting quote from evidence
3. Brief interpretation connecting the two

Weave evidence directly into the text, don't relegate to references. Show clear connection between quantitative findings and qualitative evidence.

Focus on the most significant findings with the strongest evidence support.
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            self.audit_logger.log_agent_event(
                "DualPurposeResultsInterpreter",
                "warning",
                {"error": f"Failed to generate detailed findings: {e}"}
            )
            return "Detailed findings with integrated evidence available in the full analysis."
    
    def _format_cost_breakdown(self, cost_data: Dict[str, Any]) -> str:
        """Format cost breakdown for transparency using real audit logger data."""
        if not cost_data:
            return "Cost data not available"
        
        breakdown = []
        
        # Format operations breakdown (from audit logger)
        operations = cost_data.get('operations', {})
        for operation, data in operations.items():
            if isinstance(data, dict):
                cost = data.get('cost_usd', 0.0)
                tokens = data.get('tokens', 0)
                calls = data.get('calls', 0)
                breakdown.append(f"- **{operation}**: ${cost:.6f} USD ({tokens:,} tokens, {calls} calls)")
        
        # Format models breakdown (from audit logger)
        models = cost_data.get('models', {})
        for model, data in models.items():
            if isinstance(data, dict):
                cost = data.get('cost_usd', 0.0)
                tokens = data.get('tokens', 0)
                calls = data.get('calls', 0)
                breakdown.append(f"- **{model}**: ${cost:.6f} USD ({tokens:,} tokens, {calls} calls)")
        
        return "\n".join(breakdown) if breakdown else "Cost breakdown not available"
    
    def _parse_sections_flexibly(self, report_content: str, custom_markers: Optional[Dict[str, str]] = None) -> Dict[str, str]:
        """Parse report sections using flexible marker detection."""
        lines = report_content.split('\n')
        
        # Use custom markers or defaults
        markers = custom_markers or self.default_section_markers
        
        # Find section boundaries using flexible matching
        section_boundaries = {}
        
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # Check for scanner end markers
            if not section_boundaries.get('scanner_end'):
                for marker in markers['scanner_end']:
                    if marker in line_stripped:
                        section_boundaries['scanner_end'] = i
                        break
            
            # Check for collaborator end markers
            if not section_boundaries.get('collaborator_end'):
                for marker in markers['collaborator_end']:
                    if marker in line_stripped:
                        section_boundaries['collaborator_end'] = i
                        break
        
        # Extract sections with fallbacks
        scanner_end = section_boundaries.get('scanner_end', len(lines) // 3)  # Default to 1/3
        collaborator_end = section_boundaries.get('collaborator_end', len(lines) * 2 // 3)  # Default to 2/3
        
        return {
            'scanner_section': '\n'.join(lines[:scanner_end]),
            'collaborator_section': '\n'.join(lines[scanner_end:collaborator_end]),
            'transparency_section': '\n'.join(lines[collaborator_end:])
        }
    
    def generate_dual_purpose_report(self, request: DualPurposeReportRequest) -> DualPurposeReportResponse:
        """Generate a dual-purpose report for academic collaboration."""
        try:
            self.audit_logger.log_agent_event(
                "DualPurposeResultsInterpreter",
                "start",
                {"operation": "generate_dual_purpose_report"}
            )
            
            # Generate report sections
            executive_summary = self._generate_executive_summary(request.statistical_results)
            hypothesis_table = self._generate_hypothesis_table(request.statistical_results)
            core_findings = self._generate_core_statistical_findings(request.statistical_results)
            methodology = self._generate_operational_methodology(request.framework_name, request.framework_version)
            detailed_findings = self._generate_detailed_findings_with_evidence(
                request.statistical_results, 
                request.evidence_data, 
                request.raw_llm_curation
            )
            
            # Calculate evidence and score counts
            evidence_count = len(request.evidence_data.get('evidence', []))
            score_count = len(request.scores_data.get('scores', []))
            
            # Format cost data
            cost_breakdown = self._format_cost_breakdown(request.cost_data)
            
            # Load and populate template
            template = self._load_template(request.template_path)
            report_content = template.format(
                experiment_name=request.experiment_name,
                experiment_subtitle=request.experiment_subtitle,
                run_id=request.run_id,
                execution_time_utc=request.execution_time_utc,
                execution_time_local=request.execution_time_local,
                analysis_model=request.analysis_model,
                synthesis_model=request.synthesis_model,
                framework_name=request.framework_name,
                framework_version=request.framework_version,
                document_count=request.document_count,
                corpus_type=request.corpus_type,
                corpus_composition=request.corpus_composition,
                executive_summary=executive_summary,
                hypothesis_table=hypothesis_table,
                core_statistical_findings=core_findings,
                key_visualizations="Statistical visualizations available in detailed results",
                operational_methodology=methodology,
                detailed_findings_with_evidence=detailed_findings,
                evidence_count=evidence_count,
                evidence_entries=evidence_count,
                score_count=score_count,
                run_directory=request.run_directory,
                framework_description=f"{request.framework_name} {request.framework_version} analysis",
                corpus_description=f"{request.document_count} documents of {request.corpus_type}",
                analysis_method_description="Systematic pattern extraction and scoring",
                validation_summary="Comprehensive statistical validation completed",
                total_cost=request.cost_data.get('total_cost_usd', 0.0),
                total_tokens=request.cost_data.get('total_tokens', 0),
                run_timestamp=request.run_id,
                cost_breakdown=cost_breakdown
            )
            
            # Extract sections for response using flexible parsing
            sections = self._parse_sections_flexibly(report_content, request.section_markers)
            
            self.audit_logger.log_agent_event(
                "DualPurposeResultsInterpreter",
                "complete",
                {"operation": "generate_dual_purpose_report", "success": True}
            )
            
            return DualPurposeReportResponse(
                report_content=report_content,
                scanner_section=sections['scanner_section'],
                collaborator_section=sections['collaborator_section'],
                transparency_section=sections['transparency_section'],
                success=True
            )
            
        except Exception as e:
            self.audit_logger.log_agent_event(
                "DualPurposeResultsInterpreter",
                "error",
                {"error": str(e)}
            )
            return DualPurposeReportResponse(
                report_content="",
                scanner_section="",
                collaborator_section="",
                transparency_section="",
                success=False,
                error_message=str(e)
            ) 