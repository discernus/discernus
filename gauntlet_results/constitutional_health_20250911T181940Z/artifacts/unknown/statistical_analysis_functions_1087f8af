{
  "status": "success",
  "functions_generated": 10,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 32406,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: presidential_sotu_constitutional_health_trends\nDescription: Statistical analysis experiment\nGenerated: 2025-09-11T19:43:17.785543+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef _create_metadata_mapping(data):\n    \"\"\"\n    Internal helper to create administration and speech_type columns.\n\n    This function derives administration and speech type from the document_name,\n    based on the conventions described in the experiment specification. This\n    avoids parsing external files and makes the analysis self-contained.\n\n    Args:\n        data (pd.DataFrame): The input dataframe with a 'document_name' column.\n\n    Returns:\n        pd.DataFrame: The dataframe with 'administration' and 'speech_type' columns added.\n    \"\"\"\n    import pandas as pd\n\n    if 'document_name' not in data.columns:\n        raise ValueError(\"Input data must contain a 'document_name' column.\")\n\n    def get_administration(doc_name):\n        name = str(doc_name).lower()\n        # Specific check for Bush H.W. based on year or initials\n        if 'bush' in name and ('1992' in name or 'h.w.' in name or '_hw_' in name):\n            return 'Bush H.W.'\n        if 'clinton' in name:\n            return 'Clinton'\n        # General check for Bush W. after the more specific H.W. check\n        if 'bush' in name:\n            return 'Bush W.'\n        if 'obama' in name:\n            return 'Obama'\n        if 'trump' in name:\n            return 'Trump'\n        if 'biden' in name:\n            return 'Biden'\n        return 'Unknown'\n\n    def get_speech_type(doc_name):\n        name = str(doc_name).lower()\n        if 'sotu' in name or 'state of the union' in name:\n            return 'SOTU'\n        if 'inaugural' in name:\n            return 'Inaugural'\n        if 'joint_session' in name or 'joint session' in name:\n            return 'Joint Session'\n        return 'Other'\n\n    data['administration'] = data['document_name'].apply(get_administration)\n    data['speech_type'] = data['document_name'].apply(get_speech_type)\n    \n    # Ensure correct ordering for plots and tables\n    admin_order = ['Bush H.W.', 'Clinton', 'Bush W.', 'Obama', 'Trump', 'Biden']\n    data['administration'] = pd.Categorical(data['administration'], categories=admin_order, ordered=True)\n\n    return data\n\ndef _calculate_derived_metrics(data):\n    \"\"\"\n    Internal helper to calculate all derived metrics from the framework spec.\n\n    This function takes a DataFrame with raw and salience scores and computes\n    the axis-level and summary indices as defined in the Constitutional Health\n    Framework v10.0.\n\n    Args:\n        data (pd.DataFrame): DataFrame with columns for raw and salience scores\n                             (e.g., 'procedural_legitimacy_raw', 'procedural_legitimacy_salience').\n\n    Returns:\n        pd.DataFrame: The input DataFrame with added columns for all derived metrics.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    df = data.copy()\n    \n    # Define dimensions for clarity\n    dims = {\n        'proc_legit_raw': 'procedural_legitimacy_raw', 'proc_legit_sal': 'procedural_legitimacy_salience',\n        'proc_rej_raw': 'procedural_rejection_raw', 'proc_rej_sal': 'procedural_rejection_salience',\n        'inst_resp_raw': 'institutional_respect_raw', 'inst_resp_sal': 'institutional_respect_salience',\n        'inst_subv_raw': 'institutional_subversion_raw', 'inst_subv_sal': 'institutional_subversion_salience',\n        'sys_cont_raw': 'systemic_continuity_raw', 'sys_cont_sal': 'systemic_continuity_salience',\n        'sys_repl_raw': 'systemic_replacement_raw', 'sys_repl_sal': 'systemic_replacement_salience',\n    }\n    \n    # Check if all required columns exist\n    for col in dims.values():\n        if col not in df.columns:\n            raise ValueError(f\"Missing required column for calculation: {col}\")\n\n    # Intermediate salience totals (with small epsilon for stability)\n    epsilon = 0.001\n    df['procedural_health_salience_total'] = df[dims['proc_legit_sal']] + df[dims['proc_rej_sal']] + epsilon\n    df['institutional_health_salience_total'] = df[dims['inst_resp_sal']] + df[dims['inst_subv_sal']] + epsilon\n    df['systemic_health_salience_total'] = df[dims['sys_cont_sal']] + df[dims['sys_repl_sal']] + epsilon\n    df['total_constitutional_salience'] = (df['procedural_health_salience_total'] +\n                                           df['institutional_health_salience_total'] +\n                                           df['systemic_health_salience_total'])\n\n    # Axis-level health indices\n    df['procedural_health_index'] = ((df[dims['proc_legit_raw']] * df[dims['proc_legit_sal']]) - \n                                    (df[dims['proc_rej_raw']] * df[dims['proc_rej_sal']])) / df['procedural_health_salience_total']\n    \n    df['institutional_health_index'] = ((df[dims['inst_resp_raw']] * df[dims['inst_resp_sal']]) - \n                                        (df[dims['inst_subv_raw']] * df[dims['inst_subv_sal']])) / df['institutional_health_salience_total']\n\n    df['systemic_health_index'] = ((df[dims['sys_cont_raw']] * df[dims['sys_cont_sal']]) - \n                                   (df[dims['sys_repl_raw']] * df[dims['sys_repl_sal']])) / df['systemic_health_salience_total']\n\n    # Summary metrics\n    df['constitutional_health_index'] = (((df['procedural_health_index'] * df['procedural_health_salience_total']) +\n                                          (df['institutional_health_index'] * df['institutional_health_salience_total']) +\n                                          (df['systemic_health_index'] * df['systemic_health_salience_total'])) /\n                                         df['total_constitutional_salience'])\n\n    df['constitutional_pathology_index'] = (((df[dims['proc_rej_raw']] * df[dims['proc_rej_sal']]) +\n                                             (df[dims['inst_subv_raw']] * df[dims['inst_subv_sal']]) +\n                                             (df[dims['sys_repl_raw']] * df[dims['sys_repl_sal']])) /\n                                            df['total_constitutional_salience'])\n\n    return df\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics for key constitutional health metrics, grouped by administration.\n\n    This function provides a foundational overview of the data, addressing RQ1 by revealing\n    patterns in constitutional health across presidential rhetoric. It computes the mean,\n    standard deviation, count, and other summary stats for the main health and pathology indices.\n\n    Methodology:\n    - The function first calculates all derived metrics from the raw scores.\n    - It then groups the data by the 'administration' metadata field.\n    - Descriptive statistics are computed for 'constitutional_health_index',\n      'constitutional_pathology_index', and the three axis-level health indices.\n    - The output is a dictionary of JSON-formatted strings for easy consumption.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data with score and salience columns.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing descriptive statistics as a JSON string, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        if data.empty:\n            return {\"error\": \"Input data is empty.\"}\n\n        # Calculate derived metrics and add metadata\n        df_processed = _calculate_derived_metrics(data)\n        df_processed = _create_metadata_mapping(df_processed)\n\n        metrics_to_describe = [\n            'constitutional_health_index',\n            'constitutional_pathology_index',\n            'procedural_health_index',\n            'institutional_health_index',\n            'systemic_health_index'\n        ]\n        \n        if not all(metric in df_processed.columns for metric in metrics_to_describe):\n            return {\"error\": \"One or more required metrics for description are missing.\"}\n\n        # Group by administration and calculate descriptive statistics\n        desc_stats = df_processed.groupby('administration')[metrics_to_describe].describe()\n\n        # Unstack for a more readable format if needed, but describe() is standard\n        # For this use case, a multi-index DataFrame is fine.\n        \n        return {\n            \"descriptive_statistics_by_administration\": desc_stats.to_json(orient='index', indent=2)\n        }\n\n    except Exception as e:\n        return {\"error\": f\"An error occurred during descriptive statistics calculation: {str(e)}\"}\n\ndef perform_administration_anova(data, **kwargs):\n    \"\"\"\n    Performs a one-way ANOVA on the Constitutional Health Index across presidential administrations.\n\n    This function directly tests the primary hypothesis (H\u2081) that there are significant\n    differences in constitutional health scores among modern presidential administrations.\n\n    Methodology:\n    - The analysis uses a one-way between-subjects ANOVA.\n    - The independent variable is 'administration', and the dependent variable is 'constitutional_health_index'.\n    - The 'Bush H.W.' administration is excluded (n=1) as it serves only as a baseline.\n    - The function calculates the F-statistic, p-value, and effect size (eta-squared).\n    - Power Assessment: The total sample size (N=47 for ANOVA) is well-powered (Tier 1). However,\n      individual groups for Trump (n=7) and Biden (n=6) are moderately powered (Tier 2).\n      Results should be interpreted with this in mind, especially concerning post-hoc tests\n      involving these administrations.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary with ANOVA results (F-statistic, p-value, eta-squared), or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import f_oneway\n\n    try:\n        if data.empty or len(data) < 15:\n            return {\"warning\": \"Insufficient data for ANOVA (N < 15).\"}\n\n        df_processed = _calculate_derived_metrics(data)\n        df_processed = _create_metadata_mapping(df_processed)\n\n        # Exclude baseline administration with n=1\n        df_anova = df_processed[df_processed['administration'] != 'Bush H.W.'].copy()\n        \n        # Check for sufficient data after filtering\n        if df_anova['administration'].nunique() < 2:\n            return {\"warning\": \"Fewer than two administration groups with sufficient data for ANOVA.\"}\n        \n        groups = [df_anova['constitutional_health_index'][df_anova['administration'] == admin].dropna() \n                  for admin in df_anova['administration'].unique()]\n        \n        # Filter out any empty groups that might result from dropna\n        groups = [g for g in groups if not g.empty]\n        \n        if len(groups) < 2:\n            return {\"warning\": \"Fewer than two valid groups for ANOVA after handling NaNs.\"}\n\n        f_stat, p_value = f_oneway(*groups)\n\n        # Calculate Eta-squared (\u03b7\u00b2) for effect size\n        ss_between = sum(len(g) * (g.mean() - df_anova['constitutional_health_index'].mean())**2 for g in groups)\n        ss_total = sum((x - df_anova['constitutional_health_index'].mean())**2 for x in df_anova['constitutional_health_index'].dropna())\n        eta_squared = ss_between / ss_total if ss_total > 0 else 0\n\n        return {\n            \"test\": \"One-Way ANOVA on Constitutional Health Index by Administration\",\n            \"f_statistic\": f_stat,\n            \"p_value\": p_value,\n            \"eta_squared\": eta_squared,\n            \"alpha_threshold\": 0.05,\n            \"significant\": p_value < 0.05,\n            \"power_caveat\": \"Overall analysis is well-powered (N>30), but comparisons involving Trump (n=7) and Biden (n=6) administrations are moderately powered. Interpret post-hoc results with caution.\"\n        }\n\n    except Exception as e:\n        return {\"error\": f\"An error occurred during ANOVA: {str(e)}\"}\n\ndef perform_tukey_hsd_test(data, **kwargs):\n    \"\"\"\n    Performs Tukey's HSD post-hoc test for pairwise administration comparisons.\n\n    This function addresses the secondary hypotheses (H\u2082-H\u2085) by identifying which specific\n    pairs of administrations have significantly different mean constitutional_health_index scores,\n    following a significant ANOVA result.\n\n    Methodology:\n    - Uses the Tukey Honestly Significant Difference (HSD) test to control the family-wise error rate.\n    - Compares all pairs of the 5 modern administrations (Clinton, Bush W., Obama, Trump, Biden).\n    - Power Assessment: As with the ANOVA, comparisons involving administrations with smaller\n      sample sizes (Trump, n=7; Biden, n=6) are moderately powered (Tier 2) and should be\n      interpreted with caution.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing the Tukey HSD results as a JSON string, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n\n    try:\n        if data.empty or len(data) < 15:\n            return {\"warning\": \"Insufficient data for Tukey HSD test (N < 15).\"}\n\n        df_processed = _calculate_derived_metrics(data)\n        df_processed = _create_metadata_mapping(df_processed)\n\n        df_test = df_processed[df_processed['administration'] != 'Bush H.W.'].copy()\n        df_test.dropna(subset=['constitutional_health_index', 'administration'], inplace=True)\n\n        if df_test['administration'].nunique() < 2:\n            return {\"warning\": \"Fewer than two administration groups for Tukey HSD test.\"}\n\n        tukey_result = pairwise_tukeyhsd(endog=df_test['constitutional_health_index'],\n                                         groups=df_test['administration'],\n                                         alpha=0.05)\n        \n        results_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n\n        return {\n            \"test\": \"Tukey HSD for Pairwise Administration Comparisons\",\n            \"results_json\": results_df.to_json(orient='records', indent=2),\n            \"power_caveat\": \"Comparisons involving Trump (n=7) and Biden (n=6) are moderately powered. Lack of significance may be due to low power.\"\n        }\n\n    except Exception as e:\n        return {\"error\": f\"An error occurred during Tukey HSD test: {str(e)}\"}\n\ndef perform_levene_test(data, **kwargs):\n    \"\"\"\n    Performs Levene's test for homogeneity of variances across administrations.\n\n    This function tests an assumption of ANOVA and directly addresses hypothesis H\u2086, which\n    posits that the Trump administration shows significantly higher variance in constitutional\n    health scores than other administrations.\n\n    Methodology:\n    - Levene's test assesses the null hypothesis that all input samples are from populations\n      with equal variances. It is more robust to non-normality than Bartlett's test.\n    - The test is performed on the 'constitutional_health_index' across the 5 modern administrations.\n    - A significant result (p < 0.05) indicates that variances are not equal.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary with Levene's test results (statistic, p-value), or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import levene\n\n    try:\n        if data.empty or len(data) < 15:\n            return {\"warning\": \"Insufficient data for Levene's test (N < 15).\"}\n\n        df_processed = _calculate_derived_metrics(data)\n        df_processed = _create_metadata_mapping(df_processed)\n\n        df_test = df_processed[df_processed['administration'] != 'Bush H.W.'].copy()\n        \n        if df_test['administration'].nunique() < 2:\n            return {\"warning\": \"Fewer than two administration groups for Levene's test.\"}\n\n        groups = [df_test['constitutional_health_index'][df_test['administration'] == admin].dropna()\n                  for admin in df_test['administration'].unique()]\n        \n        groups = [g for g in groups if len(g) > 1] # Levene's test needs >1 observation per group\n\n        if len(groups) < 2:\n            return {\"warning\": \"Fewer than two valid groups for Levene's test after handling NaNs.\"}\n\n        stat, p_value = levene(*groups)\n\n        return {\n            \"test\": \"Levene's Test for Homogeneity of Variances\",\n            \"dependent_variable\": \"constitutional_health_index\",\n            \"statistic\": stat,\n            \"p_value\": p_value,\n            \"alpha_threshold\": 0.05,\n            \"variances_are_equal\": p_value >= 0.05,\n            \"interpretation\": \"A p-value < 0.05 suggests that the variance in constitutional health scores is not equal across all administrations.\"\n        }\n\n    except Exception as e:\n        return {\"error\": f\"An error occurred during Levene's test: {str(e)}\"}\n\ndef calculate_cronbachs_alpha(data, **kwargs):\n    \"\"\"\n    Calculates Cronbach's alpha to assess the internal consistency of constitutional health subscales.\n\n    This function assesses the reliability of the measurement framework by testing the\n    internal consistency of the 'health' dimensions (Legitimacy, Respect, Continuity) and\n    the 'pathology' dimensions (Rejection, Subversion, Replacement) as separate subscales.\n\n    Methodology:\n    - Cronbach's alpha is calculated for two subscales:\n      1. Health Subscale: Composed of the three positive raw scores.\n      2. Pathology Subscale: Composed of the three negative raw scores.\n    - The analysis is run on the entire dataset (N=48) to ensure sufficient power.\n    - Power Assessment: The total sample size (N=48) is well-powered (Tier 1) for this analysis.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary with Cronbach's alpha for the health and pathology subscales, or None on error.\n    \"\"\"\n    import pandas as pd\n    import pingouin as pg\n\n    try:\n        if data.empty or len(data) < 30:\n            return {\"warning\": f\"Exploratory analysis - results are suggestive rather than conclusive due to sample size (N={len(data)}). Recommended N>=30.\"}\n\n        health_dims = ['procedural_legitimacy_raw', 'institutional_respect_raw', 'systemic_continuity_raw']\n        pathology_dims = ['procedural_rejection_raw', 'institutional_subversion_raw', 'systemic_replacement_raw']\n\n        if not all(col in data.columns for col in health_dims + pathology_dims):\n            return {\"error\": \"Data is missing one or more required dimension columns for alpha calculation.\"}\n\n        health_alpha = pg.cronbach_alpha(data=data[health_dims])\n        pathology_alpha = pg.cronbach_alpha(data=data[pathology_dims])\n\n        return {\n            \"test\": \"Cronbach's Alpha for Internal Consistency of Subscales\",\n            \"health_subscale\": {\n                \"dimensions\": health_dims,\n                \"alpha\": health_alpha[0],\n                \"confidence_interval_95\": health_alpha[1]\n            },\n            \"pathology_subscale\": {\n                \"dimensions\": pathology_dims,\n                \"alpha\": pathology_alpha[0],\n                \"confidence_interval_95\": pathology_alpha[1]\n            },\n            \"interpretation_guideline\": \"Alpha > 0.9 (Excellent), > 0.8 (Good), > 0.7 (Acceptable), > 0.6 (Questionable), > 0.5 (Poor), < 0.5 (Unacceptable).\"\n        }\n\n    except Exception as e:\n        return {\"error\": f\"An error occurred during Cronbach's alpha calculation: {str(e)}\"}\n\ndef calculate_euclidean_distances(data, **kwargs):\n    \"\"\"\n    Calculates pairwise Euclidean distances between administration profiles in 6D space.\n\n    This function addresses hypothesis H\u2087 by quantifying the similarity/dissimilarity\n    between administrations. The distance is calculated based on the mean scores of the\n    six core constitutional dimensions.\n\n    Methodology:\n    - A 6-dimensional vector is created for each administration, representing the mean\n      raw score for each of the six core dimensions.\n    - The Euclidean distance is calculated between the vectors of every pair of administrations.\n    - A smaller distance indicates a more similar constitutional rhetoric profile.\n    - This is an exploratory analysis and does not involve significance testing.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing a matrix of pairwise distances as a JSON string, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.metrics.pairwise import euclidean_distances\n\n    try:\n        if data.empty:\n            return {\"error\": \"Input data is empty.\"}\n\n        df_processed = _create_metadata_mapping(data)\n        \n        dims = [\n            'procedural_legitimacy_raw', 'procedural_rejection_raw',\n            'institutional_respect_raw', 'institutional_subversion_raw',\n            'systemic_continuity_raw', 'systemic_replacement_raw'\n        ]\n        \n        if not all(d in df_processed.columns for d in dims):\n            return {\"error\": \"Data is missing one or more required dimension columns.\"}\n\n        # Calculate mean profile for each administration\n        admin_profiles = df_processed.groupby('administration')[dims].mean()\n        \n        if len(admin_profiles) < 2:\n            return {\"warning\": \"Fewer than two administrations to compare.\"}\n\n        # Calculate pairwise distances\n        dist_matrix = euclidean_distances(admin_profiles)\n        \n        dist_df = pd.DataFrame(dist_matrix, index=admin_profiles.index, columns=admin_profiles.index)\n\n        return {\n            \"test\": \"Pairwise Euclidean Distances Between Administration Profiles (6D Raw Score Space)\",\n            \"distance_matrix_json\": dist_df.to_json(orient='index', indent=2),\n            \"interpretation\": \"Lower values indicate greater similarity in constitutional rhetoric profiles.\"\n        }\n\n    except Exception as e:\n        return {\"error\": f\"An error occurred during Euclidean distance calculation: {str(e)}\"}\n\ndef perform_hierarchical_clustering(data, **kwargs):\n    \"\"\"\n    Performs hierarchical clustering to visualize relationships between administrations.\n\n    This function provides a visual test of hypothesis H\u2087 by grouping administrations\n    based on the similarity of their constitutional rhetoric profiles. The output can be\n    used to generate a dendrogram.\n\n    Methodology:\n    - Administration profiles are defined by their mean scores on the six core dimensions.\n    - Hierarchical/agglomerative clustering is performed using Ward's linkage method, which\n      minimizes the variance of the clusters being merged.\n    - The function returns the linkage matrix required to plot a dendrogram.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing the linkage matrix and labels for plotting, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.cluster.hierarchy import linkage\n\n    try:\n        if data.empty:\n            return {\"error\": \"Input data is empty.\"}\n\n        df_processed = _create_metadata_mapping(data)\n        \n        dims = [\n            'procedural_legitimacy_raw', 'procedural_rejection_raw',\n            'institutional_respect_raw', 'institutional_subversion_raw',\n            'systemic_continuity_raw', 'systemic_replacement_raw'\n        ]\n        \n        if not all(d in df_processed.columns for d in dims):\n            return {\"error\": \"Data is missing one or more required dimension columns.\"}\n\n        admin_profiles = df_processed.groupby('administration')[dims].mean().dropna()\n        \n        if len(admin_profiles) < 2:\n            return {\"warning\": \"Fewer than two administrations to cluster.\"}\n\n        # Perform clustering\n        # 'ward' minimizes the variance of the clusters being merged.\n        linkage_matrix = linkage(admin_profiles, method='ward', metric='euclidean')\n\n        return {\n            \"test\": \"Hierarchical Clustering of Administration Profiles\",\n            \"linkage_matrix\": linkage_matrix.tolist(),\n            \"labels\": admin_profiles.index.tolist(),\n            \"method\": \"Ward's linkage\",\n            \"metric\": \"Euclidean\",\n            \"interpretation\": \"The linkage matrix can be used to plot a dendrogram visualizing the similarity between administration profiles.\"\n        }\n\n    except Exception as e:\n        return {\"error\": f\"An error occurred during hierarchical clustering: {str(e)}\"}\n\ndef perform_speech_type_anova(data, **kwargs):\n    \"\"\"\n    Performs a one-way ANOVA on Constitutional Health Index across speech contexts.\n\n    This function addresses RQ3 by testing if constitutional health scores differ\n    significantly across speech types (SOTU, Inaugural, Joint Session).\n\n    Methodology:\n    - A one-way ANOVA is used with 'speech_type' as the independent variable.\n    - Power Assessment: The sample sizes for speech types are highly variable:\n      SOTU (n=35, Tier 1), Inaugural (n=9, Tier 2), Joint Session (n=4, Tier 3).\n      The analysis is therefore exploratory, especially concerning the 'Joint Session'\n      group. Results must be interpreted with extreme caution.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary with ANOVA results, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import f_oneway\n\n    try:\n        if data.empty:\n            return {\"error\": \"Input data is empty.\"}\n\n        df_processed = _calculate_derived_metrics(data)\n        df_processed = _create_metadata_mapping(df_processed)\n        \n        # Filter out groups with fewer than 3 samples for ANOVA stability\n        group_counts = df_processed['speech_type'].value_counts()\n        valid_groups = group_counts[group_counts >= 3].index\n        df_anova = df_processed[df_processed['speech_type'].isin(valid_groups)].copy()\n\n        if df_anova['speech_type'].nunique() < 2:\n            return {\"warning\": \"Fewer than two speech_type groups with sufficient data (n>=3) for ANOVA.\"}\n        \n        groups = [df_anova['constitutional_health_index'][df_anova['speech_type'] == stype].dropna() \n                  for stype in df_anova['speech_type'].unique()]\n        \n        groups = [g for g in groups if not g.empty]\n        \n        if len(groups) < 2:\n            return {\"warning\": \"Fewer than two valid groups for ANOVA after handling NaNs.\"}\n\n        f_stat, p_value = f_oneway(*groups)\n\n        return {\n            \"test\": \"One-Way ANOVA on Constitutional Health Index by Speech Type\",\n            \"f_statistic\": f_stat,\n            \"p_value\": p_value,\n            \"alpha_threshold\": 0.05,\n            \"significant\": p_value < 0.05,\n            \"power_caveat\": \"Exploratory analysis. Sample sizes are unbalanced (SOTU n=35, Inaugural n=9, Joint Session n=4). Results are suggestive, not conclusive, due to low power in smaller groups.\"\n        }\n\n    except Exception as e:\n        return {\"error\": f\"An error occurred during speech type ANOVA: {str(e)}\"}\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}