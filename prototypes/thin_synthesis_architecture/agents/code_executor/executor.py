#!/usr/bin/env python3
"""
CodeExecutor

This component executes generated Python analysis code in a sandboxed environment.
It's pure software - no LLM intelligence, just deterministic code execution.

Key Design Principles:
- Sandboxed execution: Restricted environment for security
- Deterministic: Same code + same data = same results
- Error handling: Comprehensive error capture and reporting
- Resource limits: Memory and execution time constraints
- Pure software: No LLM calls, just Python execution
"""

import json
import logging
import subprocess
import tempfile
import os
import sys
import traceback
import signal
from typing import Dict, Any, Optional, Tuple
from dataclasses import dataclass
from contextlib import contextmanager
import pandas as pd
import numpy as np
from scipy import stats

@dataclass
class CodeExecutionRequest:
    """Request structure for code execution."""
    analysis_code: str
    scores_csv_path: str
    evidence_csv_path: str
    working_directory: Optional[str] = None
    timeout_seconds: int = 300  # 5 minutes default

@dataclass
class CodeExecutionResponse:
    """Response structure containing execution results."""
    results: Dict[str, Any]
    success: bool
    execution_time_seconds: float
    stdout_output: str
    stderr_output: str
    error_message: Optional[str] = None

class ExecutionTimeoutError(Exception):
    """Raised when code execution exceeds timeout."""
    pass

class CodeExecutor:
    """
    Executes generated Python analysis code in a controlled environment.
    
    This is pure software - no LLM intelligence, just deterministic execution
    of the Python code generated by the AnalyticalCodeGenerator.
    """
    
    def __init__(self, max_memory_mb: int = 1024):
        """
        Initialize the CodeExecutor.
        
        Args:
            max_memory_mb: Maximum memory usage in MB
        """
        self.max_memory_mb = max_memory_mb
        self.logger = logging.getLogger(__name__)
        
    def execute_code(self, request: CodeExecutionRequest) -> CodeExecutionResponse:
        """
        Execute Python analysis code with the provided data.
        
        Args:
            request: CodeExecutionRequest containing code and data paths
            
        Returns:
            CodeExecutionResponse with execution results
        """
        import time
        start_time = time.time()
        
        try:
            # Validate inputs
            if not self._validate_request(request):
                return CodeExecutionResponse(
                    results={},
                    success=False,
                    execution_time_seconds=0.0,
                    stdout_output="",
                    stderr_output="",
                    error_message="Invalid request parameters"
                )
            
            # Execute code in controlled environment
            results, stdout, stderr = self._execute_in_subprocess(request)
            
            execution_time = time.time() - start_time
            
            return CodeExecutionResponse(
                results=results,
                success=True,
                execution_time_seconds=execution_time,
                stdout_output=stdout,
                stderr_output=stderr
            )
            
        except ExecutionTimeoutError:
            execution_time = time.time() - start_time
            return CodeExecutionResponse(
                results={},
                success=False,
                execution_time_seconds=execution_time,
                stdout_output="",
                stderr_output="",
                error_message=f"Execution timeout after {request.timeout_seconds} seconds"
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            self.logger.error(f"Code execution failed: {str(e)}")
            return CodeExecutionResponse(
                results={},
                success=False,
                execution_time_seconds=execution_time,
                stdout_output="",
                stderr_output="",
                error_message=str(e)
            )
    
    def _validate_request(self, request: CodeExecutionRequest) -> bool:
        """Validate the execution request."""
        
        # Check that code is provided
        if not request.analysis_code or not request.analysis_code.strip():
            self.logger.error("No analysis code provided")
            return False
        
        # Check that CSV files exist
        if not os.path.exists(request.scores_csv_path):
            self.logger.error(f"Scores CSV not found: {request.scores_csv_path}")
            return False
            
        if not os.path.exists(request.evidence_csv_path):
            self.logger.error(f"Evidence CSV not found: {request.evidence_csv_path}")
            return False
        
        return True
    
    def _execute_in_subprocess(self, request: CodeExecutionRequest) -> Tuple[Dict[str, Any], str, str]:
        """Execute code in a subprocess for isolation."""
        
        # Create temporary script file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as temp_file:
            # Write the analysis code to temporary file
            temp_file.write(request.analysis_code)
            temp_script_path = temp_file.name
        
        try:
            # Set up environment
            env = os.environ.copy()
            env['PYTHONPATH'] = os.pathsep.join(sys.path)
            
            # Change to working directory if specified
            cwd = request.working_directory or os.getcwd()
            
            # Execute the script
            process = subprocess.Popen(
                [sys.executable, temp_script_path],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                cwd=cwd,
                env=env,
                text=True
            )
            
            try:
                stdout, stderr = process.communicate(timeout=request.timeout_seconds)
            except subprocess.TimeoutExpired:
                process.kill()
                process.communicate()  # Clean up
                raise ExecutionTimeoutError(f"Code execution timeout after {request.timeout_seconds} seconds")
            
            if process.returncode != 0:
                raise RuntimeError(f"Script execution failed with return code {process.returncode}:\n{stderr}")
            
            # Try to parse JSON results from stdout
            results = self._extract_results_from_output(stdout)
            
            return results, stdout, stderr
            
        finally:
            # Clean up temporary file
            try:
                os.unlink(temp_script_path)
            except OSError:
                pass
    
    def _execute_in_process(self, request: CodeExecutionRequest) -> Tuple[Dict[str, Any], str, str]:
        """
        Execute code in the current process (alternative to subprocess).
        
        This is faster but less secure than subprocess execution.
        Use only for trusted code in development/testing.
        """
        
        # Capture stdout/stderr
        from io import StringIO
        import contextlib
        
        stdout_capture = StringIO()
        stderr_capture = StringIO()
        
        # Set up execution environment
        exec_globals = {
            '__name__': '__main__',
            'pd': pd,
            'pandas': pd,
            'np': np,
            'numpy': np,
            'stats': stats,
            'json': json,
            'os': os
        }
        
        # Set working directory
        original_cwd = os.getcwd()
        if request.working_directory:
            os.chdir(request.working_directory)
        
        try:
            with contextlib.redirect_stdout(stdout_capture), \
                 contextlib.redirect_stderr(stderr_capture):
                
                # Execute the code
                exec(request.analysis_code, exec_globals)
            
            stdout_output = stdout_capture.getvalue()
            stderr_output = stderr_capture.getvalue()
            
            # Extract results from stdout
            results = self._extract_results_from_output(stdout_output)
            
            return results, stdout_output, stderr_output
            
        finally:
            # Restore working directory
            os.chdir(original_cwd)
    
    def _extract_results_from_output(self, output: str) -> Dict[str, Any]:
        """Extract JSON results from script output."""
        
        # Look for JSON output in the script output
        lines = output.split('\n')
        
        for line in lines:
            line = line.strip()
            if line.startswith('{') and line.endswith('}'):
                try:
                    return json.loads(line)
                except json.JSONDecodeError:
                    continue
        
        # If no JSON found, look for results printed with specific markers
        json_start_marker = "=== ANALYSIS RESULTS ==="
        json_end_marker = "=== END RESULTS ==="
        
        try:
            start_idx = output.find(json_start_marker)
            end_idx = output.find(json_end_marker)
            
            if start_idx != -1 and end_idx != -1:
                json_content = output[start_idx + len(json_start_marker):end_idx].strip()
                return json.loads(json_content)
        except json.JSONDecodeError:
            pass
        
        # Fallback: return empty results with a note
        return {
            "error": "Could not extract JSON results from script output",
            "raw_output": output[:1000] + "..." if len(output) > 1000 else output
        }
    
    def create_test_data(self, scores_csv_path: str, evidence_csv_path: str, num_artifacts: int = 50):
        """
        Create synthetic test data for development and testing.
        
        Args:
            scores_csv_path: Path where to create scores CSV
            evidence_csv_path: Path where to create evidence CSV  
            num_artifacts: Number of artifacts to generate
        """
        
        # Generate synthetic scores data
        np.random.seed(42)  # For reproducible test data
        
        artifact_ids = [f"artifact_{i:03d}" for i in range(num_artifacts)]
        
        # Generate correlated virtue scores (higher values)
        virtue_base = np.random.beta(3, 2, num_artifacts)  # Skewed toward higher values
        virtue_noise = np.random.normal(0, 0.1, (num_artifacts, 5))
        
        virtue_scores = np.clip(virtue_base.reshape(-1, 1) + virtue_noise, 0, 1)
        
        # Generate correlated vice scores (lower values, inverse of virtues)
        vice_base = 1 - virtue_base + np.random.normal(0, 0.2, num_artifacts)
        vice_noise = np.random.normal(0, 0.1, (num_artifacts, 5))
        
        vice_scores = np.clip(vice_base.reshape(-1, 1) + vice_noise, 0, 1)
        
        # Create scores DataFrame
        scores_df = pd.DataFrame({
            'artifact_id': artifact_ids,
            'integrity_score': virtue_scores[:, 0],
            'courage_score': virtue_scores[:, 1], 
            'compassion_score': virtue_scores[:, 2],
            'justice_score': virtue_scores[:, 3],
            'wisdom_score': virtue_scores[:, 4],
            'corruption_score': vice_scores[:, 0],
            'cowardice_score': vice_scores[:, 1],
            'cruelty_score': vice_scores[:, 2],
            'injustice_score': vice_scores[:, 3],
            'folly_score': vice_scores[:, 4]
        })
        
        scores_df.to_csv(scores_csv_path, index=False)
        
        # Generate synthetic evidence data
        dimensions = ['integrity', 'courage', 'compassion', 'justice', 'wisdom',
                     'corruption', 'cowardice', 'cruelty', 'injustice', 'folly']
        
        evidence_data = []
        
        for artifact_id in artifact_ids:
            # Generate 2-4 evidence entries per artifact
            num_evidence = np.random.randint(2, 5)
            
            for _ in range(num_evidence):
                dimension = np.random.choice(dimensions)
                evidence_data.append({
                    'artifact_id': artifact_id,
                    'dimension': dimension,
                    'evidence_text': f"Sample evidence text for {dimension} in {artifact_id}",
                    'context': f"Context surrounding the {dimension} evidence",
                    'confidence': np.random.uniform(0.6, 1.0),
                    'reasoning': f"This quote demonstrates {dimension} because..."
                })
        
        evidence_df = pd.DataFrame(evidence_data)
        evidence_df.to_csv(evidence_csv_path, index=False)
        
        self.logger.info(f"Created test data: {len(scores_df)} artifacts, {len(evidence_df)} evidence entries")
        
        return scores_df, evidence_df 