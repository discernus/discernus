
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Experimental Design Framework for Narrative Analysis - Discernus Project</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.342714a4.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#experimental-design-framework-for-narrative-analysis" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Discernus Project" class="md-header__button md-logo" aria-label="Discernus Project" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Discernus Project
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Experimental Design Framework for Narrative Analysis
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Discernus Project" class="md-nav__button md-logo" aria-label="Discernus Project" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Discernus Project
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/DOCUMENTATION_INDEX.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Documentation Index
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/CONTRIBUTING.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Contributing
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/platform-development/DEV_ENVIRONMENT.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Development Environment
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/CODE_ORGANIZATION_STANDARDS.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Code Standards
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/platform-development/RELEASE_PROCESS.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Release Process
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      üéØ Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-five-dimensional-experimental-design-space" class="md-nav__link">
    <span class="md-ellipsis">
      üìê The Five-Dimensional Experimental Design Space
    </span>
  </a>
  
    <nav class="md-nav" aria-label="üìê The Five-Dimensional Experimental Design Space">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dimension-1-texts" class="md-nav__link">
    <span class="md-ellipsis">
      Dimension 1: TEXTS
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dimension-2-frameworks" class="md-nav__link">
    <span class="md-ellipsis">
      Dimension 2: FRAMEWORKS
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dimension-3-prompt-templates" class="md-nav__link">
    <span class="md-ellipsis">
      Dimension 3: PROMPT TEMPLATES
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dimension-4-weighting-schemes" class="md-nav__link">
    <span class="md-ellipsis">
      Dimension 4: WEIGHTING SCHEMES
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dimension-5-evaluators" class="md-nav__link">
    <span class="md-ellipsis">
      Dimension 5: EVALUATORS
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#experimental-design-methodologies" class="md-nav__link">
    <span class="md-ellipsis">
      üî¨ Experimental Design Methodologies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="üî¨ Experimental Design Methodologies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#single-factor-experiments" class="md-nav__link">
    <span class="md-ellipsis">
      Single-Factor Experiments
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#two-factor-experiments" class="md-nav__link">
    <span class="md-ellipsis">
      Two-Factor Experiments
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-factor-experiments" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-Factor Experiments
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#component-matrix-experiments" class="md-nav__link">
    <span class="md-ellipsis">
      Component Matrix Experiments
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#validation-studies" class="md-nav__link">
    <span class="md-ellipsis">
      Validation Studies
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#experimental-outcome-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      üìä Experimental Outcome Analysis
    </span>
  </a>
  
    <nav class="md-nav" aria-label="üìä Experimental Outcome Analysis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#component-performance-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      Component Performance Metrics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interaction-effect-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      Interaction Effect Analysis
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimization-outcomes" class="md-nav__link">
    <span class="md-ellipsis">
      Optimization Outcomes
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#research-question-framework" class="md-nav__link">
    <span class="md-ellipsis">
      üéØ Research Question Framework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="üéØ Research Question Framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#methodological-research-questions" class="md-nav__link">
    <span class="md-ellipsis">
      Methodological Research Questions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#substantive-research-questions" class="md-nav__link">
    <span class="md-ellipsis">
      Substantive Research Questions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#implementation-guidelines" class="md-nav__link">
    <span class="md-ellipsis">
      üìö Implementation Guidelines
    </span>
  </a>
  
    <nav class="md-nav" aria-label="üìö Implementation Guidelines">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#experimental-planning-process" class="md-nav__link">
    <span class="md-ellipsis">
      Experimental Planning Process
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quality-assurance-integration" class="md-nav__link">
    <span class="md-ellipsis">
      Quality Assurance Integration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#documentation-standards" class="md-nav__link">
    <span class="md-ellipsis">
      Documentation Standards
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#iterative-experimental-development" class="md-nav__link">
    <span class="md-ellipsis">
      üîÑ Iterative Experimental Development
    </span>
  </a>
  
    <nav class="md-nav" aria-label="üîÑ Iterative Experimental Development">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#component-development-cycle" class="md-nav__link">
    <span class="md-ellipsis">
      Component Development Cycle
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#framework-evolution" class="md-nav__link">
    <span class="md-ellipsis">
      Framework Evolution
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="experimental-design-framework-for-narrative-analysis">Experimental Design Framework for Narrative Analysis</h1>
<p><em>Version: 2.1.0</em><br />
<em>Last Updated: June 13, 2025</em></p>
<h2 id="overview"><strong>üéØ Overview</strong></h2>
<p>Narrative analysis experiments are systematic explorations of a <strong>five-dimensional design space</strong> where each dimension represents independent methodological choices. This framework enables rigorous hypothesis testing about the interaction effects between different analytical approaches, content types, and evaluation methods.</p>
<h2 id="the-five-dimensional-experimental-design-space"><strong>üìê The Five-Dimensional Experimental Design Space</strong></h2>
<h3 id="dimension-1-texts"><strong>Dimension 1: TEXTS</strong></h3>
<p><em>What content is being analyzed</em></p>
<p><strong>Design Choices</strong>:
- <strong>Content Type</strong>: Political speeches, literary works, social media, news articles, historical documents
- <strong>Text Length</strong>: Short-form (tweets, headlines) vs Long-form (speeches, articles)
- <strong>Historical Period</strong>: Contemporary vs Historical texts
- <strong>Author Characteristics</strong>: Known vs anonymous authors, demographic attributes
- <strong>Genre Conventions</strong>: Formal vs informal register, rhetorical vs descriptive style
- <strong>Temporal Scope</strong>: Single text vs text collections vs longitudinal corpora</p>
<p><strong>Experimental Implications</strong>:
- <strong>Content Validity</strong>: Different frameworks may be more appropriate for different content types
- <strong>Length Effects</strong>: Short texts may show higher variance, long texts more stable patterns
- <strong>Temporal Stability</strong>: Historical texts may require period-appropriate interpretation
- <strong>Author Effects</strong>: Known authorship may bias framework application
- <strong>Genre Sensitivity</strong>: Formal political speech vs casual social media require different analytical approaches</p>
<p><strong>Hypothesis Examples</strong>:
- <em>H1</em>: Civic virtue framework shows higher reliability on formal political texts than informal social media
- <em>H2</em>: Historical texts (&gt;50 years old) require adjusted weighting schemes for contemporary frameworks
- <em>H3</em>: Author anonymity reduces systematic bias in LLM moral framework application</p>
<h3 id="dimension-2-frameworks"><strong>Dimension 2: FRAMEWORKS</strong></h3>
<p><em>What theoretical lens is applied to the analysis</em></p>
<p><strong>Design Choices</strong>:
- <strong>Theoretical Foundation</strong>: Virtue ethics, moral foundations, political spectrum, rhetorical analysis
- <strong>Dimensional Structure</strong>: Number of dimensions, dipole vs single-pole design
- <strong>Complexity Level</strong>: Simple binary classifications vs complex multi-dimensional spaces
- <strong>Domain Specificity</strong>: General-purpose vs domain-specific frameworks
- <strong>Cultural Context</strong>: Western vs non-Western philosophical foundations
- <strong>Temporal Orientation</strong>: Contemporary vs historical theoretical frameworks</p>
<p><strong>Experimental Implications</strong>:
- <strong>Framework Fit</strong>: Some frameworks may be inappropriate for certain content types
- <strong>Dimensional Sufficiency</strong>: Complex texts may require more dimensional frameworks
- <strong>Cultural Bias</strong>: Western frameworks may not capture non-Western narrative patterns
- <strong>Theoretical Validity</strong>: Framework choice affects what patterns can be detected
- <strong>Comparative Analysis</strong>: Multiple frameworks enable triangulation and validation</p>
<p><strong>Hypothesis Examples</strong>:
- <em>H4</em>: Moral foundations framework shows higher inter-rater reliability than political spectrum for moral argumentation texts
- <em>H5</em>: Domain-specific frameworks (civic virtue for political texts) outperform general frameworks for specialized content
- <em>H6</em>: Multi-dimensional frameworks capture more nuanced patterns than binary classifications</p>
<h3 id="dimension-3-prompt-templates"><strong>Dimension 3: PROMPT TEMPLATES</strong></h3>
<p><em>How evaluators are instructed to perform analysis</em></p>
<p><strong>Design Choices</strong>:
- <strong>Analysis Approach</strong>: Hierarchical ranking vs simultaneous scoring vs comparative assessment
- <strong>Evidence Requirements</strong>: No evidence vs textual citations vs comprehensive justification
- <strong>Instruction Detail</strong>: Minimal vs comprehensive vs step-by-step guidance
- <strong>Output Format</strong>: Structured JSON vs natural language vs hybrid approaches
- <strong>Cognitive Load</strong>: Simple scoring vs complex analytical reasoning
- <strong>Model Compatibility</strong>: Model-agnostic vs model-optimized instructions</p>
<p><strong>Experimental Implications</strong>:
- <strong>Reliability Effects</strong>: More structured prompts may increase consistency but reduce nuance
- <strong>Evidence Quality</strong>: Evidence requirements improve justification but increase response length/cost
- <strong>Cognitive Complexity</strong>: Complex instructions may exceed some models' capabilities
- <strong>Parsing Reliability</strong>: Structured outputs enable automation but may constrain natural reasoning
- <strong>Model Bias</strong>: Different models may respond differently to identical instructions</p>
<p><strong>Hypothesis Examples</strong>:
- <em>H7</em>: Hierarchical prompts produce more reliable results than simultaneous scoring across all frameworks
- <em>H8</em>: Evidence-required prompts improve human-LLM agreement at the cost of response consistency
- <em>H9</em>: Model-optimized prompts show higher reliability within-model but lower cross-model generalizability</p>
<h3 id="dimension-4-weighting-schemes"><strong>Dimension 4: WEIGHTING SCHEMES</strong></h3>
<p><em>How scoring results are mathematically interpreted</em></p>
<p><strong>Design Choices</strong>:
- <strong>Mathematical Approach</strong>: Linear averaging vs nonlinear transformations vs hierarchical weighting
- <strong>Dominance Handling</strong>: Equal treatment vs amplification of dominant signals
- <strong>Noise Reduction</strong>: Standard calculation vs noise-suppressing algorithms
- <strong>Interpretability</strong>: Simple geometric positioning vs complex mathematical transformations
- <strong>Edge Case Handling</strong>: Standard vs specialized handling of ties, zeros, outliers
- <strong>Normalization Method</strong>: Raw scores vs normalized vs standardized approaches</p>
<p><strong>Experimental Implications</strong>:
- <strong>Pattern Detection</strong>: Different schemes emphasize different aspects of the underlying data
- <strong>Interpretability Trade-offs</strong>: Complex schemes may reveal patterns but reduce understandability
- <strong>Reliability Effects</strong>: Some schemes may amplify or reduce measurement error
- <strong>Comparative Validity</strong>: Scheme choice affects conclusions about relative positioning
- <strong>Mathematical Properties</strong>: Different schemes have different statistical properties</p>
<p><strong>Hypothesis Examples</strong>:
- <em>H10</em>: Winner-take-most weighting improves pattern clarity for texts with dominant themes
- <em>H11</em>: Hierarchical weighting based on LLM rankings shows higher validity than equal weighting
- <em>H12</em>: Linear schemes show higher reliability while nonlinear schemes show higher discriminative validity</p>
<h3 id="dimension-5-evaluators"><strong>Dimension 5: EVALUATORS</strong></h3>
<p><em>What agents perform the analysis</em></p>
<p><strong>Design Choices</strong>:
- <strong>Evaluator Type</strong>: Large Language Models vs Human expert reviewers vs Hybrid approaches
- <strong>LLM Provider</strong>: OpenAI vs Anthropic vs Mistral vs Google AI vs Open-source models
- <strong>Model Capability</strong>: Reasoning-optimized vs efficiency-optimized vs specialized models
- <strong>Human Expertise</strong>: Domain experts vs naive coders vs trained research assistants
- <strong>Evaluation Protocol</strong>: Independent assessment vs consensus building vs iterative refinement
- <strong>Scale Considerations</strong>: Single evaluator vs multiple evaluators vs crowd-sourcing</p>
<p><strong>Experimental Implications</strong>:
- <strong>Reliability Patterns</strong>: Different evaluators show different consistency patterns
- <strong>Validity Questions</strong>: Human-LLM agreement varies by task type and complexity
- <strong>Cost-Quality Trade-offs</strong>: Human evaluation expensive but potentially higher quality
- <strong>Bias Patterns</strong>: Different models and humans show different systematic biases
- <strong>Scalability</strong>: Evaluation choice affects feasible study scope and timeline</p>
<p><strong>Hypothesis Examples</strong>:
- <em>H13</em>: Claude models show higher evidence quality while GPT models show higher consistency
- <em>H14</em>: Human expert evaluation shows higher validity but lower reliability than LLM evaluation
- <em>H15</em>: Multi-model consensus approaches reduce systematic bias while maintaining efficiency</p>
<h2 id="experimental-design-methodologies"><strong>üî¨ Experimental Design Methodologies</strong></h2>
<h3 id="single-factor-experiments"><strong>Single-Factor Experiments</strong></h3>
<p><strong>Purpose</strong>: Isolate the effect of one dimensional choice while holding others constant</p>
<p><strong>Design Pattern</strong>:</p>
<pre><code>Texts: [Fixed set]
Frameworks: [Fixed framework]  
Prompts: [Variable: A, B, C]
Weighting: [Fixed scheme]
Evaluators: [Fixed model]
</code></pre>
<p><strong>Example</strong>: Testing whether hierarchical vs traditional vs evidence-based prompts affect reliability for civic virtue analysis of presidential speeches using GPT-4.1-mini and linear weighting.</p>
<p><strong>Statistical Analysis</strong>: ANOVA comparing means across prompt conditions
<strong>Key Metrics</strong>: Reliability (CV), validity (human agreement), efficiency (cost/time)</p>
<h3 id="two-factor-experiments"><strong>Two-Factor Experiments</strong></h3>
<p><strong>Purpose</strong>: Examine interaction effects between two dimensional choices</p>
<p><strong>Design Pattern</strong>:</p>
<pre><code>Texts: [Fixed set]
Frameworks: [Variable: Framework A, Framework B]
Prompts: [Variable: Prompt X, Prompt Y]  
Weighting: [Fixed scheme]
Evaluators: [Fixed model]
</code></pre>
<p><strong>Example</strong>: Testing framework √ó prompt interactions to determine whether hierarchical prompts work better with some frameworks than others.</p>
<p><strong>Statistical Analysis</strong>: 2√ó2 factorial ANOVA with interaction terms
<strong>Key Metrics</strong>: Main effects, interaction effects, effect sizes</p>
<h3 id="multi-factor-experiments"><strong>Multi-Factor Experiments</strong></h3>
<p><strong>Purpose</strong>: Systematic exploration of complex interaction patterns</p>
<p><strong>Design Pattern</strong>:</p>
<pre><code>Texts: [Stratified sample across content types]
Frameworks: [2-3 frameworks]
Prompts: [2-3 prompt types]
Weighting: [2-3 schemes]  
Evaluators: [2-3 models]
</code></pre>
<p><strong>Example</strong>: Full factorial design comparing civic virtue vs political spectrum frameworks with hierarchical vs traditional prompts using linear vs winner-take-most weighting across GPT vs Claude models.</p>
<p><strong>Statistical Analysis</strong>: Multi-way ANOVA, mixed-effects models, component analysis
<strong>Key Metrics</strong>: Main effects, all interaction terms, optimal configurations</p>
<h3 id="component-matrix-experiments"><strong>Component Matrix Experiments</strong></h3>
<p><strong>Purpose</strong>: Systematic optimization across all dimensional choices</p>
<p><strong>Design Pattern</strong>: Complete enumeration of practically feasible combinations
<strong>Sample Size</strong>: Determined by statistical power requirements for planned comparisons
<strong>Controls</strong>: Randomization of execution order, balanced assignment, replication</p>
<p><strong>Output</strong>: 
- <strong>Optimal Configurations</strong>: Best combinations for specific research goals
- <strong>Component Rankings</strong>: Relative importance of each dimensional choice
- <strong>Interaction Maps</strong>: Which combinations work well together
- <strong>Efficiency Frontiers</strong>: Cost-quality trade-offs across configurations</p>
<h3 id="validation-studies"><strong>Validation Studies</strong></h3>
<p><strong>Purpose</strong>: Compare LLM approaches against human expert evaluation</p>
<p><strong>Design Pattern</strong>:</p>
<pre><code>Texts: [Representative sample with known characteristics]
Frameworks: [Established, validated framework]
Prompts: [Best-performing from prior experiments]
Weighting: [Validated scheme]
Evaluators: [LLMs vs Human experts]
</code></pre>
<p><strong>Gold Standard</strong>: Expert human evaluation with high inter-rater reliability
<strong>Validation Metrics</strong>: Correlation, agreement rates, systematic bias detection
<strong>Outcome</strong>: Confidence bounds for LLM-based analysis validity</p>
<h2 id="experimental-outcome-analysis"><strong>üìä Experimental Outcome Analysis</strong></h2>
<h3 id="component-performance-metrics"><strong>Component Performance Metrics</strong></h3>
<p><strong>Reliability Measures</strong>:
- <strong>Intra-evaluator consistency</strong>: Multiple runs with same configuration
- <strong>Inter-evaluator agreement</strong>: Different evaluators, same configuration<br />
- <strong>Test-retest stability</strong>: Same analysis repeated over time
- <strong>Internal consistency</strong>: Coherence across framework dimensions</p>
<p><strong>Validity Measures</strong>:
- <strong>Content validity</strong>: Framework appropriateness for text type
- <strong>Construct validity</strong>: Framework captures intended theoretical constructs
- <strong>Criterion validity</strong>: Agreement with external validation measures
- <strong>Convergent validity</strong>: Agreement across different analytical approaches</p>
<p><strong>Efficiency Measures</strong>:
- <strong>Cost efficiency</strong>: Analysis quality per dollar spent
- <strong>Time efficiency</strong>: Analysis quality per unit time
- <strong>Scalability</strong>: Performance degradation with increased scope
- <strong>Resource utilization</strong>: Optimal use of computational/human resources</p>
<h3 id="interaction-effect-analysis"><strong>Interaction Effect Analysis</strong></h3>
<p><strong>Synergistic Effects</strong>: Combinations that perform better than individual components predict
<strong>Antagonistic Effects</strong>: Combinations that perform worse than expected
<strong>Compensatory Effects</strong>: Weaknesses in one dimension offset by strengths in another
<strong>Multiplicative Effects</strong>: Performance improvements that compound across dimensions</p>
<h3 id="optimization-outcomes"><strong>Optimization Outcomes</strong></h3>
<p><strong>Configuration Recommendations</strong>: Best combinations for specific research goals
<strong>Trade-off Analysis</strong>: Cost vs quality vs speed vs reliability optimization
<strong>Robustness Assessment</strong>: Performance stability across different contexts
<strong>Generalizability</strong>: Applicability of findings to new domains/applications</p>
<h2 id="research-question-framework"><strong>üéØ Research Question Framework</strong></h2>
<h3 id="methodological-research-questions"><strong>Methodological Research Questions</strong></h3>
<p><strong>Prompt Engineering</strong>:
- Which instruction approaches produce most reliable results?
- How do evidence requirements affect analysis quality vs efficiency?
- What level of instruction detail optimizes performance?</p>
<p><strong>Framework Validation</strong>:
- Which theoretical frameworks best capture specific content types?<br />
- How do framework complexity and analytical depth interact?
- What domain-specificity vs generalizability trade-offs exist?</p>
<p><strong>Weighting Methodology</strong>:
- Which mathematical approaches best reveal meaningful patterns?
- How do noise reduction vs information preservation trade-offs affect outcomes?
- What weighting schemes optimize interpretability vs discriminative power?</p>
<p><strong>Evaluator Performance</strong>:
- How do different LLMs compare on reliability, validity, and efficiency?
- What systematic biases exist across different models and providers?
- How does human expert evaluation compare to optimized LLM approaches?</p>
<p><strong>System Integration</strong>:
- Which component combinations produce optimal results for specific research goals?
- How do interaction effects vary across content types and research contexts?
- What are the efficiency frontiers for cost, quality, and speed optimization?</p>
<h3 id="substantive-research-questions"><strong>Substantive Research Questions</strong></h3>
<p><strong>Content Analysis</strong>:
- How do political communication patterns vary across speakers, time periods, contexts?
- What moral and rhetorical strategies characterize different types of persuasive discourse?
- How do narrative frameworks reveal bias, manipulation, or persuasive intent?</p>
<p><strong>Comparative Analysis</strong>:
- How do different authors, parties, or movements compare on specific analytical dimensions?
- What patterns distinguish effective vs ineffective persuasive communication?
- How do cultural, temporal, or contextual factors affect narrative analysis results?</p>
<p><strong>Longitudinal Analysis</strong>:
- How do communication patterns change over time within speakers or movements?
- What events or contexts trigger systematic changes in rhetorical approach?
- How do narrative frameworks capture evolution in political or cultural discourse?</p>
<h2 id="implementation-guidelines"><strong>üìö Implementation Guidelines</strong></h2>
<h3 id="experimental-planning-process"><strong>Experimental Planning Process</strong></h3>
<ol>
<li><strong>Research Question Definition</strong>: Clearly specify what hypotheses are being tested</li>
<li><strong>Dimensional Analysis</strong>: Identify which dimensions are experimental variables vs controls</li>
<li><strong>Power Analysis</strong>: Determine required sample sizes for planned statistical tests</li>
<li><strong>Resource Planning</strong>: Estimate costs, time requirements, and computational needs</li>
<li><strong>Protocol Design</strong>: Specify randomization, controls, and data collection procedures</li>
</ol>
<h3 id="quality-assurance-integration"><strong>Quality Assurance Integration</strong></h3>
<p><strong>Pre-experimental Validation</strong>:
- Component compatibility verification
- Pilot testing with small samples
- Statistical power confirmation
- Resource requirement validation</p>
<p><strong>During-experiment Monitoring</strong>:
- Real-time quality metrics tracking
- Anomaly detection and flagging
- Cost and time tracking against projections
- Interim analysis for early stopping or modification</p>
<p><strong>Post-experimental Validation</strong>:
- Comprehensive quality assessment using 6-layer QA system
- Statistical assumption testing
- Effect size calculation and interpretation
- Replication readiness verification</p>
<h3 id="documentation-standards"><strong>Documentation Standards</strong></h3>
<p><strong>Pre-registration</strong>: Complete experimental design specification before data collection
<strong>Provenance Tracking</strong>: Complete audit trail for all analytical choices and configurations
<strong>Replication Package</strong>: All code, data, and instructions necessary for independent replication
<strong>Transparency Reporting</strong>: Full disclosure of all analytical choices, including those that didn't work</p>
<h2 id="iterative-experimental-development"><strong>üîÑ Iterative Experimental Development</strong></h2>
<h3 id="component-development-cycle"><strong>Component Development Cycle</strong></h3>
<ol>
<li><strong>Hypothesis Formation</strong>: Specific predictions about component performance</li>
<li><strong>Pilot Testing</strong>: Small-scale validation of component functionality</li>
<li><strong>Systematic Evaluation</strong>: Controlled comparison against established alternatives</li>
<li><strong>Integration Testing</strong>: Performance in combination with other components</li>
<li><strong>Optimization</strong>: Parameter tuning and refinement based on empirical results</li>
</ol>
<h3 id="framework-evolution"><strong>Framework Evolution</strong></h3>
<p><strong>Version Control</strong>: Systematic tracking of component changes and performance impact
<strong>Backward Compatibility</strong>: Ensuring new versions can reproduce previous results
<strong>Migration Pathways</strong>: Clear procedures for updating experimental configurations
<strong>Deprecation Management</strong>: Graceful handling of obsolete components and methods</p>
<hr />
<p><strong>This framework enables systematic, rigorous experimental research that treats narrative analysis as a multidimensional methodological space rather than a collection of independent tools. It supports both component development and substantive research while maintaining standards for reproducibility and academic rigor.</strong> </p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
    
  </body>
</html>