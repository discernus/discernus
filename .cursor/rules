# Cursor Agent Rules for Discernus Project

## Critical Regression Remediation: Test-Driven Development (TDD) Protocol

### When Critical Regressions Are Identified
- **IMMEDIATE STOP**: Do not attempt live debugging or expensive API-based experiments
- **Document the Issue**: Log in `pm/inbox.md` with ARCH-XXX identifier (use "inbox this" command)
- **Create Remediation Plan**: Document detailed 6-phase TDD approach before any implementation

### 6-Phase TDD Protocol (Proven Success Pattern)
1. **Unit Test Development**: Write focused tests validating regression patterns and expected behavior
2. **Path Bug Investigation**: Identify and fix configuration/path issues with minimal code changes
3. **Core Implementation**: Implement fixes using proven patterns from working code
4. **Integration Testing**: Use mocks to verify end-to-end pipeline logic without API calls
5. **Limited Live Testing**: Create minimal 1-document experiments to validate fixes
6. **Full Validation**: Rerun original experiments to confirm complete resolution

### Cost Containment Rules
- **NO live experiments until Phase 5**: Use unit tests and mocks for 90% of debugging
- **Maximum 1-document tests**: Validate fixes without expensive multi-document runs
- **Document all costs**: Track API usage and justify each live experiment
- **Use proven patterns**: Import working functionality rather than rebuilding from scratch

### Success Criteria
- ✅ Unit tests pass for all regression patterns
- ✅ Integration tests pass with mocked dependencies  
- ✅ Limited live test completes successfully
- ✅ Original experiment reruns without errors
- ✅ Individual processing restored (not batch processing)
- ✅ All artifacts properly structured and accessible

### Documentation Requirements
- Update `docs/developer/` with detailed remediation plans
- Update `pm/sprints.md` with progress and completion (after grooming from inbox)
- Preserve all test files for future regression detection
- Document lessons learned in project memory

# THIN Architecture Rules - Cursor AI must follow these STRICTLY

## 🚨 CRITICAL: ENVIRONMENT MANAGEMENT RULES (Updated August 2025)
- **ENVIRONMENT IS STABLE**: No venv needed, uses system Python 3.13.5
- **USE `discernus` command directly** - it's installed as an entry point
- **NEVER recreate environments** - system is stable and working
- **If imports fail, run `make install`** to fix dependencies
- **Use `make check` to verify environment health**

## ✅ ARCHITECTURAL COMPLIANCE RULES (THIN COMPLIANCE ACHIEVED!)
- **COMPLIANCE STATUS**: ✅ All checks passed - THIN architecture fully compliant
- **PRE-COMMIT CHECK**: Run `python3 scripts/compliance_tools/thin_compliance_check.py` to verify continued compliance
- **THIN FIRST**: Tiered limits - Simple <200, Core <400, Complex <600, Orchestration <800 lines
- **YAML EXTERNALIZATION**: All prompts in .yaml files, never inline in Python code
- **NO COMPLEX PARSING**: Trust LLM output, use envelope extraction over parsing
- **ACADEMIC INTEGRITY**: All statistics via code execution, complete provenance chains
- **FRAMEWORK AGNOSTIC**: No hardcoded framework assumptions in orchestration
- **REFERENCE**: See docs/developer/CURSOR_AGENT_DISCIPLINE_GUIDE.md for full checklist
- **VIOLATION POLICY**: Any new code that increases violation count will be rejected

## 🚨 CRITICAL: GIT COMMAND RULES  
- **ALWAYS use SHORT commit messages (under 50 characters)**
- **NEVER write long multi-line commit messages**
- **ALWAYS use `git status` before complex git operations**
- **ALWAYS use `gh issue view [number] | cat` (never without | cat)**
- **NEVER use complex git commit messages that can hang terminals**

## ✅ REQUIRED GIT PATTERNS:
```bash
# GOOD - Short commit messages that won't hang
git commit -m "Fix documentation structure"
git commit -m "Add new feature X"
git commit -m "Update config files"

# GOOD - Safe GitHub CLI usage
gh issue view 135 | cat
gh issue list --limit 10
gh auth status

# ALWAYS check status first
git status && git commit -m "Brief message"
```

## 🚫 FORBIDDEN GIT PATTERNS:
```bash
# BAD - Long messages that can hang AI terminals
git commit -m "Very long commit message with lots of details and explanations that can cause terminal hanging in AI agent environments..."

# BAD - Commands without pager bypass
gh issue view 135  # Can hang indefinitely

# BAD - Complex multi-line operations
git commit -m """
Multi-line message
with lots of details
""" # Can cause hanging
```

## ✅ REQUIRED ENVIRONMENT PATTERNS (Updated August 2025):
```bash
# ALWAYS use these simple, reliable commands:
discernus list                                   # List experiments
discernus run projects/simple_test              # Run experiment
make check                                       # Verify environment

# FAST TESTING (recommended):
discernus run projects/simple_test --skip-validation  # ~47 seconds, $0.014

# LOCAL CONFIG (from experiment directory):
cd projects/simple_test && discernus run .      # Uses local Flash Lite config
```

## 🚫 FORBIDDEN ENVIRONMENT PATTERNS (Updated August 2025):
- Creating/recreating venv (system is stable)
- Complex Python module paths (use simple `discernus` command)
- Manual framework.md symlinks (canonical paths work automatically)
- Ignoring existing .discernus.yaml configs

## 🚨 FORBIDDEN PATTERNS - NEVER write these:
- import re, regex, bs4, xml.etree (use LLM for parsing instead)
- def parse_*, extract_*, validate_*, analyze_* (use LLM calls instead)
- More than 3 if/elif statements in a function (use LLM logic instead)
- String manipulation like .split(), .replace(), .strip() (use LLM formatting instead)
- Complex try/except blocks with multiple handlers (use LLM error recovery)
- Regex patterns, string parsing, content validation (use LLM processing)

## ✅ REQUIRED PATTERNS - ALWAYS write these:
- Read file → pass to LLM → store result (simple orchestration)
- Route message → call LLM → store response (no processing)
- Maximum 50 lines per function/class (complexity limit)
- Use llm_client.call_llm() for ANY content processing
- Simple file operations, Redis pub-sub, basic routing only
- Store raw LLM responses, don't parse or validate them

## 🤔 BEFORE WRITING ANY CODE, ASK:
1. Could an LLM solve this problem better?
2. Am I building intelligence into software? (Should be NO)
3. Is this simple orchestration/routing? (Should be YES)
4. Am I parsing or validating content? (Should be NO)
5. Could I explain this to a non-programmer? (Should be YES)

## 🎯 THIN SUCCESS PATTERNS:
```python
# GOOD - THIN pattern
def load_framework(path):
    content = Path(path).read_text()
    validation = llm_client.call_llm(f"Validate this framework: {content}")
    return {'content': content, 'validation': validation}

# BAD - THICK pattern  
def load_framework(path):
    content = Path(path).read_text()
    if not re.search(r'dimensions?:', content):
        raise ValueError("Missing dimensions")
    # ... complex parsing logic
```

## 🛑 IF YOU START WRITING PARSING/VALIDATION LOGIC, STOP!
Use LLM calls instead. The software orchestrates, the LLM provides intelligence.



# 📋 PROJECT MANAGEMENT RULES - NEW BACKLOG STRUCTURE (January 2025)

## 🎯 BACKLOG MANAGEMENT WORKFLOW

### Quick Capture Commands
- **"inbox this"** → **TRULY APPEND** new backlog item to `pm/inbox.md` with minimal formatting
- **"inbox this: description"** → **TRULY APPEND** item without reading existing content or rewriting file
- **CRITICAL**: Use file append operations, NOT file read-then-write operations
- **Purpose**: Fast capture during implementation work, no context switching required

### Backlog Organization Commands  
- **"groom our sprints"** → Move all items from `pm/inbox.md` to `pm/sprints.md` with proper organization
- **"groom our sprints"** → Organize items into appropriate sprints, map dependencies, assign priorities
- **Purpose**: Systematic organization when ready to plan and prioritize

### Completion Tracking Commands
- **"log it to done"** → Move completed items from `pm/sprints.md` to `pm/done.md`
- **"log it to done"** → Archive completed work with completion dates and final status
- **Purpose**: Maintain project history and track completion progress

## 📁 FILE STRUCTURE & PURPOSE

### `pm/inbox.md`
- **Content**: Raw, unformatted backlog captures
- **Format**: Simple bullet points with minimal details
- **Usage**: Quick capture only, no organization or sprint assignment

### `pm/sprints.md` 
- **Content**: Organized backlog with sprint planning and detailed specifications
- **Format**: Full backlog items with sprint assignments, dependencies, acceptance criteria
- **Usage**: Main backlog for planning and tracking active work

### `pm/done.md`
- **Content**: Archive of completed backlog items
- **Format**: Completed items with completion dates and final status
- **Usage**: Historical tracking and project completion records

## 🔄 WORKFLOW RULES

### Context Efficiency Rules
- **Quick Capture**: **NEVER read entire `pm/inbox.md`** for "inbox this" commands
- **Quick Capture**: Use file append operations to add new items without reading existing content
- **Grooming**: Only parse `pm/inbox.md` and `pm/sprints.md` for "groom our sprints" commands
- **Completion**: Only parse `pm/sprints.md` for "log it to done" commands
- **No Full Backlog Parsing**: Avoid parsing entire `pm/sprints.md` for simple operations

### Formatting Rules
- **Inbox Items**: Simple, unformatted captures (e.g., "- [ ] Fix RAG index issue")
- **Sprint Items**: Full format with ID, description, dependencies, acceptance criteria
- **Done Items**: Completed format with completion date and final status

### Migration Rules
- **Inbox → Sprints**: Move all items during grooming, clear inbox after successful migration
- **Sprints → Done**: Move completed items, maintain completion history
- **Preserve Structure**: Maintain proper formatting and organization during migrations

### Technical Implementation Rules
- **"inbox this" Implementation**: Use `edit_file` with `append=True` or equivalent append operation
- **NEVER use `read_file` + `edit_file`** for inbox operations - this defeats context efficiency
- **Append Format**: Add new items under "## Inbox Items" section with simple bullet format
- **Example Format**: "- [ ] Brief description of the item"
- **File Size Limit**: If inbox.md exceeds 50 lines, suggest grooming instead of adding more items

# 🔍 DEBUGGING RULES - MANDATORY USE OF VERBOSE TRACING SYSTEM

## 🚨 CRITICAL: NO MORE AD-HOC DEBUG LOGGING

### FORBIDDEN DEBUGGING PATTERNS - NEVER DO THESE:
- **NEVER add `print(f"DEBUG: ...")` statements** - Use verbose tracing instead
- **NEVER add manual debug logging** like "Let me add some debug logging to see what's happening"
- **NEVER add temporary debug prints** that need to be cleaned up later
- **NEVER say "The logs show that X was set to true, but there's no evidence of Y running"** - Use tracing instead
- **NEVER add debug statements to trace execution flow** - Use `--verbose-trace` flag
- **NEVER add manual logging to understand component calls** - Use built-in tracing system

### ✅ REQUIRED DEBUGGING PATTERNS - ALWAYS DO THESE:

#### 1. USE VERBOSE TRACING FOR ALL DEBUGGING
```bash
# ALWAYS use this for debugging:
discernus run <experiment> --verbose-trace

# NEVER add manual debug prints - use the built-in system
```

#### 2. EXAMINE TRACE LOGS SYSTEMATICALLY
```bash
# Check trace logs for function calls and returns:
cat projects/<experiment>/session/<timestamp>/logs/trace.jsonl

# Check agent logs for detailed execution:
cat projects/<experiment>/session/<timestamp>/logs/agents.jsonl

# Check orchestrator logs for workflow:
cat projects/<experiment>/session/<timestamp>/logs/orchestrator.jsonl
```

#### 3. USE TRACE LOGS TO IDENTIFY ISSUES
- **Function Call Tracing**: See exactly which functions are called and from where
- **Parameter Tracing**: See what parameters are passed to each function
- **Return Value Tracing**: See what each function returns and its type
- **Performance Tracing**: See execution timing for each function
- **Exception Tracing**: See exactly where and why exceptions occur

#### 4. STRATEGIC TRACING ADDITIONS
- **Add tracing to key orchestrator methods** using `audit_logger.log_function_call()`
- **Add tracing to agent interfaces** using `audit_logger.log_function_return()`
- **Use `@audit_logger.trace_calls()` decorator** for automatic tracing
- **Trace artifact flows** between components for debugging data issues

## 🎯 DEBUGGING WORKFLOW - MANDATORY SEQUENCE

### Step 1: Enable Verbose Tracing
```bash
discernus run <experiment> --verbose-trace
```

### Step 2: Examine Trace Logs
```bash
# Check what functions were called and when:
cat projects/<experiment>/session/<timestamp>/logs/trace.jsonl | jq '.event_type, .component, .function'

# Check specific component execution:
cat projects/<experiment>/session/<timestamp>/logs/trace.jsonl | jq 'select(.component == "StatisticalAgent")'
```

### Step 3: Analyze Agent Logs
```bash
# Check detailed agent execution:
cat projects/<experiment>/session/<timestamp>/logs/agents.jsonl | jq '.agent_name, .event_type, .data'
```

### Step 4: Identify Root Cause
- **Use trace logs to see exact call stack**
- **Use agent logs to see detailed execution steps**
- **Use orchestrator logs to see workflow progression**
- **Never guess - always trace the actual execution**

### Step 5: Fix Based on Evidence
- **Fix the actual issue identified in trace logs**
- **Don't add debug prints - fix the root cause**
- **Use trace logs to verify the fix works**

## 🚫 ANTI-PATTERNS - NEVER DO THESE

### BAD DEBUGGING APPROACH:
```
"The logs show that statistical_prep_completed was set to true, but there's no evidence of the StatisticalAgent actually running. Let me check if there's an issue with the _run_statistical_analysis_phase method. Let me add some debug logging to see what's happening."
```

### GOOD DEBUGGING APPROACH:
```
"Let me run with --verbose-trace to see exactly what's happening with the StatisticalAgent call. I'll examine the trace logs to see if the function is being called and what it returns."
```

## ✅ SUCCESS PATTERNS

### When Debugging Issues:
1. **ALWAYS start with `--verbose-trace`**
2. **ALWAYS examine trace.jsonl first**
3. **ALWAYS check agents.jsonl for detailed execution**
4. **ALWAYS use trace logs to identify root cause**
5. **NEVER add manual debug prints**

### When Adding New Tracing:
1. **Add strategic tracing to key methods**
2. **Use `audit_logger.log_function_call()` and `log_function_return()`**
3. **Use `@audit_logger.trace_calls()` decorator for automatic tracing**
4. **Trace artifact flows between components**

## 🎯 DEBUGGING VELOCITY RULES

### MANDATORY DEBUGGING SEQUENCE:
1. **Run with `--verbose-trace`** (30 seconds)
2. **Examine trace logs** (2 minutes)
3. **Identify root cause from evidence** (5 minutes)
4. **Fix based on trace data** (10 minutes)
5. **Verify fix with trace logs** (2 minutes)

### TOTAL DEBUGGING TIME: ~20 minutes vs. hours of manual debugging

### NEVER SPEND TIME ON:
- Adding manual debug prints
- Guessing what's happening
- Adding temporary logging
- Manual execution tracing
- Ad-hoc debugging approaches

### ALWAYS SPEND TIME ON:
- Using verbose tracing system
- Analyzing trace log evidence
- Fixing root causes identified in traces
- Adding strategic tracing to key methods
- Using trace logs to verify fixes

## 🚨 ENFORCEMENT RULES

### If You Catch Yourself About To:
- Add `print(f"DEBUG: ...")` → **STOP** and use `--verbose-trace` instead
- Add manual debug logging → **STOP** and examine trace logs instead
- Guess what's happening → **STOP** and trace the actual execution instead
- Add temporary debug prints → **STOP** and use the built-in tracing system instead

### Remember:
- **Verbose tracing system exists and works perfectly**
- **Trace logs contain all the information you need**
- **Manual debug prints are unnecessary and counterproductive**
- **Use the system that was built for this purpose** 