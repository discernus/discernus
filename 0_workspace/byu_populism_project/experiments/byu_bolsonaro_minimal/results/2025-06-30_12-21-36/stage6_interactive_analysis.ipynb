{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Minimal Debug - 1 Speech, 2 Models - Interactive Analysis\n",
        "**Framework:** minimal_test  \n",
        "**Generated:** 2025-06-30T12:21:36.430281  \n",
        "**Job ID:** cd36b892-2a58-45ec-b833-e9e83cece4e9  \n",
        "\n",
        "## Overview\n",
        "This notebook provides interactive analysis of your experiment results with embedded statistical methods, ",
        "visualization tools, and academic export capabilities. Perfect for individual research - ",
        "scales beautifully until you have lots of experiments and need enterprise organization tools! üìä\n",
        "\n",
        "### Tamaki & Fuks 2019 Replication Analysis\n",
        "This analysis replicates and extends the methodology from Tamaki & Fuks (2019) using the ",
        "Democratic Tension Axis Model for Brazilian political discourse. **Models analyzed:** claude-3-5-haiku-20241022  \n",
        "**Total analyses:** 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EXPERIMENT DATA SETUP - Auto-generated from Stage 5 results\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from scipy import stats\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "# Experiment metadata\n",
        "JOB_ID = 'cd36b892-2a58-45ec-b833-e9e83cece4e9'\n",
        "FRAMEWORK_NAME = 'minimal_test'\n",
        "EXPERIMENT_NAME = 'Minimal Debug - 1 Speech, 2 Models'\n",
        "MODELS_ANALYZED = ['claude-3-5-haiku-20241022']\n",
        "TOTAL_ANALYSES = 0\n",
        "\n",
        "print(f'üìä Loaded experiment: {EXPERIMENT_NAME}')\n",
        "print(f'üéØ Framework: {FRAMEWORK_NAME}')\n",
        "print(f'ü§ñ Models: {\", \".join(MODELS_ANALYZED)}')\n",
        "print(f'üìà Total analyses: {TOTAL_ANALYSES}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# RESULTS DATA - Pre-loaded from Stage 5 experiment\n",
        "# =============================================================================\n",
        "\n",
        "# Raw experiment results\n",
        "EXPERIMENT_RESULTS = {\n  \"job_id\": \"cd36b892-2a58-45ec-b833-e9e83cece4e9\",\n  \"comparison_type\": \"multi_model\",\n  \"similarity_classification\": \"STATISTICALLY_DIFFERENT\",\n  \"confidence_level\": 0.0,\n  \"condition_results\": [\n    {\n      \"condition_identifier\": \"claude-3-5-haiku-20241022\",\n      \"centroid\": [\n        1.669972907928209e-17,\n        0.6363636363636362\n      ],\n      \"raw_scores\": {\n        \"populism\": 0.9,\n        \"pluralism\": 0.2\n      }\n    }\n  ],\n  \"statistical_metrics\": {\n    \"geometric_similarity\": {\n      \"model_average_centroids\": {\n        \"claude-3-5-haiku-20241022\": [\n          1.669972907928209e-17,\n          0.6363636363636362\n        ]\n      },\n      \"pairwise_distances\": {},\n      \"mean_distance\": 0.0,\n      \"max_distance\": 0.0,\n      \"min_distance\": 0.0,\n      \"std_distance\": 0.0\n    }\n  },\n  \"significance_tests\": {},\n  \"report_url\": null\n}\n",
        "\n",
        "# Extract condition results into DataFrame\n",
        "condition_data = []\n",
        "for condition in EXPERIMENT_RESULTS['condition_results']:\n",
        "    condition_data.append({\n",
        "        'model': condition['condition_identifier'],\n",
        "        'centroid_x': condition['centroid'][0],\n",
        "        'centroid_y': condition['centroid'][1], \n",
        "        'total_analyses': condition.get('total_analyses', 0),\n",
        "        'raw_scores': condition.get('raw_scores', {})\n",
        "    })\n",
        "\n",
        "df_results = pd.DataFrame(condition_data)\n",
        "print('‚úÖ Results loaded into DataFrame:')\n",
        "print(df_results.head())\n",
        "\n",
        "# Statistical metrics\n",
        "STATISTICAL_METRICS = {\n  \"geometric_similarity\": {\n    \"model_average_centroids\": {\n      \"claude-3-5-haiku-20241022\": [\n        1.669972907928209e-17,\n        0.6363636363636362\n      ]\n    },\n    \"pairwise_distances\": {},\n    \"mean_distance\": 0.0,\n    \"max_distance\": 0.0,\n    \"min_distance\": 0.0,\n    \"std_distance\": 0.0\n  }\n}\n",
        "print(f'\\nüìä Statistical metrics available: {list(STATISTICAL_METRICS.keys())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tamaki & Fuks 2019 Validation Analysis\n",
        "### Democratic Tension Axis Model - Brazilian Political Discourse\n",
        "\n",
        "This section provides validation analysis comparing our LLM-based methodology with the original Tamaki & Fuks manual coding approach.\n",
        "\n",
        "**Framework Description:**  \n",
        "- **Populism‚ÜîPluralism Axis** (Vertical): Direct popular sovereignty vs. institutional mediation  \n",
        "- **Patriotism‚ÜîNationalism Axis** (Horizontal): Civic attachment vs. ethnic/cultural supremacy  \n",
        "- **Brazilian Portuguese Optimized** with specific language cues from T&F 2019  \n",
        "- **Cross-validation Ready** for direct correlation analysis with manual coding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COORDINATE VISUALIZATION - Democratic Tension Quadrants\n",
        "# =============================================================================\n",
        "\n",
        "# Create quadrant visualization\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "\n",
        "# Plot model centroids\n",
        "colors = sns.color_palette('husl', len(df_results))\n",
        "for i, (_, row) in enumerate(df_results.iterrows()):\n",
        "    ax.scatter(row['centroid_x'], row['centroid_y'], \n",
        "              s=200, alpha=0.7, color=colors[i], \n",
        "              label=row['model'])\n",
        "    ax.annotate(row['model'], \n",
        "               (row['centroid_x'], row['centroid_y']),\n",
        "               xytext=(5, 5), textcoords='offset points',\n",
        "               fontsize=10, ha='left')\n",
        "\n",
        "# Add quadrant lines and labels\n",
        "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Quadrant labels for Democratic Tension Model\n",
        "ax.text(0.5, 0.5, 'High Populism\\n+ High Nationalism', \n",
        "        transform=ax.transAxes, ha='center', va='center',\n",
        "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "ax.text(-0.5, 0.5, 'High Populism\\n+ High Patriotism',\n",
        "        transform=ax.transAxes, ha='center', va='center', \n",
        "        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
        "ax.text(-0.5, -0.5, 'High Pluralism\\n+ High Patriotism',\n",
        "        transform=ax.transAxes, ha='center', va='center',\n",
        "        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
        "ax.text(0.5, -0.5, 'High Pluralism\\n+ High Nationalism',\n",
        "        transform=ax.transAxes, ha='center', va='center',\n",
        "        bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))\n",
        "\n",
        "ax.set_xlabel('Patriotism ‚Üê ‚Üí Nationalism', fontsize=12)\n",
        "ax.set_ylabel('Pluralism ‚Üê ‚Üí Populism', fontsize=12)\n",
        "ax.set_title(f'Democratic Tension Analysis: {EXPERIMENT_NAME}\\nBrazilian Political Discourse Coordinates', \n",
        "             fontsize=14, pad=20)\n",
        "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_aspect('equal')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f'üìä Coordinate plot generated for {len(df_results)} models')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EMBEDDED STATISTICAL ANALYSIS - Creates natural scaling challenges! üìà\n",
        "# =============================================================================\n",
        "\n",
        "def calculate_geometric_similarity(results_df):\n",
        "    \"\"\"Calculate pairwise geometric distances between model centroids\"\"\"\n",
        "    distances = []\n",
        "    models = results_df['model'].tolist()\n",
        "    \n",
        "    for i in range(len(results_df)):\n",
        "        for j in range(i + 1, len(results_df)):\n",
        "            x1, y1 = results_df.iloc[i][['centroid_x', 'centroid_y']]\n",
        "            x2, y2 = results_df.iloc[j][['centroid_x', 'centroid_y']]\n",
        "            distance = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
        "            distances.append({\n",
        "                'model_1': models[i],\n",
        "                'model_2': models[j], \n",
        "                'distance': distance\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(distances)\n",
        "\n",
        "def calculate_dimensional_correlation(results_df):\n",
        "    \"\"\"Calculate correlation between model positions\"\"\"\n",
        "    if len(results_df) < 2:\n",
        "        return {\"error\": \"Need at least 2 models for correlation\"}\n",
        "    \n",
        "    x_coords = results_df['centroid_x'].values\n",
        "    y_coords = results_df['centroid_y'].values\n",
        "    \n",
        "    correlation = np.corrcoef(x_coords, y_coords)[0, 1]\n",
        "    \n",
        "    return {\n",
        "        'x_y_correlation': correlation,\n",
        "        'x_mean': np.mean(x_coords),\n",
        "        'y_mean': np.mean(y_coords),\n",
        "        'x_std': np.std(x_coords),\n",
        "        'y_std': np.std(y_coords)\n",
        "    }\n",
        "\n",
        "# Run embedded statistical analysis\n",
        "geometric_analysis = calculate_geometric_similarity(df_results)\n",
        "correlation_analysis = calculate_dimensional_correlation(df_results)\n",
        "\n",
        "print('‚úÖ Geometric Similarity Analysis:')\n",
        "print(geometric_analysis)\n",
        "print('\\n‚úÖ Dimensional Correlation Analysis:')\n",
        "print(correlation_analysis)\n",
        "\n",
        "# This is getting complex... imagine having 20+ experiments to manage! ü§î"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PUBLICATION-READY EXPORT - Manual process that scales poorly üìù\n",
        "# =============================================================================\n",
        "\n",
        "def export_for_publication(results_df, job_id):\n",
        "    \"\"\"Export results in academic publication format\"\"\"\n",
        "    \n",
        "    # Create publication directory \n",
        "    pub_dir = Path(f'publication_exports/{job_id}')\n",
        "    pub_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Export data as CSV\n",
        "    results_df.to_csv(pub_dir / 'model_centroids.csv', index=False)\n",
        "    \n",
        "    # Export statistical summary\n",
        "    summary = {\n",
        "        'experiment_name': EXPERIMENT_NAME,\n",
        "        'framework': FRAMEWORK_NAME,\n",
        "        'models_analyzed': MODELS_ANALYZED,\n",
        "        'total_analyses': TOTAL_ANALYSES,\n",
        "        'mean_x': results_df['centroid_x'].mean(),\n",
        "        'mean_y': results_df['centroid_y'].mean(),\n",
        "        'std_x': results_df['centroid_x'].std(),\n",
        "        'std_y': results_df['centroid_y'].std()\n",
        "    }\n",
        "    \n",
        "    with open(pub_dir / 'summary_statistics.json', 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    \n",
        "    print(f'üìä Publication files exported to: {pub_dir}')\n",
        "    print('üìÅ Files: model_centroids.csv, summary_statistics.json')\n",
        "    \n",
        "    return pub_dir\n",
        "\n",
        "# Export for publication\n",
        "export_dir = export_for_publication(df_results, JOB_ID)\n",
        "\n",
        "print('\\nüéì Ready for academic submission!')\n",
        "print('üí° Pro tip: With multiple experiments, managing all these exports becomes... challenging!')\n",
        "print('üöÄ That\\'s when enterprise tools become really helpful! üòâ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps for BYU Collaboration\n",
        "\n",
        "### Validation Protocol\n",
        "1. **Correlation Analysis**: Compare these LLM results with Tamaki & Fuks manual coding\n",
        "2. **Statistical Significance**: Test if differences are meaningful (target: r > 0.70)\n",
        "3. **Methodological Documentation**: Prepare for academic publication\n",
        "\n",
        "### Value Demonstration\n",
        "- **Speed**: LLM analysis completes in minutes vs. weeks of manual coding\n",
        "- **Scale**: Can analyze entire corpora not feasible for manual coding\n",
        "- **Consistency**: Eliminates inter-rater reliability concerns\n",
        "- **Innovation**: Enables novel analytical approaches (temporal dynamics, cross-framework comparison)\n",
        "\n",
        "### Research Acceleration Opportunities\n",
        "- **Global Populism Database**: Scale to thousands of speeches across countries\n",
        "- **Temporal Analysis**: Track discourse evolution across election cycles  \n",
        "- **Comparative Frameworks**: Apply multiple theoretical lenses simultaneously\n",
        "- **Real-time Analysis**: Monitor contemporary political discourse as it emerges\n",
        "\n",
        "**Ready to transform computational social science research! üöÄ**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "discernus_metadata": {
      "stage6_generated": true,
      "job_id": "cd36b892-2a58-45ec-b833-e9e83cece4e9",
      "framework_name": "minimal_test",
      "template_type": "tamaki_fuks_replication",
      "generation_timestamp": "2025-06-30T12:21:36.430281"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}