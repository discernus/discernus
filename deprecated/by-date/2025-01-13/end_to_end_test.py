#!/usr/bin/env python3
"""
End-to-End MVP Test
==================

Test the complete conversation flow: User â†’ Design LLM â†’ Response
This validates our ultra-thin infrastructure with actual LLM calls.
"""

import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from discernus.core.session_manager import SessionManager
from discernus.core.message_router import MessageRouter
from discernus.core.thin_conversation_logger import ThinConversationLogger

def test_end_to_end_conversation():
    """Test complete conversation flow with actual LLM calls"""
    print("ğŸš€ Testing End-to-End Conversation Flow")
    print("=" * 50)
    
    # Initialize infrastructure
    session_mgr = SessionManager()
    router = MessageRouter()
    logger = ThinConversationLogger()
    
    # Create research session
    research_question = "Which inaugural address is more unifying - Lincoln 1865 or Trump 2025?"
    session_id = session_mgr.create_session(research_question)
    print(f"âœ… Created session: {session_id}")
    
    # Log initial user message
    logger.log_message(session_id, "user", research_question)
    print(f"âœ… Logged user question")
    
    try:
        # Test Design LLM call
        print("\nğŸ¤– Calling Design LLM...")
        
        # Prepare context for Design LLM
        context_message = f"""
Research Question: {research_question}

Please propose a systematic methodology for analyzing these inaugural addresses.
Follow the RAG++ principles and end with "Does this methodology look right to you?"

The texts are available at:
- data/inaugural_addresses/lincoln_1865_second_inaugural.txt
- data/inaugural_addresses/trump_2025_inaugural.txt
"""
        
        # Route to Design LLM
        design_response = router.route_message(
            from_role="user",
            to_role="design", 
            message=context_message,
            session_id=session_id
        )
        
        print(f"âœ… Design LLM responded ({len(design_response)} characters)")
        print(f"ğŸ“ Response preview: {design_response[:200]}...")
        
        # Log the response
        logger.log_message(session_id, "design", design_response)
        
        # Test handoff detection
        print("\nğŸ”„ Testing handoff detection...")
        if "HANDOFF" in design_response.upper() or "CALL" in design_response.upper():
            print("âœ… Handoff pattern detected in response")
        else:
            print("â„¹ï¸ No handoff pattern (normal for first interaction)")
        
        # Show conversation log
        print("\nğŸ“‹ Reading conversation log...")
        conversation = logger.read_conversation(session_id)
        print(f"âœ… Conversation log: {len(conversation)} characters")
        
        # Complete session
        session_mgr.end_session(session_id)
        print(f"âœ… Session committed to Git")
        
        print("\nğŸ‰ End-to-End Test Successful!")
        print("\nğŸ“Š Validation Results:")
        print("   âœ… Session management working")
        print("   âœ… LLM routing working") 
        print("   âœ… Conversation logging working")
        print("   âœ… Git persistence working")
        print(f"   âœ… Complete workflow: User â†’ Design LLM â†’ Response")
        
        return True
        
    except Exception as e:
        print(f"âŒ End-to-End Test Failed: {e}")
        print("\nğŸ”§ This might be due to:")
        print("   - Missing .env file with API keys")
        print("   - LiteLLM client configuration issues")
        print("   - Network connectivity")
        
        # Still commit what we have
        session_mgr.end_session(session_id)
        return False

if __name__ == "__main__":
    success = test_end_to_end_conversation()
    if success:
        print("\nğŸš€ Ready for full Lincoln vs Trump analysis!")
    else:
        print("\nğŸ› ï¸ Need to resolve LLM integration issues first") 