#!/usr/bin/env python3
"""
IDITI Production Analysis - Experimental Prototype

Uses sophisticated production QA systems to analyze real IDITI experiment data.
Demonstrates the value of LLMQualityAssuranceSystem vs deprecated "AI Academic Advisor".

Following our rules:
1. ‚úÖ Built in experimental/ first
2. ‚úÖ Using existing production systems (LLMQualityAssuranceSystem)
3. ‚úÖ Enhancing rather than rebuilding
4. ‚úÖ No deprecated system usage
"""

import sys
import json
from pathlib import Path
import pandas as pd

# Add src to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / "src"))

def analyze_iditi_experiments():
    """Analyze IDITI experiment data using production QA systems."""
    print("üî¨ IDITI Production Analysis - Using Sophisticated QA Systems")
    print("=" * 70)
    
    try:
        # Import production systems (not deprecated ones!)
        from narrative_gravity.utils.llm_quality_assurance import LLMQualityAssuranceSystem
        
        print("‚úÖ Using production LLMQualityAssuranceSystem (6-layer validation)")
        print("‚ùå NOT using deprecated 'AI Academic Advisor' (file existence checks)")
        
        # Initialize production QA system
        qa_system = LLMQualityAssuranceSystem()
        
        # Find IDITI experiment directories
        experiments_dir = project_root / "experiments"
        iditi_experiments = list(experiments_dir.glob("iditi_multi_llm_validation_*"))
        
        print(f"\nüìä Found {len(iditi_experiments)} IDITI experiments:")
        for exp in iditi_experiments:
            print(f"   - {exp.name}")
        
        if not iditi_experiments:
            print("‚ùå No IDITI experiments found")
            return False
        
        # Analyze each experiment
        analysis_results = []
        
        for exp_dir in iditi_experiments:
            print(f"\nüîç Analyzing: {exp_dir.name}")
            
            # Load structured results
            results_file = exp_dir / "enhanced_analysis" / "structured_results.json"
            if not results_file.exists():
                print(f"   ‚ö†Ô∏è No structured results found")
                continue
            
            with open(results_file, 'r') as f:
                exp_data = json.load(f)
            
            metadata = exp_data.get('metadata', {})
            
            print(f"   üìà Experiment Stats:")
            print(f"      - Total analyses: {metadata.get('total_analyses', 0)}")
            print(f"      - Successful: {metadata.get('successful_analyses', 0)}")
            print(f"      - Failed: {metadata.get('failed_analyses', 0)}")
            print(f"      - Cost: ${metadata.get('total_cost', 0):.4f}")
            
            # Load individual results for QA analysis
            pipeline_file = exp_dir / "enhanced_analysis" / "pipeline_results.json"
            if pipeline_file.exists():
                with open(pipeline_file, 'r') as f:
                    pipeline_data = json.load(f)
                
                # Analyze sample results with production QA
                sample_results = pipeline_data.get('individual_results', [])[:5]  # First 5 samples
                
                print(f"   üß™ QA Analysis of {len(sample_results)} sample results:")
                
                qa_scores = []
                for i, result in enumerate(sample_results):
                    # Extract scores
                    scores = {}
                    if 'well_scores' in result:
                        scores = result['well_scores']
                    elif 'dominant_wells' in result:
                        # Convert dominant wells to scores
                        for well_data in result['dominant_wells']:
                            scores[well_data['well'].lower()] = well_data['score']
                    
                    if scores:
                        # Run production QA validation
                        mock_response = {
                            "scores": scores,
                            "analysis": f"Analysis for result {i+1}"
                        }
                        
                        qa_assessment = qa_system.validate_llm_analysis(
                            text_input=f"Sample text for analysis {i+1}",
                            framework="iditi",
                            llm_response=mock_response,
                            parsed_scores=scores
                        )
                        
                        qa_scores.append({
                            'result_index': i+1,
                            'confidence_level': qa_assessment.confidence_level,
                            'confidence_score': qa_assessment.confidence_score,
                            'requires_second_opinion': qa_assessment.requires_second_opinion,
                            'anomalies_count': len(qa_assessment.anomalies_detected),
                            'critical_issues': len([c for c in qa_assessment.individual_checks 
                                                  if not c.passed and c.severity == 'CRITICAL'])
                        })
                
                # Summarize QA results
                if qa_scores:
                    avg_confidence = sum(r['confidence_score'] for r in qa_scores) / len(qa_scores)
                    high_confidence = sum(1 for r in qa_scores if r['confidence_level'] == 'HIGH')
                    total_critical = sum(r['critical_issues'] for r in qa_scores)
                    
                    print(f"      üéØ QA Summary:")
                    print(f"         - Average confidence: {avg_confidence:.3f}")
                    print(f"         - High confidence results: {high_confidence}/{len(qa_scores)}")
                    print(f"         - Total critical issues: {total_critical}")
                    
                    # Identify quality patterns
                    if avg_confidence < 0.3:
                        print(f"         üö® LOW QUALITY: Likely contains problematic default values")
                    elif avg_confidence < 0.6:
                        print(f"         ‚ö†Ô∏è MEDIUM QUALITY: Some issues detected")
                    else:
                        print(f"         ‚úÖ HIGH QUALITY: Good analysis results")
                
                analysis_results.append({
                    'experiment': exp_dir.name,
                    'qa_scores': qa_scores,
                    'avg_confidence': avg_confidence if qa_scores else 0,
                    'metadata': metadata
                })
        
        # Overall summary
        print(f"\nüìä OVERALL ANALYSIS SUMMARY")
        print("=" * 50)
        
        if analysis_results:
            total_experiments = len(analysis_results)
            avg_quality = sum(r['avg_confidence'] for r in analysis_results) / total_experiments
            
            print(f"‚úÖ Production QA System Performance:")
            print(f"   - Experiments analyzed: {total_experiments}")
            print(f"   - Average quality score: {avg_quality:.3f}")
            print(f"   - QA system: LLMQualityAssuranceSystem (6-layer validation)")
            
            # Compare to what deprecated system would miss
            print(f"\nüÜö vs Deprecated 'AI Academic Advisor':")
            print(f"   ‚ùå Would only check: File existence, basic string matching")
            print(f"   ‚úÖ Production QA detects: Default value ratios, variance issues,")
            print(f"      mathematical consistency, statistical anomalies, confidence scoring")
            
            # Recommendations
            print(f"\nüéØ Next Steps:")
            if avg_quality < 0.4:
                print(f"   1. üîß Data quality issues detected - investigate failed experiments")
                print(f"   2. üìä Use QAEnhancedDataExporter for clean academic export")
                print(f"   3. üß™ Re-run experiments with corrected framework boundaries")
            else:
                print(f"   1. üìä Export high-quality data with QAEnhancedDataExporter")
                print(f"   2. üìà Run statistical hypothesis testing")
                print(f"   3. üìÑ Generate academic publication materials")
        
        print(f"\n‚úÖ PRODUCTION SYSTEMS SUCCESSFULLY DEMONSTRATED VALUE!")
        return True
        
    except Exception as e:
        print(f"‚ùå Error in production analysis: {e}")
        return False

if __name__ == "__main__":
    success = analyze_iditi_experiments()
    sys.exit(0 if success else 1) 